{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wilberquito/NeuralNetworksCompetition2023/blob/main/Code/ML_STUDENT_COMPETITION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M.L Competition Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frSDoMZ6nROc",
    "outputId": "2e41768c-89ed-4f1a-842a-d7102161648d"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    from google.colab import data_table\n",
    "    data_table.DataTable.max_columns = 50\n",
    "    data_table.enable_dataframe_formatter()\n",
    "else:\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "    InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "-6Meh_vIqdTe",
    "outputId": "ecb278f0-fb78-4ac1-bf9b-861c9cc3cd65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d6c562e2f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>493553</td>\n",
       "      <td>0.315471</td>\n",
       "      <td>-0.183690</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>-1.186794</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.946208</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.690715</td>\n",
       "      <td>-0.628005</td>\n",
       "      <td>-2.832295</td>\n",
       "      <td>-1.409039</td>\n",
       "      <td>3.645067</td>\n",
       "      <td>0.233039</td>\n",
       "      <td>-3.754846</td>\n",
       "      <td>-1.061733</td>\n",
       "      <td>BDBBCACIBB</td>\n",
       "      <td>20.308715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237346</td>\n",
       "      <td>-1.286392</td>\n",
       "      <td>1.780592</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>-2.690658</td>\n",
       "      <td>1.321997</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>0.371070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664517</td>\n",
       "      <td>-2.871912</td>\n",
       "      <td>3.826628</td>\n",
       "      <td>3.087653</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>3.210875</td>\n",
       "      <td>-0.666457</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>ACBDCBCADA</td>\n",
       "      <td>-449.291063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37368</td>\n",
       "      <td>-0.290303</td>\n",
       "      <td>-0.485907</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>1.083632</td>\n",
       "      <td>-1.129914</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.494988</td>\n",
       "      <td>-0.946303</td>\n",
       "      <td>2.333223</td>\n",
       "      <td>2.084169</td>\n",
       "      <td>-4.782668</td>\n",
       "      <td>-1.671375</td>\n",
       "      <td>2.774382</td>\n",
       "      <td>2.273130</td>\n",
       "      <td>AABBABCLAF</td>\n",
       "      <td>-86.206118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>665220</td>\n",
       "      <td>1.243590</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.854267</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>-2.020416</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.066427</td>\n",
       "      <td>-2.430158</td>\n",
       "      <td>-0.185332</td>\n",
       "      <td>-0.701691</td>\n",
       "      <td>-2.769142</td>\n",
       "      <td>-6.534231</td>\n",
       "      <td>-0.557677</td>\n",
       "      <td>-0.429972</td>\n",
       "      <td>ADBBABEEBA</td>\n",
       "      <td>-30.157403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41499</td>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-1.045483</td>\n",
       "      <td>0.718374</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>-0.936620</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.899984</td>\n",
       "      <td>1.427460</td>\n",
       "      <td>-4.992610</td>\n",
       "      <td>1.154162</td>\n",
       "      <td>-1.931443</td>\n",
       "      <td>2.325042</td>\n",
       "      <td>2.143811</td>\n",
       "      <td>-1.039599</td>\n",
       "      <td>ABBBBBCMBB</td>\n",
       "      <td>296.484562</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
       "0  493553  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208   \n",
       "1  237346 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894   \n",
       "2   37368 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914   \n",
       "3  665220  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892   \n",
       "4   41499  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451   \n",
       "\n",
       "       f_06  f_07  f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  \\\n",
       "0  0.729857     0     4     1     3     1     2     4     1     5     2     0   \n",
       "1  0.371070     3     0     3     3     5     3     2     0     1     6     0   \n",
       "2  0.767396     3     1     3     2     3     4     1     1     1     0     2   \n",
       "3 -2.020416     2     0     4     5     0     5     1     0     3     1     1   \n",
       "4 -0.936620     1     2     2     2     2     5     0     3     1     1     2   \n",
       "\n",
       "   f_18      f_19      f_20      f_21      f_22      f_23      f_24      f_25  \\\n",
       "0     1 -3.690715 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846   \n",
       "1     1  0.664517 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457   \n",
       "2     6 -0.494988 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382   \n",
       "3     3 -3.066427 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677   \n",
       "4     4 -1.899984  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811   \n",
       "\n",
       "       f_26        f_27        f_28  f_29  f_30  target  \n",
       "0 -1.061733  BDBBCACIBB   20.308715     1     0       0  \n",
       "1  0.123854  ACBDCBCADA -449.291063     1     0       0  \n",
       "2  2.273130  AABBABCLAF  -86.206118     0     1       1  \n",
       "3 -0.429972  ADBBABEEBA  -30.157403     0     2       1  \n",
       "4 -1.039599  ABBBBBCMBB  296.484562     0     2       1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = Path('/content/drive/MyDrive/train.csv') if IN_COLAB else Path('../Data/train.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlPl0dVDrByn",
    "outputId": "f7521bab-1e56-406e-cc68-be2e4b7dd827"
   },
   "outputs": [],
   "source": [
    "X, y = df.drop(['target', 'f_27', 'id'], axis=1), df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "26pKNWPurBwZ",
    "outputId": "d86646d1-3361-4e42-d119-a949102f62ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>-0.001910</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>-0.000971</td>\n",
       "      <td>2.032086</td>\n",
       "      <td>2.057980</td>\n",
       "      <td>2.362590</td>\n",
       "      <td>2.177847</td>\n",
       "      <td>1.804038</td>\n",
       "      <td>2.842322</td>\n",
       "      <td>2.239307</td>\n",
       "      <td>1.515411</td>\n",
       "      <td>2.101359</td>\n",
       "      <td>2.096502</td>\n",
       "      <td>1.857758</td>\n",
       "      <td>2.065975</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>-0.178796</td>\n",
       "      <td>-0.156877</td>\n",
       "      <td>-0.009749</td>\n",
       "      <td>-0.369114</td>\n",
       "      <td>-0.342708</td>\n",
       "      <td>0.175932</td>\n",
       "      <td>0.356640</td>\n",
       "      <td>-0.448086</td>\n",
       "      <td>0.345565</td>\n",
       "      <td>1.002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.998719</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>1.000892</td>\n",
       "      <td>1.000081</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999535</td>\n",
       "      <td>1.656749</td>\n",
       "      <td>1.590698</td>\n",
       "      <td>1.637473</td>\n",
       "      <td>1.645519</td>\n",
       "      <td>1.537589</td>\n",
       "      <td>1.762672</td>\n",
       "      <td>1.537712</td>\n",
       "      <td>1.359798</td>\n",
       "      <td>1.568952</td>\n",
       "      <td>1.559978</td>\n",
       "      <td>1.467507</td>\n",
       "      <td>1.565593</td>\n",
       "      <td>2.314858</td>\n",
       "      <td>2.400672</td>\n",
       "      <td>2.484487</td>\n",
       "      <td>2.450494</td>\n",
       "      <td>2.454113</td>\n",
       "      <td>2.387102</td>\n",
       "      <td>2.416753</td>\n",
       "      <td>2.476792</td>\n",
       "      <td>238.735832</td>\n",
       "      <td>0.475553</td>\n",
       "      <td>0.818851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.599856</td>\n",
       "      <td>-4.682199</td>\n",
       "      <td>-4.642676</td>\n",
       "      <td>-4.628484</td>\n",
       "      <td>-4.748501</td>\n",
       "      <td>-4.750214</td>\n",
       "      <td>-4.842919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-11.280941</td>\n",
       "      <td>-11.257917</td>\n",
       "      <td>-12.183785</td>\n",
       "      <td>-11.853530</td>\n",
       "      <td>-12.301097</td>\n",
       "      <td>-11.416189</td>\n",
       "      <td>-11.918306</td>\n",
       "      <td>-14.300577</td>\n",
       "      <td>-1229.753052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.674386</td>\n",
       "      <td>-0.674814</td>\n",
       "      <td>-0.674948</td>\n",
       "      <td>-0.676899</td>\n",
       "      <td>-0.676212</td>\n",
       "      <td>-0.672953</td>\n",
       "      <td>-0.675204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.236465</td>\n",
       "      <td>-1.804803</td>\n",
       "      <td>-1.819358</td>\n",
       "      <td>-1.645711</td>\n",
       "      <td>-2.019762</td>\n",
       "      <td>-1.956021</td>\n",
       "      <td>-1.440454</td>\n",
       "      <td>-1.262614</td>\n",
       "      <td>-159.456219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>-0.001907</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-0.001630</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.329103</td>\n",
       "      <td>-0.189919</td>\n",
       "      <td>-0.153055</td>\n",
       "      <td>0.030148</td>\n",
       "      <td>-0.390120</td>\n",
       "      <td>-0.342316</td>\n",
       "      <td>0.160621</td>\n",
       "      <td>0.405216</td>\n",
       "      <td>-0.573352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.674684</td>\n",
       "      <td>0.675737</td>\n",
       "      <td>0.676736</td>\n",
       "      <td>0.671851</td>\n",
       "      <td>0.672968</td>\n",
       "      <td>0.675826</td>\n",
       "      <td>0.673790</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.878621</td>\n",
       "      <td>1.444271</td>\n",
       "      <td>1.506658</td>\n",
       "      <td>1.660455</td>\n",
       "      <td>1.257267</td>\n",
       "      <td>1.266450</td>\n",
       "      <td>1.794780</td>\n",
       "      <td>2.028263</td>\n",
       "      <td>158.941960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.749301</td>\n",
       "      <td>4.815699</td>\n",
       "      <td>4.961982</td>\n",
       "      <td>4.454920</td>\n",
       "      <td>4.948983</td>\n",
       "      <td>4.971881</td>\n",
       "      <td>4.822668</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.108150</td>\n",
       "      <td>11.475325</td>\n",
       "      <td>12.029242</td>\n",
       "      <td>11.344080</td>\n",
       "      <td>12.247100</td>\n",
       "      <td>12.389844</td>\n",
       "      <td>12.529179</td>\n",
       "      <td>12.913041</td>\n",
       "      <td>1229.562577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f_00           f_01           f_02           f_03  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.000368       0.001564       0.000306      -0.001910   \n",
       "std         0.998719       0.999402       1.000892       1.000081   \n",
       "min        -4.599856      -4.682199      -4.642676      -4.628484   \n",
       "25%        -0.674386      -0.674814      -0.674948      -0.676899   \n",
       "50%         0.002005       0.002531       0.001148      -0.002556   \n",
       "75%         0.674684       0.675737       0.676736       0.671851   \n",
       "max         4.749301       4.815699       4.961982       4.454920   \n",
       "\n",
       "                f_04           f_05           f_06           f_07  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean       -0.000820       0.000644      -0.000971       2.032086   \n",
       "std         1.000000       0.999999       0.999535       1.656749   \n",
       "min        -4.748501      -4.750214      -4.842919       0.000000   \n",
       "25%        -0.676212      -0.672953      -0.675204       1.000000   \n",
       "50%        -0.001907       0.000071      -0.001630       2.000000   \n",
       "75%         0.672968       0.675826       0.673790       3.000000   \n",
       "max         4.948983       4.971881       4.822668      15.000000   \n",
       "\n",
       "                f_08           f_09           f_10           f_11  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        2.057980       2.362590       2.177847       1.804038   \n",
       "std         1.590698       1.637473       1.645519       1.537589   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         1.000000       1.000000       1.000000       1.000000   \n",
       "50%         2.000000       2.000000       2.000000       2.000000   \n",
       "75%         3.000000       3.000000       3.000000       3.000000   \n",
       "max        16.000000      14.000000      14.000000      13.000000   \n",
       "\n",
       "                f_12           f_13           f_14           f_15  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        2.842322       2.239307       1.515411       2.101359   \n",
       "std         1.762672       1.537712       1.359798       1.568952   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         2.000000       1.000000       0.000000       1.000000   \n",
       "50%         3.000000       2.000000       1.000000       2.000000   \n",
       "75%         4.000000       3.000000       2.000000       3.000000   \n",
       "max        16.000000      12.000000      14.000000      14.000000   \n",
       "\n",
       "                f_16           f_17           f_18           f_19  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        2.096502       1.857758       2.065975       0.307220   \n",
       "std         1.559978       1.467507       1.565593       2.314858   \n",
       "min         0.000000       0.000000       0.000000     -11.280941   \n",
       "25%         1.000000       1.000000       1.000000      -1.236465   \n",
       "50%         2.000000       2.000000       2.000000       0.329103   \n",
       "75%         3.000000       3.000000       3.000000       1.878621   \n",
       "max        15.000000      12.000000      13.000000      11.108150   \n",
       "\n",
       "                f_20           f_21           f_22           f_23  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean       -0.178796      -0.156877      -0.009749      -0.369114   \n",
       "std         2.400672       2.484487       2.450494       2.454113   \n",
       "min       -11.257917     -12.183785     -11.853530     -12.301097   \n",
       "25%        -1.804803      -1.819358      -1.645711      -2.019762   \n",
       "50%        -0.189919      -0.153055       0.030148      -0.390120   \n",
       "75%         1.444271       1.506658       1.660455       1.257267   \n",
       "max        11.475325      12.029242      11.344080      12.247100   \n",
       "\n",
       "                f_24           f_25           f_26           f_28  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean       -0.342708       0.175932       0.356640      -0.448086   \n",
       "std         2.387102       2.416753       2.476792     238.735832   \n",
       "min       -11.416189     -11.918306     -14.300577   -1229.753052   \n",
       "25%        -1.956021      -1.440454      -1.262614    -159.456219   \n",
       "50%        -0.342316       0.160621       0.405216      -0.573352   \n",
       "75%         1.266450       1.794780       2.028263     158.941960   \n",
       "max        12.389844      12.529179      12.913041    1229.562577   \n",
       "\n",
       "                f_29           f_30  \n",
       "count  810000.000000  810000.000000  \n",
       "mean        0.345565       1.002373  \n",
       "std         0.475553       0.818851  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       1.000000  \n",
       "75%         1.000000       2.000000  \n",
       "max         1.000000       2.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmznjs5grBnV",
    "outputId": "c7388ce9-7b1c-4dfc-c48a-4b832222cc6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    415945\n",
       "1    394055\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nc_P-uEg4BZ",
    "outputId": "da2894c5-9823-461f-ec2f-33f130e7e10b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6xuWymPQrBd_"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features=30):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x)\n",
    "\n",
    "# Define a custom dataset by extending the PyTorch Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        return item, label\n",
    "    \n",
    "def datasets(dataset, stratify, test_size):\n",
    "    train_size = 1 - test_size\n",
    "\n",
    "    # Stratify by label\n",
    "    labels = np.array(stratify)\n",
    "    positive_indices = np.where(labels == 1)[0]\n",
    "    negative_indices = np.where(labels == 0)[0]\n",
    "\n",
    "    positive_split = int(train_size * len(positive_indices))\n",
    "    negative_split = int(train_size * len(negative_indices))\n",
    "\n",
    "    positive_train_indices = positive_indices[:positive_split]\n",
    "    positive_test_indices = positive_indices[positive_split:]\n",
    "    negative_train_indices = negative_indices[:negative_split]\n",
    "    negative_test_indices = negative_indices[negative_split:]\n",
    "\n",
    "    train_indices = np.concatenate([positive_train_indices, negative_train_indices])\n",
    "    test_indices = np.concatenate([positive_test_indices, negative_test_indices])\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ceGWFcgVykIZ"
   },
   "outputs": [],
   "source": [
    "# Declare the hole dataset\n",
    "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
    "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
    "X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([810000, 30]), torch.Size([810000]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor = (X_tensor - X_tensor.min()) / (X_tensor.max() - X_tensor.min())\n",
    "X_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5LkKFcbPwlHM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(688499, 121501)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "KUnpQLqcwylO"
   },
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "def checkpoint(epoch, model, optimizer, criteria, save_as: Path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criteria': criteria,\n",
    "    }, save_as)\n",
    "    \n",
    "\n",
    "# Validate\n",
    "def validation(model, device, valid_loader, criteria):\n",
    "    # Settings\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    accuracy_total = 0\n",
    "\n",
    "    # Test validation data\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss_total += loss.item() * inputs.size(0)\n",
    "            matches = torch.round(torch.sigmoid(outputs)) == labels\n",
    "            accuracy_total += (torch.sum(matches.to(torch.int32))).item()\n",
    "                        \n",
    "    return (loss_total / len(valid_loader), accuracy_total / len(valid_loader.dataset))\n",
    "\n",
    "\n",
    "# Train\n",
    "def train(device, model, epochs, optimizer, criteria, train_loader, valid_loader, resume=1, save_as=Path('../Output/model.pth')):\n",
    "    # Early stopping\n",
    "    last_loss = 100\n",
    "    patience = 3\n",
    "    trigger_times = 0\n",
    "    \n",
    "    train_loss_track, train_accuracy_track = [], []\n",
    "    test_loss_track, test_accuracy_track = [], []\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(resume, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        for times, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward propagation\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Train loss and train accuracy\n",
    "        train_current_loss, train_current_accuracy = validation(model, device, train_loader, criteria)\n",
    "\n",
    "        # Early stopping\n",
    "        current_loss, current_accuracy = validation(model, device, valid_loader, criteria)\n",
    "        print('[{}/{} | Train loss: {:.8} | Train accuracy: {:.8} | Test loss: {:.8} | Test accuracy: {:.8}]'.format(epoch, epochs, train_current_loss, train_current_accuracy, current_loss, current_accuracy))\n",
    "        \n",
    "        train_loss_track.append(train_current_loss)\n",
    "        train_accuracy_track.append(train_current_accuracy)\n",
    "        test_loss_track.append(current_loss)\n",
    "        test_accuracy_track.append(current_accuracy)\n",
    "        \n",
    "        if current_loss > last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger times:', trigger_times)\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model\n",
    "\n",
    "        else:\n",
    "            print('Trigger times: 0')\n",
    "            trigger_times = 0\n",
    "\n",
    "        last_loss = current_loss\n",
    "        \n",
    "        if trigger_times == 0:\n",
    "            print('New checkpoint...')\n",
    "            checkpoint(epoch, model, optimizer, criteria, save_as=save_as)\n",
    "\n",
    "    return model, (train_loss_track, train_accuracy_track), (test_loss_track, test_accuracy_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sZnDhkhlwyiV",
    "outputId": "d5c2646c-1dff-41af-f336-44eda69a44f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for new training process... =)\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_features=X_tensor.shape[1])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "resume = 1\n",
    "v0_path = Path('../Output/NetV0.pth')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "recover = False\n",
    "\n",
    "if v0_path.exists() and recover:\n",
    "    print('Recovering old training process...')\n",
    "    checkpoint = torch.load(v0_path)\n",
    "    resume = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criteria = checkpoint['criteria']\n",
    "else:\n",
    "    print('Ready for new training process... =)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "iekQga58wybC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not training this model anymore...\n"
     ]
    }
   ],
   "source": [
    "wanna_train = False\n",
    "if wanna_train:\n",
    "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v0_path)\n",
    "else:\n",
    "    print('Not training this model anymore...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "j44nOD2HwyYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying last checkpoint of model NetV0 state to trained folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('../Trained/NetV0.pth')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "v0_path_trained = Path('../Trained/NetV0.pth')\n",
    "if v0_path.exists():\n",
    "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV0'))\n",
    "    shutil.copy(v0_path, v0_path_trained)\n",
    "else:\n",
    "    print('Model {0} is not created yeat'.format('NetV0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode f_27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the way to go because the dataframe increases a lot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 31), (810000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678113, 810000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_27 = X['f_27']\n",
    "f_27.nunique(), len(f_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 678112), scipy.sparse._csr.csr_matrix)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X['f_27'] = X['f_27'].astype('category')\n",
    "drop_enc = OneHotEncoder(drop='first')\n",
    "sparse = drop_enc.fit_transform(X[['f_27']])\n",
    "sparse.shape, type(sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimentionally reduction doesn't work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TruncatedSVD(n_components=10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.47292388e-05 1.33993433e-05 1.17483190e-05 1.14381288e-05\n",
      " 1.05022168e-05 1.01821724e-05 9.86606175e-06 9.74468205e-06\n",
      " 9.52573634e-06 8.92134886e-06]\n",
      "0.00011005724817312332\n",
      "[3.44591513 3.27903335 3.04689248 2.99168421 2.88523113 2.82947893\n",
      " 2.78254694 2.75723908 2.72105923 2.65208615]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "pca = TruncatedSVD(n_components=10)\n",
    "pca.fit(sparse)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusterization doesnt perform good either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements asigned to each cluster: [810000]\n",
      "Number of elements asigned to each cluster: [809996      4]\n",
      "Number of elements asigned to each cluster: [809994      4      2]\n",
      "Number of elements asigned to each cluster: [809993      4      2      1]\n",
      "Number of elements asigned to each cluster: [809990      4      2      1      3]\n",
      "Number of elements asigned to each cluster: [809988      4      2      1      3      2]\n",
      "Number of elements asigned to each cluster: [809984      4      2      1      3      2      4]\n",
      "Number of elements asigned to each cluster: [809980      4      2      3      2      4      3      2]\n",
      "Number of elements asigned to each cluster: [809979      4      2      3      2      4      3      2      1]\n",
      "Number of elements asigned to each cluster: [809978      4      2      3      2      4      3      2      1      1]\n",
      "\n",
      "True number of documents in each category according to the class labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for c in range(1, 11):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=c,\n",
    "        max_iter=300,\n",
    "        n_init='auto',\n",
    "        random_state=42,\n",
    "    ).fit(sparse)\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "    print(f\"Number of elements asigned to each cluster: {cluster_sizes}\")\n",
    "print()\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{cluster_ids}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart encoding f_27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "df['f_27'] = df['f_27'].str.upper()\n",
    "df['length'] = df['f_27'].str.len()\n",
    "print(df['length'].min(), df['length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 32), (810000,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>length</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>pos_0</th>\n",
       "      <th>pos_1</th>\n",
       "      <th>pos_2</th>\n",
       "      <th>pos_3</th>\n",
       "      <th>pos_4</th>\n",
       "      <th>pos_5</th>\n",
       "      <th>pos_6</th>\n",
       "      <th>pos_7</th>\n",
       "      <th>pos_8</th>\n",
       "      <th>pos_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>-0.183690</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>-1.186794</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.946208</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.690715</td>\n",
       "      <td>-0.628005</td>\n",
       "      <td>-2.832295</td>\n",
       "      <td>-1.409039</td>\n",
       "      <td>3.645067</td>\n",
       "      <td>0.233039</td>\n",
       "      <td>-3.754846</td>\n",
       "      <td>-1.061733</td>\n",
       "      <td>BDBBCACIBB</td>\n",
       "      <td>20.308715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.286392</td>\n",
       "      <td>1.780592</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>-2.690658</td>\n",
       "      <td>1.321997</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>0.371070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664517</td>\n",
       "      <td>-2.871912</td>\n",
       "      <td>3.826628</td>\n",
       "      <td>3.087653</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>3.210875</td>\n",
       "      <td>-0.666457</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>ACBDCBCADA</td>\n",
       "      <td>-449.291063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.290303</td>\n",
       "      <td>-0.485907</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>1.083632</td>\n",
       "      <td>-1.129914</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.494988</td>\n",
       "      <td>-0.946303</td>\n",
       "      <td>2.333223</td>\n",
       "      <td>2.084169</td>\n",
       "      <td>-4.782668</td>\n",
       "      <td>-1.671375</td>\n",
       "      <td>2.774382</td>\n",
       "      <td>2.273130</td>\n",
       "      <td>AABBABCLAF</td>\n",
       "      <td>-86.206118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.243590</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.854267</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>-2.020416</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.066427</td>\n",
       "      <td>-2.430158</td>\n",
       "      <td>-0.185332</td>\n",
       "      <td>-0.701691</td>\n",
       "      <td>-2.769142</td>\n",
       "      <td>-6.534231</td>\n",
       "      <td>-0.557677</td>\n",
       "      <td>-0.429972</td>\n",
       "      <td>ADBBABEEBA</td>\n",
       "      <td>-30.157403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-1.045483</td>\n",
       "      <td>0.718374</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>-0.936620</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.899984</td>\n",
       "      <td>1.427460</td>\n",
       "      <td>-4.992610</td>\n",
       "      <td>1.154162</td>\n",
       "      <td>-1.931443</td>\n",
       "      <td>2.325042</td>\n",
       "      <td>2.143811</td>\n",
       "      <td>-1.039599</td>\n",
       "      <td>ABBBBBCMBB</td>\n",
       "      <td>296.484562</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
       "0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
       "1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
       "2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
       "3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
       "4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
       "\n",
       "   f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
       "0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
       "1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
       "2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
       "3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
       "4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
       "\n",
       "       f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
       "0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
       "1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
       "2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
       "3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
       "4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
       "\n",
       "         f_27        f_28  f_29  f_30  length  A  B  C  D  E  F  G  H  I  J  \\\n",
       "0  BDBBCACIBB   20.308715     1     0      10  1  5  2  1  0  0  0  0  1  0   \n",
       "1  ACBDCBCADA -449.291063     1     0      10  3  2  3  2  0  0  0  0  0  0   \n",
       "2  AABBABCLAF  -86.206118     0     1      10  4  3  1  0  0  1  0  0  0  0   \n",
       "3  ADBBABEEBA  -30.157403     0     2      10  3  4  0  1  2  0  0  0  0  0   \n",
       "4  ABBBBBCMBB  296.484562     0     2      10  1  7  1  0  0  0  0  0  0  0   \n",
       "\n",
       "   K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z  pos_0  pos_1  pos_2  pos_3  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      1      3      1      1   \n",
       "1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      2      1      3   \n",
       "2  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      0      1      1   \n",
       "3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      3      1      1   \n",
       "4  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0      0      1      1      1   \n",
       "\n",
       "   pos_4  pos_5  pos_6  pos_7  pos_8  pos_9  \n",
       "0      2      0      2      8      1      1  \n",
       "1      2      1      2      0      3      0  \n",
       "2      0      1      2     11      0      5  \n",
       "3      0      1      4      4      1      0  \n",
       "4      1      1      2     12      1      1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts the number of times a letter appear in the code\n",
    "def add_letters_count(data):\n",
    "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for char in letters:\n",
    "        data[char] = data['f_27'].str.count(char)\n",
    "    return data\n",
    "\n",
    "# For each of the 10 position of the letter code I assign which code letter was assigned to it\n",
    "def add_letter_position(data):\n",
    "    for i in range(10):\n",
    "        data['pos_' + str(i)] = (data['f_27'].str[i]).apply(lambda x: ord(x)) - 65\n",
    "    return data\n",
    "\n",
    "X = add_letters_count(X)\n",
    "X = add_letter_position(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>pos_0</th>\n",
       "      <th>pos_1</th>\n",
       "      <th>pos_2</th>\n",
       "      <th>pos_3</th>\n",
       "      <th>pos_4</th>\n",
       "      <th>pos_5</th>\n",
       "      <th>pos_6</th>\n",
       "      <th>pos_7</th>\n",
       "      <th>pos_8</th>\n",
       "      <th>pos_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>-0.183690</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>-1.186794</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.946208</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.690715</td>\n",
       "      <td>-0.628005</td>\n",
       "      <td>-2.832295</td>\n",
       "      <td>-1.409039</td>\n",
       "      <td>3.645067</td>\n",
       "      <td>0.233039</td>\n",
       "      <td>-3.754846</td>\n",
       "      <td>-1.061733</td>\n",
       "      <td>20.308715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.286392</td>\n",
       "      <td>1.780592</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>-2.690658</td>\n",
       "      <td>1.321997</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>0.371070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664517</td>\n",
       "      <td>-2.871912</td>\n",
       "      <td>3.826628</td>\n",
       "      <td>3.087653</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>3.210875</td>\n",
       "      <td>-0.666457</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>-449.291063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.290303</td>\n",
       "      <td>-0.485907</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>1.083632</td>\n",
       "      <td>-1.129914</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.494988</td>\n",
       "      <td>-0.946303</td>\n",
       "      <td>2.333223</td>\n",
       "      <td>2.084169</td>\n",
       "      <td>-4.782668</td>\n",
       "      <td>-1.671375</td>\n",
       "      <td>2.774382</td>\n",
       "      <td>2.273130</td>\n",
       "      <td>-86.206118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.243590</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.854267</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>-2.020416</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.066427</td>\n",
       "      <td>-2.430158</td>\n",
       "      <td>-0.185332</td>\n",
       "      <td>-0.701691</td>\n",
       "      <td>-2.769142</td>\n",
       "      <td>-6.534231</td>\n",
       "      <td>-0.557677</td>\n",
       "      <td>-0.429972</td>\n",
       "      <td>-30.157403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-1.045483</td>\n",
       "      <td>0.718374</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>-0.936620</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.899984</td>\n",
       "      <td>1.427460</td>\n",
       "      <td>-4.992610</td>\n",
       "      <td>1.154162</td>\n",
       "      <td>-1.931443</td>\n",
       "      <td>2.325042</td>\n",
       "      <td>2.143811</td>\n",
       "      <td>-1.039599</td>\n",
       "      <td>296.484562</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
       "0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
       "1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
       "2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
       "3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
       "4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
       "\n",
       "   f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
       "0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
       "1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
       "2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
       "3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
       "4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
       "\n",
       "       f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
       "0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
       "1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
       "2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
       "3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
       "4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
       "\n",
       "         f_28  f_29  f_30  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  \\\n",
       "0   20.308715     1     0  1  5  2  1  0  0  0  0  1  0  0  0  0  0  0  0  0   \n",
       "1 -449.291063     1     0  3  2  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2  -86.206118     0     1  4  3  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0   \n",
       "3  -30.157403     0     2  3  4  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "4  296.484562     0     2  1  7  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0   \n",
       "\n",
       "   R  S  T  U  V  W  X  Y  Z  pos_0  pos_1  pos_2  pos_3  pos_4  pos_5  pos_6  \\\n",
       "0  0  0  0  0  0  0  0  0  0      1      3      1      1      2      0      2   \n",
       "1  0  0  0  0  0  0  0  0  0      0      2      1      3      2      1      2   \n",
       "2  0  0  0  0  0  0  0  0  0      0      0      1      1      0      1      2   \n",
       "3  0  0  0  0  0  0  0  0  0      0      3      1      1      0      1      4   \n",
       "4  0  0  0  0  0  0  0  0  0      0      1      1      1      1      1      2   \n",
       "\n",
       "   pos_7  pos_8  pos_9  \n",
       "0      8      1      1  \n",
       "1      0      3      0  \n",
       "2     11      0      5  \n",
       "3      4      1      0  \n",
       "4     12      1      1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops unneded features\n",
    "X = X.drop(['f_27', 'length'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3155, -0.1837,  0.6644,  ...,  8.0000,  1.0000,  1.0000],\n",
       "        [-1.2864,  1.7806,  0.5767,  ...,  0.0000,  3.0000,  0.0000],\n",
       "        [-0.2903, -0.4859,  0.8084,  ..., 11.0000,  0.0000,  5.0000],\n",
       "        ...,\n",
       "        [-0.2353, -0.3175, -0.3885,  ..., 11.0000,  3.0000,  3.0000],\n",
       "        [-0.9948, -0.2667,  1.9090,  ..., 13.0000,  1.0000,  2.0000],\n",
       "        [-0.2540,  1.2699,  1.9680,  ..., 17.0000,  1.0000,  2.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the hole dataset\n",
    "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
    "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
    "X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
    "X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "7QqrCvTnwyT_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810000, 66) (810000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5002, 0.5000, 0.5003,  ..., 0.5033, 0.5004, 0.5004],\n",
       "        [0.4995, 0.5008, 0.5003,  ..., 0.5000, 0.5013, 0.5000],\n",
       "        [0.4999, 0.4998, 0.5004,  ..., 0.5045, 0.5000, 0.5021],\n",
       "        ...,\n",
       "        [0.4999, 0.4999, 0.4999,  ..., 0.5045, 0.5013, 0.5013],\n",
       "        [0.4996, 0.4999, 0.5008,  ..., 0.5053, 0.5004, 0.5009],\n",
       "        [0.4999, 0.5006, 0.5008,  ..., 0.5070, 0.5004, 0.5009]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor = (X_tensor - X_tensor.min()) / (X_tensor.max() - X_tensor.min())\n",
    "print(X.shape, y.shape)\n",
    "X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "fDII01Oiwx6m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for new training process... =)\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
    "\n",
    "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = Net(input_features=X_tensor.shape[1])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "resume = 1\n",
    "v1_path = Path('../Output/NetV1.pth')\n",
    "\n",
    "recover = False\n",
    "\n",
    "if v_path.exists() and recover:\n",
    "    print('Recovering old training process...')\n",
    "    checkpoint = torch.load(v1_path)\n",
    "    resume = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criteria = checkpoint['criteria']\n",
    "else:\n",
    "    print('Ready for new training process... =)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100 | Train loss: 177.31987 | Train accuracy: 0.51351273 | Test loss: 177.21183 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n"
     ]
    }
   ],
   "source": [
    "wanna_train = True\n",
    "if wanna_train:\n",
    "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v1_path)\n",
    "else:\n",
    "    print('Not training this model anymore...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "v1_path_trained = Path('../Trained/NetV1.pth')\n",
    "if v0_path.exists():\n",
    "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV1'))\n",
    "    shutil.copy(v1_path, v1_path_trained)\n",
    "else:\n",
    "    print('Model {0} is not created yeat'.format('NetV1'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO1PZIgcCRT/kG8sHY+B9DH",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
