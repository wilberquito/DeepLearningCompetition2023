{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wilberquito/NeuralNetworksCompetition2023/blob/main/Code/ML_STUDENT_COMPETITION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frSDoMZ6nROc",
    "outputId": "2e41768c-89ed-4f1a-842a-d7102161648d"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    from google.colab import data_table\n",
    "    data_table.DataTable.max_columns = 50\n",
    "    data_table.enable_dataframe_formatter()\n",
    "else:\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "    InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "-6Meh_vIqdTe",
    "outputId": "ecb278f0-fb78-4ac1-bf9b-861c9cc3cd65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4170abe6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>493553</td>\n",
       "      <td>0.315471</td>\n",
       "      <td>-0.183690</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>-1.186794</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.946208</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.690715</td>\n",
       "      <td>-0.628005</td>\n",
       "      <td>-2.832295</td>\n",
       "      <td>-1.409039</td>\n",
       "      <td>3.645067</td>\n",
       "      <td>0.233039</td>\n",
       "      <td>-3.754846</td>\n",
       "      <td>-1.061733</td>\n",
       "      <td>BDBBCACIBB</td>\n",
       "      <td>20.308715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237346</td>\n",
       "      <td>-1.286392</td>\n",
       "      <td>1.780592</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>-2.690658</td>\n",
       "      <td>1.321997</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>0.371070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664517</td>\n",
       "      <td>-2.871912</td>\n",
       "      <td>3.826628</td>\n",
       "      <td>3.087653</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>3.210875</td>\n",
       "      <td>-0.666457</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>ACBDCBCADA</td>\n",
       "      <td>-449.291063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37368</td>\n",
       "      <td>-0.290303</td>\n",
       "      <td>-0.485907</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>1.083632</td>\n",
       "      <td>-1.129914</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.494988</td>\n",
       "      <td>-0.946303</td>\n",
       "      <td>2.333223</td>\n",
       "      <td>2.084169</td>\n",
       "      <td>-4.782668</td>\n",
       "      <td>-1.671375</td>\n",
       "      <td>2.774382</td>\n",
       "      <td>2.273130</td>\n",
       "      <td>AABBABCLAF</td>\n",
       "      <td>-86.206118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>665220</td>\n",
       "      <td>1.243590</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.854267</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>-2.020416</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.066427</td>\n",
       "      <td>-2.430158</td>\n",
       "      <td>-0.185332</td>\n",
       "      <td>-0.701691</td>\n",
       "      <td>-2.769142</td>\n",
       "      <td>-6.534231</td>\n",
       "      <td>-0.557677</td>\n",
       "      <td>-0.429972</td>\n",
       "      <td>ADBBABEEBA</td>\n",
       "      <td>-30.157403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41499</td>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-1.045483</td>\n",
       "      <td>0.718374</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>-0.936620</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.899984</td>\n",
       "      <td>1.427460</td>\n",
       "      <td>-4.992610</td>\n",
       "      <td>1.154162</td>\n",
       "      <td>-1.931443</td>\n",
       "      <td>2.325042</td>\n",
       "      <td>2.143811</td>\n",
       "      <td>-1.039599</td>\n",
       "      <td>ABBBBBCMBB</td>\n",
       "      <td>296.484562</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
       "0  493553  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208   \n",
       "1  237346 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894   \n",
       "2   37368 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914   \n",
       "3  665220  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892   \n",
       "4   41499  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451   \n",
       "\n",
       "       f_06  f_07  f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  \\\n",
       "0  0.729857     0     4     1     3     1     2     4     1     5     2     0   \n",
       "1  0.371070     3     0     3     3     5     3     2     0     1     6     0   \n",
       "2  0.767396     3     1     3     2     3     4     1     1     1     0     2   \n",
       "3 -2.020416     2     0     4     5     0     5     1     0     3     1     1   \n",
       "4 -0.936620     1     2     2     2     2     5     0     3     1     1     2   \n",
       "\n",
       "   f_18      f_19      f_20      f_21      f_22      f_23      f_24      f_25  \\\n",
       "0     1 -3.690715 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846   \n",
       "1     1  0.664517 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457   \n",
       "2     6 -0.494988 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382   \n",
       "3     3 -3.066427 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677   \n",
       "4     4 -1.899984  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811   \n",
       "\n",
       "       f_26        f_27        f_28  f_29  f_30  target  \n",
       "0 -1.061733  BDBBCACIBB   20.308715     1     0       0  \n",
       "1  0.123854  ACBDCBCADA -449.291063     1     0       0  \n",
       "2  2.273130  AABBABCLAF  -86.206118     0     1       1  \n",
       "3 -0.429972  ADBBABEEBA  -30.157403     0     2       1  \n",
       "4 -1.039599  ABBBBBCMBB  296.484562     0     2       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = Path('/content/drive/MyDrive/train.csv') if IN_COLAB else Path('../Data/train.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlPl0dVDrByn",
    "outputId": "f7521bab-1e56-406e-cc68-be2e4b7dd827"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 30), (810000,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop(['target', 'f_27', 'id'], axis=1), df['target']\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "26pKNWPurBwZ",
    "outputId": "d86646d1-3361-4e42-d119-a949102f62ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.492047</td>\n",
       "      <td>0.493137</td>\n",
       "      <td>0.483409</td>\n",
       "      <td>0.509344</td>\n",
       "      <td>0.489579</td>\n",
       "      <td>0.488666</td>\n",
       "      <td>0.500947</td>\n",
       "      <td>0.135472</td>\n",
       "      <td>0.128624</td>\n",
       "      <td>0.168756</td>\n",
       "      <td>0.155560</td>\n",
       "      <td>0.138772</td>\n",
       "      <td>0.177645</td>\n",
       "      <td>0.186609</td>\n",
       "      <td>0.108244</td>\n",
       "      <td>0.150097</td>\n",
       "      <td>0.139767</td>\n",
       "      <td>0.154813</td>\n",
       "      <td>0.158921</td>\n",
       "      <td>0.517581</td>\n",
       "      <td>0.487353</td>\n",
       "      <td>0.496712</td>\n",
       "      <td>0.510560</td>\n",
       "      <td>0.486064</td>\n",
       "      <td>0.465154</td>\n",
       "      <td>0.494703</td>\n",
       "      <td>0.538599</td>\n",
       "      <td>0.499857</td>\n",
       "      <td>0.345565</td>\n",
       "      <td>0.501186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.106824</td>\n",
       "      <td>0.105223</td>\n",
       "      <td>0.104209</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.103120</td>\n",
       "      <td>0.102858</td>\n",
       "      <td>0.103412</td>\n",
       "      <td>0.110450</td>\n",
       "      <td>0.099419</td>\n",
       "      <td>0.116962</td>\n",
       "      <td>0.117537</td>\n",
       "      <td>0.118276</td>\n",
       "      <td>0.110167</td>\n",
       "      <td>0.128143</td>\n",
       "      <td>0.097128</td>\n",
       "      <td>0.112068</td>\n",
       "      <td>0.103999</td>\n",
       "      <td>0.122292</td>\n",
       "      <td>0.120430</td>\n",
       "      <td>0.103392</td>\n",
       "      <td>0.105602</td>\n",
       "      <td>0.102610</td>\n",
       "      <td>0.105636</td>\n",
       "      <td>0.099971</td>\n",
       "      <td>0.100273</td>\n",
       "      <td>0.098855</td>\n",
       "      <td>0.091013</td>\n",
       "      <td>0.097074</td>\n",
       "      <td>0.475553</td>\n",
       "      <td>0.409425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.419874</td>\n",
       "      <td>0.421923</td>\n",
       "      <td>0.413105</td>\n",
       "      <td>0.435033</td>\n",
       "      <td>0.419932</td>\n",
       "      <td>0.419381</td>\n",
       "      <td>0.431191</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.448633</td>\n",
       "      <td>0.415828</td>\n",
       "      <td>0.428052</td>\n",
       "      <td>0.440037</td>\n",
       "      <td>0.418822</td>\n",
       "      <td>0.397385</td>\n",
       "      <td>0.428586</td>\n",
       "      <td>0.479097</td>\n",
       "      <td>0.435201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.492222</td>\n",
       "      <td>0.493239</td>\n",
       "      <td>0.483497</td>\n",
       "      <td>0.509273</td>\n",
       "      <td>0.489467</td>\n",
       "      <td>0.488607</td>\n",
       "      <td>0.500879</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.518558</td>\n",
       "      <td>0.486864</td>\n",
       "      <td>0.496870</td>\n",
       "      <td>0.512280</td>\n",
       "      <td>0.485208</td>\n",
       "      <td>0.465171</td>\n",
       "      <td>0.494076</td>\n",
       "      <td>0.540384</td>\n",
       "      <td>0.499806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.564173</td>\n",
       "      <td>0.564118</td>\n",
       "      <td>0.553837</td>\n",
       "      <td>0.583519</td>\n",
       "      <td>0.559059</td>\n",
       "      <td>0.558114</td>\n",
       "      <td>0.570758</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.587767</td>\n",
       "      <td>0.558750</td>\n",
       "      <td>0.565416</td>\n",
       "      <td>0.582559</td>\n",
       "      <td>0.552316</td>\n",
       "      <td>0.532749</td>\n",
       "      <td>0.560920</td>\n",
       "      <td>0.600025</td>\n",
       "      <td>0.564667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f_00           f_01           f_02           f_03  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.492047       0.493137       0.483409       0.509344   \n",
       "std         0.106824       0.105223       0.104209       0.110100   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.419874       0.421923       0.413105       0.435033   \n",
       "50%         0.492222       0.493239       0.483497       0.509273   \n",
       "75%         0.564173       0.564118       0.553837       0.583519   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                f_04           f_05           f_06           f_07  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.489579       0.488666       0.500947       0.135472   \n",
       "std         0.103120       0.102858       0.103412       0.110450   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.419932       0.419381       0.431191       0.066667   \n",
       "50%         0.489467       0.488607       0.500879       0.133333   \n",
       "75%         0.559059       0.558114       0.570758       0.200000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                f_08           f_09           f_10           f_11  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.128624       0.168756       0.155560       0.138772   \n",
       "std         0.099419       0.116962       0.117537       0.118276   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.062500       0.071429       0.071429       0.076923   \n",
       "50%         0.125000       0.142857       0.142857       0.153846   \n",
       "75%         0.187500       0.214286       0.214286       0.230769   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                f_12           f_13           f_14           f_15  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.177645       0.186609       0.108244       0.150097   \n",
       "std         0.110167       0.128143       0.097128       0.112068   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.125000       0.083333       0.000000       0.071429   \n",
       "50%         0.187500       0.166667       0.071429       0.142857   \n",
       "75%         0.250000       0.250000       0.142857       0.214286   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                f_16           f_17           f_18           f_19  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.139767       0.154813       0.158921       0.517581   \n",
       "std         0.103999       0.122292       0.120430       0.103392   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.066667       0.083333       0.076923       0.448633   \n",
       "50%         0.133333       0.166667       0.153846       0.518558   \n",
       "75%         0.200000       0.250000       0.230769       0.587767   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                f_20           f_21           f_22           f_23  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.487353       0.496712       0.510560       0.486064   \n",
       "std         0.105602       0.102610       0.105636       0.099971   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.415828       0.428052       0.440037       0.418822   \n",
       "50%         0.486864       0.496870       0.512280       0.485208   \n",
       "75%         0.558750       0.565416       0.582559       0.552316   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                f_24           f_25           f_26           f_28  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.465154       0.494703       0.538599       0.499857   \n",
       "std         0.100273       0.098855       0.091013       0.097074   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.397385       0.428586       0.479097       0.435201   \n",
       "50%         0.465171       0.494076       0.540384       0.499806   \n",
       "75%         0.532749       0.560920       0.600025       0.564667   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                f_29           f_30  \n",
       "count  810000.000000  810000.000000  \n",
       "mean        0.345565       0.501186  \n",
       "std         0.475553       0.409425  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.500000  \n",
       "75%         1.000000       1.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmznjs5grBnV",
    "outputId": "c7388ce9-7b1c-4dfc-c48a-4b832222cc6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    415945\n",
       "1    394055\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nc_P-uEg4BZ",
    "outputId": "da2894c5-9823-461f-ec2f-33f130e7e10b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "6xuWymPQrBd_"
   },
   "outputs": [],
   "source": [
    "class NetV0(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features=30):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x)\n",
    "\n",
    "# Define a custom dataset by extending the PyTorch Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        return item, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "ceGWFcgVykIZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810000"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the hole dataset\n",
    "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
    "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
    "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "5LkKFcbPwlHM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(688499, 121501)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the train and test dataset\n",
    "import numpy as np\n",
    "\n",
    "test_size = 0.15\n",
    "train_size = 1 - test_size\n",
    "\n",
    "# Stratify by label\n",
    "labels = np.array(y)\n",
    "positive_indices = np.where(labels == 1)[0]\n",
    "negative_indices = np.where(labels == 0)[0]\n",
    "\n",
    "positive_split = int(train_size * len(positive_indices))\n",
    "negative_split = int(train_size * len(negative_indices))\n",
    "\n",
    "positive_train_indices = positive_indices[:positive_split]\n",
    "positive_test_indices = positive_indices[positive_split:]\n",
    "negative_train_indices = negative_indices[:negative_split]\n",
    "negative_test_indices = negative_indices[negative_split:]\n",
    "\n",
    "train_indices = np.concatenate([positive_train_indices, negative_train_indices])\n",
    "test_indices = np.concatenate([positive_test_indices, negative_test_indices])\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "KUnpQLqcwylO"
   },
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "def checkpoint(epoch, model, optimizer, criteria, save_as: Path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criteria': criteria,\n",
    "    }, save_as)\n",
    "    \n",
    "\n",
    "# Validate\n",
    "def validation(model, device, valid_loader, criteria):\n",
    "    # Settings\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    accuracy_total = 0\n",
    "\n",
    "    # Test validation data\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss_total += loss.item() * inputs.size(0)\n",
    "            matches = torch.round(torch.sigmoid(outputs)) == labels\n",
    "            accuracy_total += (torch.sum(matches.to(torch.int32))).item()\n",
    "                        \n",
    "    return (loss_total / len(valid_loader), accuracy_total / len(valid_loader.dataset))\n",
    "\n",
    "\n",
    "# Train\n",
    "def train(device, model, epochs, optimizer, criteria, train_loader, valid_loader, resume=1, save_as=Path('../Output/model.pth')):\n",
    "    # Early stopping\n",
    "    last_loss = 100\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "    \n",
    "    train_loss_track, train_accuracy_track = [], []\n",
    "    test_loss_track, test_accuracy_track = [], []\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(resume, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        for times, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward propagation\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Train loss and train accuracy\n",
    "        train_current_loss, train_current_accuracy = validation(model, device, train_loader, criteria)\n",
    "\n",
    "        # Early stopping\n",
    "        current_loss, current_accuracy = validation(model, device, valid_loader, criteria)\n",
    "        print('[{}/{} | Train loss: {:.8} | Train accuracy: {:.8} | Test loss: {:.8} | Test accuracy: {:.8}]'.format(epoch, epochs, train_current_loss, train_current_accuracy, current_loss, current_accuracy))\n",
    "        \n",
    "        train_loss_track.append(train_current_loss)\n",
    "        train_accuracy_track.append(train_current_accuracy)\n",
    "        test_loss_track.append(current_loss)\n",
    "        test_accuracy_track.append(current_accuracy)\n",
    "        \n",
    "        if current_loss > last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger times:', trigger_times)\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model\n",
    "\n",
    "        else:\n",
    "            print('Trigger times: 0')\n",
    "            trigger_times = 0\n",
    "\n",
    "        last_loss = current_loss\n",
    "        \n",
    "        if trigger_times == 0:\n",
    "            print('New checkpoint...')\n",
    "            checkpoint(epoch, model, optimizer, criteria, save_as=save_as)\n",
    "\n",
    "    return model, (train_loss_track, train_accuracy_track), (test_loss_track, test_accuracy_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sZnDhkhlwyiV",
    "outputId": "d5c2646c-1dff-41af-f336-44eda69a44f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training process... =)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = NetV0(input_features=30)\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "resume = 1\n",
    "v0_path = Path('../Output/NetV0.pth')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "recover = False\n",
    "\n",
    "if v0_path.exists() and recover:\n",
    "    print('Recovering old training process...')\n",
    "    checkpoint = torch.load(v0_path)\n",
    "    resume = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criteria = checkpoint['criteria']\n",
    "else:\n",
    "    print('New training process... =)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iekQga58wybC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000 | Train loss: 161.75128 | Train accuracy: 0.63953034 | Test loss: 161.9159 | Test accuracy: 0.63787129]\n",
      "Trigger times: 1\n",
      "[2/1000 | Train loss: 152.26798 | Train accuracy: 0.68091166 | Test loss: 152.33664 | Test accuracy: 0.68103143]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[3/1000 | Train loss: 142.63881 | Train accuracy: 0.71794585 | Test loss: 142.79468 | Test accuracy: 0.71694883]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[4/1000 | Train loss: 143.54637 | Train accuracy: 0.71339682 | Test loss: 143.86779 | Test accuracy: 0.71304763]\n",
      "Trigger times: 1\n",
      "[5/1000 | Train loss: 137.62164 | Train accuracy: 0.73494515 | Test loss: 137.82923 | Test accuracy: 0.73377997]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[6/1000 | Train loss: 126.06116 | Train accuracy: 0.76493503 | Test loss: 126.54347 | Test accuracy: 0.76456161]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[7/1000 | Train loss: 116.9635 | Train accuracy: 0.78994595 | Test loss: 117.30675 | Test accuracy: 0.789154]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[8/1000 | Train loss: 111.35083 | Train accuracy: 0.80394743 | Test loss: 111.65596 | Test accuracy: 0.804051]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[9/1000 | Train loss: 99.17174 | Train accuracy: 0.83415372 | Test loss: 99.961273 | Test accuracy: 0.83353223]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[10/1000 | Train loss: 99.877094 | Train accuracy: 0.83332292 | Test loss: 100.73108 | Test accuracy: 0.83247875]\n",
      "Trigger times: 1\n",
      "[11/1000 | Train loss: 93.816153 | Train accuracy: 0.8455684 | Test loss: 94.476188 | Test accuracy: 0.84432227]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[12/1000 | Train loss: 91.976613 | Train accuracy: 0.85070712 | Test loss: 92.844527 | Test accuracy: 0.84920289]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[13/1000 | Train loss: 92.983459 | Train accuracy: 0.84771801 | Test loss: 94.221243 | Test accuracy: 0.84647863]\n",
      "Trigger times: 1\n",
      "[14/1000 | Train loss: 88.823475 | Train accuracy: 0.85677249 | Test loss: 89.506527 | Test accuracy: 0.85600119]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[15/1000 | Train loss: 84.994618 | Train accuracy: 0.86715159 | Test loss: 86.174273 | Test accuracy: 0.86503815]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[16/1000 | Train loss: 87.232134 | Train accuracy: 0.85995913 | Test loss: 88.667493 | Test accuracy: 0.85705467]\n",
      "Trigger times: 1\n",
      "[17/1000 | Train loss: 82.089307 | Train accuracy: 0.87258805 | Test loss: 83.475263 | Test accuracy: 0.8697377]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[18/1000 | Train loss: 81.541851 | Train accuracy: 0.87454303 | Test loss: 82.867329 | Test accuracy: 0.87194344]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[19/1000 | Train loss: 81.28173 | Train accuracy: 0.87427433 | Test loss: 82.784754 | Test accuracy: 0.87170476]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[20/1000 | Train loss: 83.391313 | Train accuracy: 0.87049073 | Test loss: 84.829251 | Test accuracy: 0.86709574]\n",
      "Trigger times: 1\n",
      "[21/1000 | Train loss: 79.859113 | Train accuracy: 0.87744354 | Test loss: 81.337951 | Test accuracy: 0.87480761]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[22/1000 | Train loss: 81.774855 | Train accuracy: 0.87309931 | Test loss: 83.353835 | Test accuracy: 0.87043728]\n",
      "Trigger times: 1\n",
      "[23/1000 | Train loss: 76.9313 | Train accuracy: 0.88389816 | Test loss: 78.770776 | Test accuracy: 0.88044543]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[24/1000 | Train loss: 77.691389 | Train accuracy: 0.88213345 | Test loss: 79.590717 | Test accuracy: 0.87890635]\n",
      "Trigger times: 1\n",
      "[25/1000 | Train loss: 75.965323 | Train accuracy: 0.88623949 | Test loss: 77.828667 | Test accuracy: 0.88307915]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[26/1000 | Train loss: 78.801817 | Train accuracy: 0.87984442 | Test loss: 80.571322 | Test accuracy: 0.8758611]\n",
      "Trigger times: 1\n",
      "[27/1000 | Train loss: 76.446732 | Train accuracy: 0.88407245 | Test loss: 78.702628 | Test accuracy: 0.87944955]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[28/1000 | Train loss: 77.99259 | Train accuracy: 0.88233389 | Test loss: 80.240935 | Test accuracy: 0.87817384]\n",
      "Trigger times: 1\n",
      "[29/1000 | Train loss: 75.083423 | Train accuracy: 0.88742613 | Test loss: 77.08933 | Test accuracy: 0.88371289]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[30/1000 | Train loss: 76.638544 | Train accuracy: 0.88367013 | Test loss: 78.646031 | Test accuracy: 0.87969646]\n",
      "Trigger times: 1\n",
      "[31/1000 | Train loss: 79.236576 | Train accuracy: 0.87830629 | Test loss: 81.599523 | Test accuracy: 0.87431379]\n",
      "Trigger times: 2\n",
      "[32/1000 | Train loss: 74.729786 | Train accuracy: 0.88806374 | Test loss: 77.06953 | Test accuracy: 0.88282401]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[33/1000 | Train loss: 76.578174 | Train accuracy: 0.8837994 | Test loss: 78.808236 | Test accuracy: 0.87914503]\n",
      "Trigger times: 1\n",
      "[34/1000 | Train loss: 77.279823 | Train accuracy: 0.8816832 | Test loss: 79.568013 | Test accuracy: 0.87772117]\n",
      "Trigger times: 2\n",
      "[35/1000 | Train loss: 75.212258 | Train accuracy: 0.88783426 | Test loss: 77.31336 | Test accuracy: 0.88409972]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[36/1000 | Train loss: 73.579037 | Train accuracy: 0.8901872 | Test loss: 75.841614 | Test accuracy: 0.88595156]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[37/1000 | Train loss: 79.126345 | Train accuracy: 0.87867521 | Test loss: 81.230401 | Test accuracy: 0.87429733]\n",
      "Trigger times: 1\n"
     ]
    }
   ],
   "source": [
    "trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v0_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode f_27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 31), (810000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678113, 810000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_27 = X['f_27']\n",
    "f_27.nunique(), len(f_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810000, 678112)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X['f_27'] = X['f_27'].astype('category')\n",
    "drop_enc = OneHotEncoder(drop='first')\n",
    "sparse_matrix = pd.DataFrame.sparse.from_spmatrix(data=drop_enc.fit_transform(X[['f_27']]), columns=drop_enc.get_feature_names_out())\n",
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('f_27', axis=1, errors='ignore')\n",
    "X = pd.concat([X, sparse_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810000, 678142)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j44nOD2HwyYZ"
   },
   "outputs": [],
   "source": [
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QqrCvTnwyT_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDII01Oiwx6m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO1PZIgcCRT/kG8sHY+B9DH",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
