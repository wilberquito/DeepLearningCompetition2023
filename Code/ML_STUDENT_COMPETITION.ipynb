{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilberquito/NeuralNetworksCompetition2023/blob/main/Code/ML_STUDENT_COMPETITION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c54WRH8cPepW"
      },
      "source": [
        "# M.L Competition Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vH9vPZFyPepb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frSDoMZ6nROc",
        "outputId": "aa651fa3-34f6-4798-ba76-7c68b425723a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    from google.colab import data_table\n",
        "    data_table.DataTable.max_columns = 50\n",
        "    data_table.enable_dataframe_formatter()\n",
        "else:\n",
        "    from IPython.core.interactiveshell import InteractiveShell\n",
        "    InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-6Meh_vIqdTe"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q-5llJpgPepf",
        "outputId": "3878590d-b5d4-4f2a-8930-d63bd5828516"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "yKXMcANfPepf",
        "outputId": "3d35ab2a-e1d4-4226-a333-a53fc837725f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
              "0  493553  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208   \n",
              "1  237346 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894   \n",
              "2   37368 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914   \n",
              "3  665220  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892   \n",
              "4   41499  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451   \n",
              "\n",
              "       f_06  f_07  f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  \\\n",
              "0  0.729857     0     4     1     3     1     2     4     1     5     2     0   \n",
              "1  0.371070     3     0     3     3     5     3     2     0     1     6     0   \n",
              "2  0.767396     3     1     3     2     3     4     1     1     1     0     2   \n",
              "3 -2.020416     2     0     4     5     0     5     1     0     3     1     1   \n",
              "4 -0.936620     1     2     2     2     2     5     0     3     1     1     2   \n",
              "\n",
              "   f_18      f_19      f_20      f_21      f_22      f_23      f_24      f_25  \\\n",
              "0     1 -3.690715 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846   \n",
              "1     1  0.664517 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457   \n",
              "2     6 -0.494988 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382   \n",
              "3     3 -3.066427 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677   \n",
              "4     4 -1.899984  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811   \n",
              "\n",
              "       f_26        f_27        f_28  f_29  f_30  target  \n",
              "0 -1.061733  BDBBCACIBB   20.308715     1     0       0  \n",
              "1  0.123854  ACBDCBCADA -449.291063     1     0       0  \n",
              "2  2.273130  AABBABCLAF  -86.206118     0     1       1  \n",
              "3 -0.429972  ADBBABEEBA  -30.157403     0     2       1  \n",
              "4 -1.039599  ABBBBBCMBB  296.484562     0     2       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4740b34a-696a-430d-8156-6e245a1165f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_27</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>493553</td>\n",
              "      <td>0.315471</td>\n",
              "      <td>-0.183690</td>\n",
              "      <td>0.664383</td>\n",
              "      <td>-1.186794</td>\n",
              "      <td>0.665098</td>\n",
              "      <td>0.946208</td>\n",
              "      <td>0.729857</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.690715</td>\n",
              "      <td>-0.628005</td>\n",
              "      <td>-2.832295</td>\n",
              "      <td>-1.409039</td>\n",
              "      <td>3.645067</td>\n",
              "      <td>0.233039</td>\n",
              "      <td>-3.754846</td>\n",
              "      <td>-1.061733</td>\n",
              "      <td>BDBBCACIBB</td>\n",
              "      <td>20.308715</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>237346</td>\n",
              "      <td>-1.286392</td>\n",
              "      <td>1.780592</td>\n",
              "      <td>0.576698</td>\n",
              "      <td>-2.690658</td>\n",
              "      <td>1.321997</td>\n",
              "      <td>-0.675894</td>\n",
              "      <td>0.371070</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.664517</td>\n",
              "      <td>-2.871912</td>\n",
              "      <td>3.826628</td>\n",
              "      <td>3.087653</td>\n",
              "      <td>0.494209</td>\n",
              "      <td>3.210875</td>\n",
              "      <td>-0.666457</td>\n",
              "      <td>0.123854</td>\n",
              "      <td>ACBDCBCADA</td>\n",
              "      <td>-449.291063</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37368</td>\n",
              "      <td>-0.290303</td>\n",
              "      <td>-0.485907</td>\n",
              "      <td>0.808350</td>\n",
              "      <td>-0.156288</td>\n",
              "      <td>1.083632</td>\n",
              "      <td>-1.129914</td>\n",
              "      <td>0.767396</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.494988</td>\n",
              "      <td>-0.946303</td>\n",
              "      <td>2.333223</td>\n",
              "      <td>2.084169</td>\n",
              "      <td>-4.782668</td>\n",
              "      <td>-1.671375</td>\n",
              "      <td>2.774382</td>\n",
              "      <td>2.273130</td>\n",
              "      <td>AABBABCLAF</td>\n",
              "      <td>-86.206118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>665220</td>\n",
              "      <td>1.243590</td>\n",
              "      <td>0.035112</td>\n",
              "      <td>-1.013236</td>\n",
              "      <td>0.854267</td>\n",
              "      <td>0.019192</td>\n",
              "      <td>0.597892</td>\n",
              "      <td>-2.020416</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-3.066427</td>\n",
              "      <td>-2.430158</td>\n",
              "      <td>-0.185332</td>\n",
              "      <td>-0.701691</td>\n",
              "      <td>-2.769142</td>\n",
              "      <td>-6.534231</td>\n",
              "      <td>-0.557677</td>\n",
              "      <td>-0.429972</td>\n",
              "      <td>ADBBABEEBA</td>\n",
              "      <td>-30.157403</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41499</td>\n",
              "      <td>0.702716</td>\n",
              "      <td>0.817044</td>\n",
              "      <td>-0.064907</td>\n",
              "      <td>-1.045483</td>\n",
              "      <td>0.718374</td>\n",
              "      <td>0.164451</td>\n",
              "      <td>-0.936620</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.899984</td>\n",
              "      <td>1.427460</td>\n",
              "      <td>-4.992610</td>\n",
              "      <td>1.154162</td>\n",
              "      <td>-1.931443</td>\n",
              "      <td>2.325042</td>\n",
              "      <td>2.143811</td>\n",
              "      <td>-1.039599</td>\n",
              "      <td>ABBBBBCMBB</td>\n",
              "      <td>296.484562</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4740b34a-696a-430d-8156-6e245a1165f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4740b34a-696a-430d-8156-6e245a1165f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4740b34a-696a-430d-8156-6e245a1165f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 493553,\n            'f': \"493553\",\n        },\n{\n            'v': 0.3154713179849845,\n            'f': \"0.3154713179849845\",\n        },\n{\n            'v': -0.1836904487704342,\n            'f': \"-0.1836904487704342\",\n        },\n{\n            'v': 0.6643825071902617,\n            'f': \"0.6643825071902617\",\n        },\n{\n            'v': -1.1867943779102057,\n            'f': \"-1.1867943779102057\",\n        },\n{\n            'v': 0.6650983554084389,\n            'f': \"0.6650983554084389\",\n        },\n{\n            'v': 0.946207747620988,\n            'f': \"0.946207747620988\",\n        },\n{\n            'v': 0.7298569499431469,\n            'f': \"0.7298569499431469\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': -3.690715300182188,\n            'f': \"-3.690715300182188\",\n        },\n{\n            'v': -0.6280049668119927,\n            'f': \"-0.6280049668119927\",\n        },\n{\n            'v': -2.832295150712397,\n            'f': \"-2.832295150712397\",\n        },\n{\n            'v': -1.4090385458787378,\n            'f': \"-1.4090385458787378\",\n        },\n{\n            'v': 3.645067262233898,\n            'f': \"3.645067262233898\",\n        },\n{\n            'v': 0.2330390733102534,\n            'f': \"0.2330390733102534\",\n        },\n{\n            'v': -3.754846123350904,\n            'f': \"-3.754846123350904\",\n        },\n{\n            'v': -1.0617332126449046,\n            'f': \"-1.0617332126449046\",\n        },\n\"BDBBCACIBB\",\n{\n            'v': 20.30871493564873,\n            'f': \"20.30871493564873\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 237346,\n            'f': \"237346\",\n        },\n{\n            'v': -1.286392342706846,\n            'f': \"-1.286392342706846\",\n        },\n{\n            'v': 1.7805917149101176,\n            'f': \"1.7805917149101176\",\n        },\n{\n            'v': 0.5766976404384039,\n            'f': \"0.5766976404384039\",\n        },\n{\n            'v': -2.690658095318418,\n            'f': \"-2.690658095318418\",\n        },\n{\n            'v': 1.3219967933572998,\n            'f': \"1.3219967933572998\",\n        },\n{\n            'v': -0.6758939824083278,\n            'f': \"-0.6758939824083278\",\n        },\n{\n            'v': 0.3710700267903256,\n            'f': \"0.3710700267903256\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0.6645165908993002,\n            'f': \"0.6645165908993002\",\n        },\n{\n            'v': -2.871912315757452,\n            'f': \"-2.871912315757452\",\n        },\n{\n            'v': 3.826627810842657,\n            'f': \"3.826627810842657\",\n        },\n{\n            'v': 3.087653485668633,\n            'f': \"3.087653485668633\",\n        },\n{\n            'v': 0.4942092665117156,\n            'f': \"0.4942092665117156\",\n        },\n{\n            'v': 3.210874753404081,\n            'f': \"3.210874753404081\",\n        },\n{\n            'v': -0.6664566785948545,\n            'f': \"-0.6664566785948545\",\n        },\n{\n            'v': 0.1238538978213764,\n            'f': \"0.1238538978213764\",\n        },\n\"ACBDCBCADA\",\n{\n            'v': -449.29106308063297,\n            'f': \"-449.29106308063297\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 37368,\n            'f': \"37368\",\n        },\n{\n            'v': -0.2903031581594694,\n            'f': \"-0.2903031581594694\",\n        },\n{\n            'v': -0.4859073078779998,\n            'f': \"-0.4859073078779998\",\n        },\n{\n            'v': 0.8083500053822478,\n            'f': \"0.8083500053822478\",\n        },\n{\n            'v': -0.1562882034675039,\n            'f': \"-0.1562882034675039\",\n        },\n{\n            'v': 1.083631744129082,\n            'f': \"1.083631744129082\",\n        },\n{\n            'v': -1.1299142294071929,\n            'f': \"-1.1299142294071929\",\n        },\n{\n            'v': 0.7673957922691375,\n            'f': \"0.7673957922691375\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        },\n{\n            'v': -0.4949879257256799,\n            'f': \"-0.4949879257256799\",\n        },\n{\n            'v': -0.9463030178123564,\n            'f': \"-0.9463030178123564\",\n        },\n{\n            'v': 2.3332230088140475,\n            'f': \"2.3332230088140475\",\n        },\n{\n            'v': 2.084169468574978,\n            'f': \"2.084169468574978\",\n        },\n{\n            'v': -4.7826682477273135,\n            'f': \"-4.7826682477273135\",\n        },\n{\n            'v': -1.671374690518444,\n            'f': \"-1.671374690518444\",\n        },\n{\n            'v': 2.7743819671532446,\n            'f': \"2.7743819671532446\",\n        },\n{\n            'v': 2.27312974480488,\n            'f': \"2.27312974480488\",\n        },\n\"AABBABCLAF\",\n{\n            'v': -86.20611797456267,\n            'f': \"-86.20611797456267\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 665220,\n            'f': \"665220\",\n        },\n{\n            'v': 1.2435902764135691,\n            'f': \"1.2435902764135691\",\n        },\n{\n            'v': 0.0351124481295802,\n            'f': \"0.0351124481295802\",\n        },\n{\n            'v': -1.0132364049578737,\n            'f': \"-1.0132364049578737\",\n        },\n{\n            'v': 0.854266756747409,\n            'f': \"0.854266756747409\",\n        },\n{\n            'v': 0.0191923478108258,\n            'f': \"0.0191923478108258\",\n        },\n{\n            'v': 0.5978920134148811,\n            'f': \"0.5978920134148811\",\n        },\n{\n            'v': -2.020416084119066,\n            'f': \"-2.020416084119066\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': -3.066426700596244,\n            'f': \"-3.066426700596244\",\n        },\n{\n            'v': -2.430158197671881,\n            'f': \"-2.430158197671881\",\n        },\n{\n            'v': -0.1853322722460298,\n            'f': \"-0.1853322722460298\",\n        },\n{\n            'v': -0.7016912514931994,\n            'f': \"-0.7016912514931994\",\n        },\n{\n            'v': -2.76914243525596,\n            'f': \"-2.76914243525596\",\n        },\n{\n            'v': -6.534230848040475,\n            'f': \"-6.534230848040475\",\n        },\n{\n            'v': -0.5576772758338957,\n            'f': \"-0.5576772758338957\",\n        },\n{\n            'v': -0.4299722583622913,\n            'f': \"-0.4299722583622913\",\n        },\n\"ADBBABEEBA\",\n{\n            'v': -30.15740300923009,\n            'f': \"-30.15740300923009\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 41499,\n            'f': \"41499\",\n        },\n{\n            'v': 0.7027157143556877,\n            'f': \"0.7027157143556877\",\n        },\n{\n            'v': 0.8170440444671354,\n            'f': \"0.8170440444671354\",\n        },\n{\n            'v': -0.064906742184267,\n            'f': \"-0.064906742184267\",\n        },\n{\n            'v': -1.0454825809420276,\n            'f': \"-1.0454825809420276\",\n        },\n{\n            'v': 0.7183736998235606,\n            'f': \"0.7183736998235606\",\n        },\n{\n            'v': 0.1644514962992729,\n            'f': \"0.1644514962992729\",\n        },\n{\n            'v': -0.9366196529132428,\n            'f': \"-0.9366196529132428\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': -1.8999840497433944,\n            'f': \"-1.8999840497433944\",\n        },\n{\n            'v': 1.4274596343623616,\n            'f': \"1.4274596343623616\",\n        },\n{\n            'v': -4.992609842066788,\n            'f': \"-4.992609842066788\",\n        },\n{\n            'v': 1.154162234302733,\n            'f': \"1.154162234302733\",\n        },\n{\n            'v': -1.9314431250014568,\n            'f': \"-1.9314431250014568\",\n        },\n{\n            'v': 2.3250420992192806,\n            'f': \"2.3250420992192806\",\n        },\n{\n            'v': 2.143810732830162,\n            'f': \"2.143810732830162\",\n        },\n{\n            'v': -1.0395985915012504,\n            'f': \"-1.0395985915012504\",\n        },\n\"ABBBBBCMBB\",\n{\n            'v': 296.4845619201336,\n            'f': \"296.4845619201336\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"id\"], [\"number\", \"f_00\"], [\"number\", \"f_01\"], [\"number\", \"f_02\"], [\"number\", \"f_03\"], [\"number\", \"f_04\"], [\"number\", \"f_05\"], [\"number\", \"f_06\"], [\"number\", \"f_07\"], [\"number\", \"f_08\"], [\"number\", \"f_09\"], [\"number\", \"f_10\"], [\"number\", \"f_11\"], [\"number\", \"f_12\"], [\"number\", \"f_13\"], [\"number\", \"f_14\"], [\"number\", \"f_15\"], [\"number\", \"f_16\"], [\"number\", \"f_17\"], [\"number\", \"f_18\"], [\"number\", \"f_19\"], [\"number\", \"f_20\"], [\"number\", \"f_21\"], [\"number\", \"f_22\"], [\"number\", \"f_23\"], [\"number\", \"f_24\"], [\"number\", \"f_25\"], [\"number\", \"f_26\"], [\"string\", \"f_27\"], [\"number\", \"f_28\"], [\"number\", \"f_29\"], [\"number\", \"f_30\"], [\"number\", \"target\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "csv_path = Path('/content/drive/MyDrive/train.csv') if IN_COLAB else Path('../Data/train.csv')\n",
        "df = pd.read_csv(csv_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AVt-KnvPepg"
      },
      "source": [
        "## First approuch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlPl0dVDrByn"
      },
      "outputs": [],
      "source": [
        "X, y = df.drop(['target', 'f_27', 'id'], axis=1), df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "26pKNWPurBwZ",
        "outputId": "d86646d1-3361-4e42-d119-a949102f62ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "      <td>810000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.001564</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>-0.001910</td>\n",
              "      <td>-0.000820</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>-0.000971</td>\n",
              "      <td>2.032086</td>\n",
              "      <td>2.057980</td>\n",
              "      <td>2.362590</td>\n",
              "      <td>2.177847</td>\n",
              "      <td>1.804038</td>\n",
              "      <td>2.842322</td>\n",
              "      <td>2.239307</td>\n",
              "      <td>1.515411</td>\n",
              "      <td>2.101359</td>\n",
              "      <td>2.096502</td>\n",
              "      <td>1.857758</td>\n",
              "      <td>2.065975</td>\n",
              "      <td>0.307220</td>\n",
              "      <td>-0.178796</td>\n",
              "      <td>-0.156877</td>\n",
              "      <td>-0.009749</td>\n",
              "      <td>-0.369114</td>\n",
              "      <td>-0.342708</td>\n",
              "      <td>0.175932</td>\n",
              "      <td>0.356640</td>\n",
              "      <td>-0.448086</td>\n",
              "      <td>0.345565</td>\n",
              "      <td>1.002373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.998719</td>\n",
              "      <td>0.999402</td>\n",
              "      <td>1.000892</td>\n",
              "      <td>1.000081</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.999535</td>\n",
              "      <td>1.656749</td>\n",
              "      <td>1.590698</td>\n",
              "      <td>1.637473</td>\n",
              "      <td>1.645519</td>\n",
              "      <td>1.537589</td>\n",
              "      <td>1.762672</td>\n",
              "      <td>1.537712</td>\n",
              "      <td>1.359798</td>\n",
              "      <td>1.568952</td>\n",
              "      <td>1.559978</td>\n",
              "      <td>1.467507</td>\n",
              "      <td>1.565593</td>\n",
              "      <td>2.314858</td>\n",
              "      <td>2.400672</td>\n",
              "      <td>2.484487</td>\n",
              "      <td>2.450494</td>\n",
              "      <td>2.454113</td>\n",
              "      <td>2.387102</td>\n",
              "      <td>2.416753</td>\n",
              "      <td>2.476792</td>\n",
              "      <td>238.735832</td>\n",
              "      <td>0.475553</td>\n",
              "      <td>0.818851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.599856</td>\n",
              "      <td>-4.682199</td>\n",
              "      <td>-4.642676</td>\n",
              "      <td>-4.628484</td>\n",
              "      <td>-4.748501</td>\n",
              "      <td>-4.750214</td>\n",
              "      <td>-4.842919</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-11.280941</td>\n",
              "      <td>-11.257917</td>\n",
              "      <td>-12.183785</td>\n",
              "      <td>-11.853530</td>\n",
              "      <td>-12.301097</td>\n",
              "      <td>-11.416189</td>\n",
              "      <td>-11.918306</td>\n",
              "      <td>-14.300577</td>\n",
              "      <td>-1229.753052</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.674386</td>\n",
              "      <td>-0.674814</td>\n",
              "      <td>-0.674948</td>\n",
              "      <td>-0.676899</td>\n",
              "      <td>-0.676212</td>\n",
              "      <td>-0.672953</td>\n",
              "      <td>-0.675204</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.236465</td>\n",
              "      <td>-1.804803</td>\n",
              "      <td>-1.819358</td>\n",
              "      <td>-1.645711</td>\n",
              "      <td>-2.019762</td>\n",
              "      <td>-1.956021</td>\n",
              "      <td>-1.440454</td>\n",
              "      <td>-1.262614</td>\n",
              "      <td>-159.456219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.002005</td>\n",
              "      <td>0.002531</td>\n",
              "      <td>0.001148</td>\n",
              "      <td>-0.002556</td>\n",
              "      <td>-0.001907</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>-0.001630</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.329103</td>\n",
              "      <td>-0.189919</td>\n",
              "      <td>-0.153055</td>\n",
              "      <td>0.030148</td>\n",
              "      <td>-0.390120</td>\n",
              "      <td>-0.342316</td>\n",
              "      <td>0.160621</td>\n",
              "      <td>0.405216</td>\n",
              "      <td>-0.573352</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.674684</td>\n",
              "      <td>0.675737</td>\n",
              "      <td>0.676736</td>\n",
              "      <td>0.671851</td>\n",
              "      <td>0.672968</td>\n",
              "      <td>0.675826</td>\n",
              "      <td>0.673790</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.878621</td>\n",
              "      <td>1.444271</td>\n",
              "      <td>1.506658</td>\n",
              "      <td>1.660455</td>\n",
              "      <td>1.257267</td>\n",
              "      <td>1.266450</td>\n",
              "      <td>1.794780</td>\n",
              "      <td>2.028263</td>\n",
              "      <td>158.941960</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.749301</td>\n",
              "      <td>4.815699</td>\n",
              "      <td>4.961982</td>\n",
              "      <td>4.454920</td>\n",
              "      <td>4.948983</td>\n",
              "      <td>4.971881</td>\n",
              "      <td>4.822668</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>11.108150</td>\n",
              "      <td>11.475325</td>\n",
              "      <td>12.029242</td>\n",
              "      <td>11.344080</td>\n",
              "      <td>12.247100</td>\n",
              "      <td>12.389844</td>\n",
              "      <td>12.529179</td>\n",
              "      <td>12.913041</td>\n",
              "      <td>1229.562577</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                f_00           f_01           f_02           f_03  \\\n",
              "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
              "mean        0.000368       0.001564       0.000306      -0.001910   \n",
              "std         0.998719       0.999402       1.000892       1.000081   \n",
              "min        -4.599856      -4.682199      -4.642676      -4.628484   \n",
              "25%        -0.674386      -0.674814      -0.674948      -0.676899   \n",
              "50%         0.002005       0.002531       0.001148      -0.002556   \n",
              "75%         0.674684       0.675737       0.676736       0.671851   \n",
              "max         4.749301       4.815699       4.961982       4.454920   \n",
              "\n",
              "                f_04           f_05           f_06           f_07  \\\n",
              "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
              "mean       -0.000820       0.000644      -0.000971       2.032086   \n",
              "std         1.000000       0.999999       0.999535       1.656749   \n",
              "min        -4.748501      -4.750214      -4.842919       0.000000   \n",
              "25%        -0.676212      -0.672953      -0.675204       1.000000   \n",
              "50%        -0.001907       0.000071      -0.001630       2.000000   \n",
              "75%         0.672968       0.675826       0.673790       3.000000   \n",
              "max         4.948983       4.971881       4.822668      15.000000   \n",
              "\n",
              "                f_08           f_09           f_10           f_11  \\\n",
              "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
              "mean        2.057980       2.362590       2.177847       1.804038   \n",
              "std         1.590698       1.637473       1.645519       1.537589   \n",
              "min         0.000000       0.000000       0.000000       0.000000   \n",
              "25%         1.000000       1.000000       1.000000       1.000000   \n",
              "50%         2.000000       2.000000       2.000000       2.000000   \n",
              "75%         3.000000       3.000000       3.000000       3.000000   \n",
              "max        16.000000      14.000000      14.000000      13.000000   \n",
              "\n",
              "                f_12           f_13           f_14           f_15  \\\n",
              "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
              "mean        2.842322       2.239307       1.515411       2.101359   \n",
              "std         1.762672       1.537712       1.359798       1.568952   \n",
              "min         0.000000       0.000000       0.000000       0.000000   \n",
              "25%         2.000000       1.000000       0.000000       1.000000   \n",
              "50%         3.000000       2.000000       1.000000       2.000000   \n",
              "75%         4.000000       3.000000       2.000000       3.000000   \n",
              "max        16.000000      12.000000      14.000000      14.000000   \n",
              "\n",
              "                f_16           f_17           f_18           f_19  \\\n",
              "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
              "mean        2.096502       1.857758       2.065975       0.307220   \n",
              "std         1.559978       1.467507       1.565593       2.314858   \n",
              "min         0.000000       0.000000       0.000000     -11.280941   \n",
              "25%         1.000000       1.000000       1.000000      -1.236465   \n",
              "50%         2.000000       2.000000       2.000000       0.329103   \n",
              "75%         3.000000       3.000000       3.000000       1.878621   \n",
              "max        15.000000      12.000000      13.000000      11.108150   \n",
              "\n",
              "                f_20           f_21           f_22           f_23  \\\n",
              "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
              "mean       -0.178796      -0.156877      -0.009749      -0.369114   \n",
              "std         2.400672       2.484487       2.450494       2.454113   \n",
              "min       -11.257917     -12.183785     -11.853530     -12.301097   \n",
              "25%        -1.804803      -1.819358      -1.645711      -2.019762   \n",
              "50%        -0.189919      -0.153055       0.030148      -0.390120   \n",
              "75%         1.444271       1.506658       1.660455       1.257267   \n",
              "max        11.475325      12.029242      11.344080      12.247100   \n",
              "\n",
              "                f_24           f_25           f_26           f_28  \\\n",
              "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
              "mean       -0.342708       0.175932       0.356640      -0.448086   \n",
              "std         2.387102       2.416753       2.476792     238.735832   \n",
              "min       -11.416189     -11.918306     -14.300577   -1229.753052   \n",
              "25%        -1.956021      -1.440454      -1.262614    -159.456219   \n",
              "50%        -0.342316       0.160621       0.405216      -0.573352   \n",
              "75%         1.266450       1.794780       2.028263     158.941960   \n",
              "max        12.389844      12.529179      12.913041    1229.562577   \n",
              "\n",
              "                f_29           f_30  \n",
              "count  810000.000000  810000.000000  \n",
              "mean        0.345565       1.002373  \n",
              "std         0.475553       0.818851  \n",
              "min         0.000000       0.000000  \n",
              "25%         0.000000       0.000000  \n",
              "50%         0.000000       1.000000  \n",
              "75%         1.000000       2.000000  \n",
              "max         1.000000       2.000000  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmznjs5grBnV",
        "outputId": "c7388ce9-7b1c-4dfc-c48a-4b832222cc6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    415945\n",
              "1    394055\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nc_P-uEg4BZ",
        "outputId": "da2894c5-9823-461f-ec2f-33f130e7e10b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(X), type(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xuWymPQrBd_"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, input_features=30):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=input_features, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=128, out_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=128, out_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=128, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=64, out_features=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.main(x)\n",
        "\n",
        "# Define a custom dataset by extending the PyTorch Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels \n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        label = self.labels[index]\n",
        "        if self.transform:\n",
        "            item = self.transform(item)\n",
        "        return item, label\n",
        "    \n",
        "def datasets(dataset, stratify, test_size):\n",
        "    train_size = 1 - test_size\n",
        "\n",
        "    # Stratify by label\n",
        "    labels = np.array(stratify)\n",
        "    positive_indices = np.where(labels == 1)[0]\n",
        "    negative_indices = np.where(labels == 0)[0]\n",
        "\n",
        "    positive_split = int(train_size * len(positive_indices))\n",
        "    negative_split = int(train_size * len(negative_indices))\n",
        "\n",
        "    positive_train_indices = positive_indices[:positive_split]\n",
        "    positive_test_indices = positive_indices[positive_split:]\n",
        "    negative_train_indices = negative_indices[:negative_split]\n",
        "    negative_test_indices = negative_indices[negative_split:]\n",
        "\n",
        "    train_indices = np.concatenate([positive_train_indices, negative_train_indices])\n",
        "    test_indices = np.concatenate([positive_test_indices, negative_test_indices])\n",
        "\n",
        "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "    \n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b1ydxiAPepj",
        "outputId": "dbabf924-3388-44d0-ece1-6a36e9ec26f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.315508</td>\n",
              "      <td>-0.185365</td>\n",
              "      <td>0.663485</td>\n",
              "      <td>-1.184789</td>\n",
              "      <td>0.665919</td>\n",
              "      <td>0.945566</td>\n",
              "      <td>0.731168</td>\n",
              "      <td>-1.226551</td>\n",
              "      <td>1.220861</td>\n",
              "      <td>-0.832130</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>-0.522922</td>\n",
              "      <td>-0.477867</td>\n",
              "      <td>1.145009</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>1.847503</td>\n",
              "      <td>-0.061861</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>-1.727077</td>\n",
              "      <td>-0.187118</td>\n",
              "      <td>-1.076850</td>\n",
              "      <td>-0.571024</td>\n",
              "      <td>1.635696</td>\n",
              "      <td>0.241191</td>\n",
              "      <td>-1.626472</td>\n",
              "      <td>-0.572666</td>\n",
              "      <td>0.086945</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.288412</td>\n",
              "      <td>1.780094</td>\n",
              "      <td>0.575879</td>\n",
              "      <td>-2.688532</td>\n",
              "      <td>1.322818</td>\n",
              "      <td>-0.676539</td>\n",
              "      <td>0.372214</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>2.078555</td>\n",
              "      <td>0.089454</td>\n",
              "      <td>-0.155626</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>2.502279</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>0.154349</td>\n",
              "      <td>-1.121819</td>\n",
              "      <td>1.603352</td>\n",
              "      <td>1.263992</td>\n",
              "      <td>0.351786</td>\n",
              "      <td>1.488661</td>\n",
              "      <td>-0.348562</td>\n",
              "      <td>-0.093987</td>\n",
              "      <td>-1.880083</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.291044</td>\n",
              "      <td>-0.487763</td>\n",
              "      <td>0.807325</td>\n",
              "      <td>-0.154365</td>\n",
              "      <td>1.084453</td>\n",
              "      <td>-1.130560</td>\n",
              "      <td>0.768724</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-0.665105</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.777817</td>\n",
              "      <td>0.656775</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-1.343932</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>2.512803</td>\n",
              "      <td>-0.346548</td>\n",
              "      <td>-0.319705</td>\n",
              "      <td>1.002260</td>\n",
              "      <td>0.854489</td>\n",
              "      <td>-1.798432</td>\n",
              "      <td>-0.556603</td>\n",
              "      <td>1.075183</td>\n",
              "      <td>0.773779</td>\n",
              "      <td>-0.359217</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>-0.002898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.244818</td>\n",
              "      <td>0.033569</td>\n",
              "      <td>-1.012640</td>\n",
              "      <td>0.856109</td>\n",
              "      <td>0.020013</td>\n",
              "      <td>0.597249</td>\n",
              "      <td>-2.020385</td>\n",
              "      <td>-0.019367</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.999962</td>\n",
              "      <td>1.715054</td>\n",
              "      <td>-1.173291</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>0.572765</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>-0.584500</td>\n",
              "      <td>0.596595</td>\n",
              "      <td>-1.457389</td>\n",
              "      <td>-0.937806</td>\n",
              "      <td>-0.011453</td>\n",
              "      <td>-0.282369</td>\n",
              "      <td>-0.977962</td>\n",
              "      <td>-2.593742</td>\n",
              "      <td>-0.303552</td>\n",
              "      <td>-0.317593</td>\n",
              "      <td>-0.124444</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.703249</td>\n",
              "      <td>0.815969</td>\n",
              "      <td>-0.065154</td>\n",
              "      <td>-1.043488</td>\n",
              "      <td>0.719195</td>\n",
              "      <td>0.163808</td>\n",
              "      <td>-0.936084</td>\n",
              "      <td>-0.622959</td>\n",
              "      <td>-0.036450</td>\n",
              "      <td>-0.221433</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.127447</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-1.456260</td>\n",
              "      <td>1.091772</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>1.235331</td>\n",
              "      <td>-0.953495</td>\n",
              "      <td>0.669086</td>\n",
              "      <td>-1.946372</td>\n",
              "      <td>0.474970</td>\n",
              "      <td>-0.636617</td>\n",
              "      <td>1.117569</td>\n",
              "      <td>0.814266</td>\n",
              "      <td>-0.563729</td>\n",
              "      <td>1.243772</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0  0.315508 -0.185365  0.663485 -1.184789  0.665919  0.945566  0.731168   \n",
              "1 -1.288412  1.780094  0.575879 -2.688532  1.322818 -0.676539  0.372214   \n",
              "2 -0.291044 -0.487763  0.807325 -0.154365  1.084453 -1.130560  0.768724   \n",
              "3  1.244818  0.033569 -1.012640  0.856109  0.020013  0.597249 -2.020385   \n",
              "4  0.703249  0.815969 -0.065154 -1.043488  0.719195  0.163808 -0.936084   \n",
              "\n",
              "       f_07      f_08      f_09      f_10      f_11      f_12      f_13  \\\n",
              "0 -1.226551  1.220861 -0.832130  0.499632 -0.522922 -0.477867  1.145009   \n",
              "1  0.584225 -1.293760  0.389265  0.499632  2.078555  0.089454 -0.155626   \n",
              "2  0.584225 -0.665105  0.389265 -0.108080  0.777817  0.656775 -0.805943   \n",
              "3 -0.019367 -1.293760  0.999962  1.715054 -1.173291  1.224096 -0.805943   \n",
              "4 -0.622959 -0.036450 -0.221433 -0.108080  0.127447  1.224096 -1.456260   \n",
              "\n",
              "       f_14      f_15      f_16      f_17      f_18      f_19      f_20  \\\n",
              "0 -0.379035  1.847503 -0.061861 -1.265929 -0.680877 -1.727077 -0.187118   \n",
              "1 -1.114439 -0.701972  2.502279 -1.265929 -0.680877  0.154349 -1.121819   \n",
              "2 -0.379035 -0.701972 -1.343932  0.096928  2.512803 -0.346548 -0.319705   \n",
              "3 -1.114439  0.572765 -0.702897 -0.584500  0.596595 -1.457389 -0.937806   \n",
              "4  1.091772 -0.701972 -0.702897  0.096928  1.235331 -0.953495  0.669086   \n",
              "\n",
              "       f_21      f_22      f_23      f_24      f_25      f_26      f_28  \\\n",
              "0 -1.076850 -0.571024  1.635696  0.241191 -1.626472 -0.572666  0.086945   \n",
              "1  1.603352  1.263992  0.351786  1.488661 -0.348562 -0.093987 -1.880083   \n",
              "2  1.002260  0.854489 -1.798432 -0.556603  1.075183  0.773779 -0.359217   \n",
              "3 -0.011453 -0.282369 -0.977962 -2.593742 -0.303552 -0.317593 -0.124444   \n",
              "4 -1.946372  0.474970 -0.636617  1.117569  0.814266 -0.563729  1.243772   \n",
              "\n",
              "       f_29      f_30  \n",
              "0  1.376157 -1.224123  \n",
              "1  1.376157 -1.224123  \n",
              "2 -0.726661 -0.002898  \n",
              "3 -0.726661  1.218327  \n",
              "4 -0.726661  1.218327  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_R50gWyPepk",
        "outputId": "9498d4f0-73fe-43ab-ec72-e09ed1110c11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "      <td>8.100000e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.277222e-17</td>\n",
              "      <td>-2.741291e-18</td>\n",
              "      <td>-1.000900e-17</td>\n",
              "      <td>-7.184377e-18</td>\n",
              "      <td>2.745677e-18</td>\n",
              "      <td>-9.684434e-18</td>\n",
              "      <td>-1.142132e-17</td>\n",
              "      <td>-5.172049e-17</td>\n",
              "      <td>-1.485648e-16</td>\n",
              "      <td>6.184353e-17</td>\n",
              "      <td>6.965073e-17</td>\n",
              "      <td>-3.005333e-17</td>\n",
              "      <td>1.459683e-17</td>\n",
              "      <td>-1.030901e-16</td>\n",
              "      <td>9.513378e-17</td>\n",
              "      <td>-6.732612e-17</td>\n",
              "      <td>1.230906e-16</td>\n",
              "      <td>-1.571966e-17</td>\n",
              "      <td>-6.828228e-17</td>\n",
              "      <td>-7.308941e-17</td>\n",
              "      <td>1.747409e-17</td>\n",
              "      <td>-2.929892e-18</td>\n",
              "      <td>-4.035181e-18</td>\n",
              "      <td>-2.333387e-18</td>\n",
              "      <td>1.224590e-17</td>\n",
              "      <td>3.477273e-17</td>\n",
              "      <td>7.921236e-17</td>\n",
              "      <td>-1.642143e-17</td>\n",
              "      <td>1.593019e-17</td>\n",
              "      <td>-1.786445e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "      <td>1.000001e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-4.606128e+00</td>\n",
              "      <td>-4.686569e+00</td>\n",
              "      <td>-4.638847e+00</td>\n",
              "      <td>-4.626203e+00</td>\n",
              "      <td>-4.747684e+00</td>\n",
              "      <td>-4.750866e+00</td>\n",
              "      <td>-4.844201e+00</td>\n",
              "      <td>-1.226551e+00</td>\n",
              "      <td>-1.293760e+00</td>\n",
              "      <td>-1.442828e+00</td>\n",
              "      <td>-1.323502e+00</td>\n",
              "      <td>-1.173291e+00</td>\n",
              "      <td>-1.612509e+00</td>\n",
              "      <td>-1.456260e+00</td>\n",
              "      <td>-1.114439e+00</td>\n",
              "      <td>-1.339340e+00</td>\n",
              "      <td>-1.343932e+00</td>\n",
              "      <td>-1.265929e+00</td>\n",
              "      <td>-1.319613e+00</td>\n",
              "      <td>-5.005996e+00</td>\n",
              "      <td>-4.615012e+00</td>\n",
              "      <td>-4.840804e+00</td>\n",
              "      <td>-4.833224e+00</td>\n",
              "      <td>-4.862037e+00</td>\n",
              "      <td>-4.638884e+00</td>\n",
              "      <td>-5.004338e+00</td>\n",
              "      <td>-5.917825e+00</td>\n",
              "      <td>-5.149230e+00</td>\n",
              "      <td>-7.266613e-01</td>\n",
              "      <td>-1.224123e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-6.756203e-01</td>\n",
              "      <td>-6.767832e-01</td>\n",
              "      <td>-6.746520e-01</td>\n",
              "      <td>-6.749347e-01</td>\n",
              "      <td>-6.753925e-01</td>\n",
              "      <td>-6.735980e-01</td>\n",
              "      <td>-6.745475e-01</td>\n",
              "      <td>-6.229591e-01</td>\n",
              "      <td>-6.651048e-01</td>\n",
              "      <td>-8.321305e-01</td>\n",
              "      <td>-7.157908e-01</td>\n",
              "      <td>-5.229217e-01</td>\n",
              "      <td>-4.778670e-01</td>\n",
              "      <td>-8.059429e-01</td>\n",
              "      <td>-1.114439e+00</td>\n",
              "      <td>-7.019718e-01</td>\n",
              "      <td>-7.028966e-01</td>\n",
              "      <td>-5.845004e-01</td>\n",
              "      <td>-6.808767e-01</td>\n",
              "      <td>-6.668599e-01</td>\n",
              "      <td>-6.773138e-01</td>\n",
              "      <td>-6.691450e-01</td>\n",
              "      <td>-6.676056e-01</td>\n",
              "      <td>-6.726050e-01</td>\n",
              "      <td>-6.758463e-01</td>\n",
              "      <td>-6.688259e-01</td>\n",
              "      <td>-6.537707e-01</td>\n",
              "      <td>-6.660426e-01</td>\n",
              "      <td>-7.266613e-01</td>\n",
              "      <td>-1.224123e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.639482e-03</td>\n",
              "      <td>9.684993e-04</td>\n",
              "      <td>8.415122e-04</td>\n",
              "      <td>-6.451620e-04</td>\n",
              "      <td>-1.086850e-03</td>\n",
              "      <td>-5.721649e-04</td>\n",
              "      <td>-6.598558e-04</td>\n",
              "      <td>-1.936711e-02</td>\n",
              "      <td>-3.644958e-02</td>\n",
              "      <td>-2.214329e-01</td>\n",
              "      <td>-1.080796e-01</td>\n",
              "      <td>1.274475e-01</td>\n",
              "      <td>8.945390e-02</td>\n",
              "      <td>-1.556257e-01</td>\n",
              "      <td>-3.790352e-01</td>\n",
              "      <td>-6.460321e-02</td>\n",
              "      <td>-6.186147e-02</td>\n",
              "      <td>9.692768e-02</td>\n",
              "      <td>-4.214080e-02</td>\n",
              "      <td>9.453292e-03</td>\n",
              "      <td>-4.633341e-03</td>\n",
              "      <td>1.538327e-03</td>\n",
              "      <td>1.628116e-02</td>\n",
              "      <td>-8.559671e-03</td>\n",
              "      <td>1.643847e-04</td>\n",
              "      <td>-6.335318e-03</td>\n",
              "      <td>1.961251e-02</td>\n",
              "      <td>-5.247088e-04</td>\n",
              "      <td>-7.266613e-01</td>\n",
              "      <td>-2.897771e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.751812e-01</td>\n",
              "      <td>6.745771e-01</td>\n",
              "      <td>6.758278e-01</td>\n",
              "      <td>6.737077e-01</td>\n",
              "      <td>6.737887e-01</td>\n",
              "      <td>6.751838e-01</td>\n",
              "      <td>6.750750e-01</td>\n",
              "      <td>5.842249e-01</td>\n",
              "      <td>5.922056e-01</td>\n",
              "      <td>3.892647e-01</td>\n",
              "      <td>4.996316e-01</td>\n",
              "      <td>7.778166e-01</td>\n",
              "      <td>6.567748e-01</td>\n",
              "      <td>4.946915e-01</td>\n",
              "      <td>3.563685e-01</td>\n",
              "      <td>5.727654e-01</td>\n",
              "      <td>5.791736e-01</td>\n",
              "      <td>7.783558e-01</td>\n",
              "      <td>5.965951e-01</td>\n",
              "      <td>6.788330e-01</td>\n",
              "      <td>6.760890e-01</td>\n",
              "      <td>6.695690e-01</td>\n",
              "      <td>6.815786e-01</td>\n",
              "      <td>6.627166e-01</td>\n",
              "      <td>6.741058e-01</td>\n",
              "      <td>6.698445e-01</td>\n",
              "      <td>6.749149e-01</td>\n",
              "      <td>6.676423e-01</td>\n",
              "      <td>1.376157e+00</td>\n",
              "      <td>1.218327e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.755027e+00</td>\n",
              "      <td>4.817020e+00</td>\n",
              "      <td>4.957258e+00</td>\n",
              "      <td>4.456473e+00</td>\n",
              "      <td>4.949807e+00</td>\n",
              "      <td>4.971246e+00</td>\n",
              "      <td>4.825883e+00</td>\n",
              "      <td>7.827329e+00</td>\n",
              "      <td>8.764723e+00</td>\n",
              "      <td>7.106938e+00</td>\n",
              "      <td>7.184455e+00</td>\n",
              "      <td>7.281508e+00</td>\n",
              "      <td>7.464626e+00</td>\n",
              "      <td>6.347546e+00</td>\n",
              "      <td>9.181213e+00</td>\n",
              "      <td>7.583820e+00</td>\n",
              "      <td>8.271595e+00</td>\n",
              "      <td>6.911209e+00</td>\n",
              "      <td>6.983954e+00</td>\n",
              "      <td>4.665918e+00</td>\n",
              "      <td>4.854528e+00</td>\n",
              "      <td>4.904886e+00</td>\n",
              "      <td>4.633284e+00</td>\n",
              "      <td>5.140847e+00</td>\n",
              "      <td>5.333899e+00</td>\n",
              "      <td>5.111510e+00</td>\n",
              "      <td>5.069625e+00</td>\n",
              "      <td>5.152186e+00</td>\n",
              "      <td>1.376157e+00</td>\n",
              "      <td>1.218327e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               f_00          f_01          f_02          f_03          f_04  \\\n",
              "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
              "mean   1.277222e-17 -2.741291e-18 -1.000900e-17 -7.184377e-18  2.745677e-18   \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min   -4.606128e+00 -4.686569e+00 -4.638847e+00 -4.626203e+00 -4.747684e+00   \n",
              "25%   -6.756203e-01 -6.767832e-01 -6.746520e-01 -6.749347e-01 -6.753925e-01   \n",
              "50%    1.639482e-03  9.684993e-04  8.415122e-04 -6.451620e-04 -1.086850e-03   \n",
              "75%    6.751812e-01  6.745771e-01  6.758278e-01  6.737077e-01  6.737887e-01   \n",
              "max    4.755027e+00  4.817020e+00  4.957258e+00  4.456473e+00  4.949807e+00   \n",
              "\n",
              "               f_05          f_06          f_07          f_08          f_09  \\\n",
              "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
              "mean  -9.684434e-18 -1.142132e-17 -5.172049e-17 -1.485648e-16  6.184353e-17   \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min   -4.750866e+00 -4.844201e+00 -1.226551e+00 -1.293760e+00 -1.442828e+00   \n",
              "25%   -6.735980e-01 -6.745475e-01 -6.229591e-01 -6.651048e-01 -8.321305e-01   \n",
              "50%   -5.721649e-04 -6.598558e-04 -1.936711e-02 -3.644958e-02 -2.214329e-01   \n",
              "75%    6.751838e-01  6.750750e-01  5.842249e-01  5.922056e-01  3.892647e-01   \n",
              "max    4.971246e+00  4.825883e+00  7.827329e+00  8.764723e+00  7.106938e+00   \n",
              "\n",
              "               f_10          f_11          f_12          f_13          f_14  \\\n",
              "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
              "mean   6.965073e-17 -3.005333e-17  1.459683e-17 -1.030901e-16  9.513378e-17   \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min   -1.323502e+00 -1.173291e+00 -1.612509e+00 -1.456260e+00 -1.114439e+00   \n",
              "25%   -7.157908e-01 -5.229217e-01 -4.778670e-01 -8.059429e-01 -1.114439e+00   \n",
              "50%   -1.080796e-01  1.274475e-01  8.945390e-02 -1.556257e-01 -3.790352e-01   \n",
              "75%    4.996316e-01  7.778166e-01  6.567748e-01  4.946915e-01  3.563685e-01   \n",
              "max    7.184455e+00  7.281508e+00  7.464626e+00  6.347546e+00  9.181213e+00   \n",
              "\n",
              "               f_15          f_16          f_17          f_18          f_19  \\\n",
              "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
              "mean  -6.732612e-17  1.230906e-16 -1.571966e-17 -6.828228e-17 -7.308941e-17   \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min   -1.339340e+00 -1.343932e+00 -1.265929e+00 -1.319613e+00 -5.005996e+00   \n",
              "25%   -7.019718e-01 -7.028966e-01 -5.845004e-01 -6.808767e-01 -6.668599e-01   \n",
              "50%   -6.460321e-02 -6.186147e-02  9.692768e-02 -4.214080e-02  9.453292e-03   \n",
              "75%    5.727654e-01  5.791736e-01  7.783558e-01  5.965951e-01  6.788330e-01   \n",
              "max    7.583820e+00  8.271595e+00  6.911209e+00  6.983954e+00  4.665918e+00   \n",
              "\n",
              "               f_20          f_21          f_22          f_23          f_24  \\\n",
              "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
              "mean   1.747409e-17 -2.929892e-18 -4.035181e-18 -2.333387e-18  1.224590e-17   \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
              "min   -4.615012e+00 -4.840804e+00 -4.833224e+00 -4.862037e+00 -4.638884e+00   \n",
              "25%   -6.773138e-01 -6.691450e-01 -6.676056e-01 -6.726050e-01 -6.758463e-01   \n",
              "50%   -4.633341e-03  1.538327e-03  1.628116e-02 -8.559671e-03  1.643847e-04   \n",
              "75%    6.760890e-01  6.695690e-01  6.815786e-01  6.627166e-01  6.741058e-01   \n",
              "max    4.854528e+00  4.904886e+00  4.633284e+00  5.140847e+00  5.333899e+00   \n",
              "\n",
              "               f_25          f_26          f_28          f_29          f_30  \n",
              "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  \n",
              "mean   3.477273e-17  7.921236e-17 -1.642143e-17  1.593019e-17 -1.786445e-17  \n",
              "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  \n",
              "min   -5.004338e+00 -5.917825e+00 -5.149230e+00 -7.266613e-01 -1.224123e+00  \n",
              "25%   -6.688259e-01 -6.537707e-01 -6.660426e-01 -7.266613e-01 -1.224123e+00  \n",
              "50%   -6.335318e-03  1.961251e-02 -5.247088e-04 -7.266613e-01 -2.897771e-03  \n",
              "75%    6.698445e-01  6.749149e-01  6.676423e-01  1.376157e+00  1.218327e+00  \n",
              "max    5.111510e+00  5.069625e+00  5.152186e+00  1.376157e+00  1.218327e+00  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceGWFcgVykIZ",
        "outputId": "ec70c167-98c6-46a7-c0f2-caba39de43c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "810000"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Declare the hole dataset\n",
        "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
        "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
        "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LkKFcbPwlHM",
        "outputId": "85d89a77-8ae2-4c36-9539-e21087a8e438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(688499, 121501)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
        "len(train_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUnpQLqcwylO"
      },
      "outputs": [],
      "source": [
        "# Checkpoint\n",
        "def checkpoint(epoch, model, optimizer, criteria, save_as: Path, test_loss_track=None, test_accuracy_track=None, train_loss_track=None, train_accuracy_track=None, best_state_dict=None):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'criteria': criteria,\n",
        "        'test_loss_track': test_loss_track,\n",
        "        'test_accuracy_track': test_accuracy_track,\n",
        "        'train_loss_track': train_loss_track,\n",
        "        'train_accuracy_track': train_accuracy_track,\n",
        "        'best_state_dict': best_state_dict\n",
        "    }, save_as)\n",
        "    \n",
        "\n",
        "# Validate\n",
        "def validation(model, device, valid_loader, criteria):\n",
        "    # Settings\n",
        "    model.eval()\n",
        "    loss_total = 0\n",
        "    accuracy_total = 0\n",
        "\n",
        "    # Test validation data\n",
        "    with torch.inference_mode():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs).squeeze(dim=1)\n",
        "            loss = criteria(outputs, labels)\n",
        "            loss_total += loss.item() * inputs.size(0)\n",
        "            matches = torch.round(torch.sigmoid(outputs)) == labels\n",
        "            accuracy_total += (torch.sum(matches.to(torch.int32))).item()\n",
        "                        \n",
        "    return (loss_total / len(valid_loader), accuracy_total / len(valid_loader.dataset))\n",
        "\n",
        "from math import inf\n",
        "\n",
        "# Train\n",
        "def train(device, model, epochs, optimizer, criteria, train_loader, valid_loader, resume=1, save_as=Path('../Output/model.pth')):\n",
        "    # Early stopping\n",
        "    last_loss = 100\n",
        "    patience = 5\n",
        "    trigger_times = 0\n",
        "    \n",
        "    train_loss_track, train_accuracy_track = [], []\n",
        "    test_loss_track, test_accuracy_track = [], []\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    best_test_accuracy = -inf\n",
        "    best_state_dict = None\n",
        "\n",
        "    for epoch in range(resume, epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        for times, (inputs, labels) in enumerate(train_loader, 1):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward and backward propagation\n",
        "            outputs = model(inputs).squeeze(dim=1)\n",
        "            loss = criteria(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Train loss and train accuracy\n",
        "        train_current_loss, train_current_accuracy = validation(model, device, train_loader, criteria)\n",
        "\n",
        "        # Early stopping\n",
        "        current_loss, current_accuracy = validation(model, device, valid_loader, criteria)\n",
        "        print('[{}/{} | Train loss: {:.8} | Train accuracy: {:.8} | Test loss: {:.8} | Test accuracy: {:.8}]'.format(epoch, epochs, train_current_loss, train_current_accuracy, current_loss, current_accuracy))\n",
        "        \n",
        "        train_loss_track.append(train_current_loss)\n",
        "        train_accuracy_track.append(train_current_accuracy)\n",
        "        test_loss_track.append(current_loss)\n",
        "        test_accuracy_track.append(current_accuracy)\n",
        "        \n",
        "        if current_loss > last_loss:\n",
        "            trigger_times += 1\n",
        "            print('Trigger times: {}/{}'.format(trigger_times, patience))\n",
        "\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                return model\n",
        "\n",
        "        else:\n",
        "            print('Trigger times: 0/{}'.format(patience))\n",
        "            trigger_times = 0\n",
        "\n",
        "        last_loss = current_loss\n",
        "\n",
        "        if current_accuracy > best_test_accuracy:\n",
        "            best_state_dict = model.state_dict()\n",
        "            best_test_accuracy = current_accuracy\n",
        "        \n",
        "        if trigger_times == 0:\n",
        "            print('New checkpoint...')\n",
        "            checkpoint(epoch, \n",
        "                       model, \n",
        "                       optimizer, \n",
        "                       criteria, \n",
        "                       save_as=save_as, \n",
        "                       test_loss_track=test_loss_track, \n",
        "                       test_accuracy_track=test_accuracy_track, \n",
        "                       train_loss_track=train_loss_track, \n",
        "                       train_accuracy_track=train_accuracy_track,\n",
        "                       best_state_dict=best_state_dict)\n",
        "\n",
        "    return model, (train_loss_track, train_accuracy_track), (test_loss_track, test_accuracy_track)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sZnDhkhlwyiV",
        "outputId": "d5c2646c-1dff-41af-f336-44eda69a44f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready for new training process... =)\n"
          ]
        }
      ],
      "source": [
        "model = Net(input_features=X_tensor.shape[1])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 256\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criteria = nn.BCEWithLogitsLoss()\n",
        "resume = 1\n",
        "v0_path = Path('../Output/NetV0.pth')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "recover = False\n",
        "\n",
        "if v0_path.exists() and recover:\n",
        "    print('Recovering old training process...')\n",
        "    checkpoint = torch.load(v0_path)\n",
        "    resume = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criteria = checkpoint['criteria']\n",
        "else:\n",
        "    print('Ready for new training process... =)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iekQga58wybC"
      },
      "outputs": [],
      "source": [
        "wanna_train = False\n",
        "if wanna_train:\n",
        "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v0_path)\n",
        "else:\n",
        "    print('Not training this model anymore...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j44nOD2HwyYZ",
        "outputId": "2e66b019-f2be-4bb8-fc0f-2fcd6a7f530f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying last checkpoint of model NetV0 state to trained folder\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WindowsPath('../Trained/NetV0.pth')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "v0_path_trained = Path('../Trained/NetV0.pth')\n",
        "if v0_path.exists():\n",
        "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV0'))\n",
        "    shutil.copy(v0_path, v0_path_trained)\n",
        "else:\n",
        "    print('Model {0} is not created yeat'.format('NetV0'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni9s7Ng5Pepn",
        "outputId": "38bd6f66-89f9-4235-c678-2f3a9281b840"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (main): Sequential(\n",
              "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (3): Tanh()\n",
              "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (5): Tanh()\n",
              "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (7): Tanh()\n",
              "    (8): Linear(in_features=64, out_features=128, bias=True)\n",
              "    (9): Tanh()\n",
              "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (11): Tanh()\n",
              "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (13): Tanh()\n",
              "    (14): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (15): Tanh()\n",
              "    (16): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (17): Tanh()\n",
              "    (18): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (19): Tanh()\n",
              "    (20): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (21): Tanh()\n",
              "    (22): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (90000, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>209540</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>182173</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>195991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>489479</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  target\n",
              "0  209540       0\n",
              "1  182173       0\n",
              "2  195991       1\n",
              "3   43111       0\n",
              "4  489479       1"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v0_path_trained = Path('../Trained/NetV0.pth')\n",
        "v0_state = torch.load(v0_path_trained)\n",
        "v0_state_dict = v0_state['model_state_dict']\n",
        "v0_model = Net()\n",
        "v0_model.load_state_dict(v0_state_dict)\n",
        "\n",
        "validate_path = Path('/content/drive/MyDrive/validate.csv') if IN_COLAB else Path('../Data/validate.csv')\n",
        "validate_df = pd.read_csv(validate_path)\n",
        "X_val = validate_df.drop(['id', 'f_27'], axis=1)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_val_tensor = torch.from_numpy(X_val)\n",
        "X_val_tensor = X_val_tensor.type(torch.float)\n",
        "X_val_tensor = X_val_tensor.to(device)\n",
        "\n",
        "v0_model = v0_model.to(device)\n",
        "v0_model.eval()\n",
        "with torch.inference_mode():\n",
        "    outputs = v0_model(X_val_tensor)\n",
        "    y_pred = torch.round(torch.sigmoid(outputs))\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    \n",
        "submision_df = pd.DataFrame({\n",
        "    'id': validate_df['id'],\n",
        "    'target': y_pred.flatten()\n",
        "})\n",
        "\n",
        "submision_df = submision_df.astype('int')\n",
        "\n",
        "submit_path_model_v0 = Path('../Submit/V0.csv')\n",
        "submision_df.to_csv(submit_path_model_v0, index=False)\n",
        "submision_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TijG7taePepn"
      },
      "source": [
        "## Second approch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE6FG4TJPepn"
      },
      "source": [
        "### One Hot encode f_27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5wNF8BAPepn"
      },
      "source": [
        "This is not the way to go because the dataframe increases a lot..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Nq3_X8EPepo",
        "outputId": "8a3d3bd9-cd57-4a3a-bdac-df9e784f9c8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((810000, 31), (810000,))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB7q6tE1Pepo",
        "outputId": "b5ab94f2-9415-4e31-c348-876e9c383138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(678113, 810000)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_27 = X['f_27']\n",
        "f_27.nunique(), len(f_27)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIKVtsDwrfS0"
      },
      "source": [
        "Impossible to handle this amount of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2QvtZjRPepo",
        "outputId": "c8e578c8-b09c-4241-ccf3-b7be01c065a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((810000, 678112), scipy.sparse._csr.csr_matrix)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X['f_27'] = X['f_27'].astype('category')\n",
        "drop_enc = OneHotEncoder(drop='first')\n",
        "sparse = drop_enc.fit_transform(X[['f_27']])\n",
        "sparse.shape, type(sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk4hkR-pPepo"
      },
      "source": [
        "Dimentionally reduction the one hot encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apdHGWFsPepo",
        "outputId": "67b82de0-b21e-49bc-9f01-f2c2f85fe7b4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=10)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "TruncatedSVD(n_components=10)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.46741814e-05 1.32668698e-05 1.17623774e-05 1.17496912e-05\n",
            " 1.03026316e-05 1.02071320e-05 9.98196498e-06 9.45805114e-06\n",
            " 9.19224881e-06 8.83016400e-06]\n",
            "0.00010942531230727259\n",
            "[3.43478639 3.25197463 3.0571312  3.04839654 2.85091836 2.83100822\n",
            " 2.79509639 2.71544367 2.68575579 2.63845535]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "pca = TruncatedSVD(n_components=10)\n",
        "pca.fit(sparse)\n",
        "\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.explained_variance_ratio_.sum())\n",
        "\n",
        "print(pca.singular_values_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llqmbaOrPepp"
      },
      "source": [
        "### Clusterization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9_ua3VNrxNe"
      },
      "source": [
        "I tried several unsupervised clusterization but none of them performed good or even support sparse matrix. Here I show you the KMeans approch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-z7BwvHPepp",
        "outputId": "c45fe6b1-922b-48dc-95fd-3a3d651fe55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of elements asigned to each cluster: [810000]\n",
            "Number of elements asigned to each cluster: [809996      4]\n",
            "Number of elements asigned to each cluster: [809994      4      2]\n",
            "Number of elements asigned to each cluster: [809993      4      2      1]\n",
            "Number of elements asigned to each cluster: [809990      4      2      1      3]\n",
            "Number of elements asigned to each cluster: [809988      4      2      1      3      2]\n",
            "Number of elements asigned to each cluster: [809984      4      2      1      3      2      4]\n",
            "Number of elements asigned to each cluster: [809980      4      2      3      2      4      3      2]\n",
            "Number of elements asigned to each cluster: [809979      4      2      3      2      4      3      2      1]\n",
            "Number of elements asigned to each cluster: [809978      4      2      3      2      4      3      2      1      1]\n",
            "\n",
            "True number of documents in each category according to the class labels: [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "for c in range(1, 11):\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=c,\n",
        "        max_iter=300,\n",
        "        n_init='auto',\n",
        "        random_state=42,\n",
        "    ).fit(sparse)\n",
        "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
        "    print(f\"Number of elements asigned to each cluster: {cluster_sizes}\")\n",
        "print()\n",
        "print(\n",
        "    \"True number of documents in each category according to the class labels: \"\n",
        "    f\"{cluster_ids}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uiIEJXEPepp"
      },
      "source": [
        "## Smart encoding f_27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql3wuAb0sJHZ"
      },
      "source": [
        "As I notice that it was kinda impossible to encode the f_27 with the previous techniques I tought to transform this feature into the number of occurrences that a letter of the code appear and encode each char position into ASCCII and remark which code appear in each position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TKV_DmFPepp",
        "outputId": "ba4c1015-3875-4d3b-9fb0-983d0a13885b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 10\n"
          ]
        }
      ],
      "source": [
        "df['f_27'] = df['f_27'].str.upper()\n",
        "df['length'] = df['f_27'].str.len()\n",
        "print(df['length'].min(), df['length'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in2Y0OF0Pepq",
        "outputId": "0bda536c-d0d9-448d-d061-1e54d5175449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((810000, 32), (810000,))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "FgO7jqYTPepq",
        "outputId": "ae91bd99-9b42-442d-a89b-09cb19e334c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Total number of columns (68) exceeds max_columns (50). Falling back to pandas display.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6db22645-e589-48ef-b063-ac5e151266af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_27</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>length</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>J</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>O</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>U</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>pos_0</th>\n",
              "      <th>pos_1</th>\n",
              "      <th>pos_2</th>\n",
              "      <th>pos_3</th>\n",
              "      <th>pos_4</th>\n",
              "      <th>pos_5</th>\n",
              "      <th>pos_6</th>\n",
              "      <th>pos_7</th>\n",
              "      <th>pos_8</th>\n",
              "      <th>pos_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.315471</td>\n",
              "      <td>-0.183690</td>\n",
              "      <td>0.664383</td>\n",
              "      <td>-1.186794</td>\n",
              "      <td>0.665098</td>\n",
              "      <td>0.946208</td>\n",
              "      <td>0.729857</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.690715</td>\n",
              "      <td>-0.628005</td>\n",
              "      <td>-2.832295</td>\n",
              "      <td>-1.409039</td>\n",
              "      <td>3.645067</td>\n",
              "      <td>0.233039</td>\n",
              "      <td>-3.754846</td>\n",
              "      <td>-1.061733</td>\n",
              "      <td>BDBBCACIBB</td>\n",
              "      <td>20.308715</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.286392</td>\n",
              "      <td>1.780592</td>\n",
              "      <td>0.576698</td>\n",
              "      <td>-2.690658</td>\n",
              "      <td>1.321997</td>\n",
              "      <td>-0.675894</td>\n",
              "      <td>0.371070</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.664517</td>\n",
              "      <td>-2.871912</td>\n",
              "      <td>3.826628</td>\n",
              "      <td>3.087653</td>\n",
              "      <td>0.494209</td>\n",
              "      <td>3.210875</td>\n",
              "      <td>-0.666457</td>\n",
              "      <td>0.123854</td>\n",
              "      <td>ACBDCBCADA</td>\n",
              "      <td>-449.291063</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.290303</td>\n",
              "      <td>-0.485907</td>\n",
              "      <td>0.808350</td>\n",
              "      <td>-0.156288</td>\n",
              "      <td>1.083632</td>\n",
              "      <td>-1.129914</td>\n",
              "      <td>0.767396</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.494988</td>\n",
              "      <td>-0.946303</td>\n",
              "      <td>2.333223</td>\n",
              "      <td>2.084169</td>\n",
              "      <td>-4.782668</td>\n",
              "      <td>-1.671375</td>\n",
              "      <td>2.774382</td>\n",
              "      <td>2.273130</td>\n",
              "      <td>AABBABCLAF</td>\n",
              "      <td>-86.206118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.243590</td>\n",
              "      <td>0.035112</td>\n",
              "      <td>-1.013236</td>\n",
              "      <td>0.854267</td>\n",
              "      <td>0.019192</td>\n",
              "      <td>0.597892</td>\n",
              "      <td>-2.020416</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-3.066427</td>\n",
              "      <td>-2.430158</td>\n",
              "      <td>-0.185332</td>\n",
              "      <td>-0.701691</td>\n",
              "      <td>-2.769142</td>\n",
              "      <td>-6.534231</td>\n",
              "      <td>-0.557677</td>\n",
              "      <td>-0.429972</td>\n",
              "      <td>ADBBABEEBA</td>\n",
              "      <td>-30.157403</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.702716</td>\n",
              "      <td>0.817044</td>\n",
              "      <td>-0.064907</td>\n",
              "      <td>-1.045483</td>\n",
              "      <td>0.718374</td>\n",
              "      <td>0.164451</td>\n",
              "      <td>-0.936620</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.899984</td>\n",
              "      <td>1.427460</td>\n",
              "      <td>-4.992610</td>\n",
              "      <td>1.154162</td>\n",
              "      <td>-1.931443</td>\n",
              "      <td>2.325042</td>\n",
              "      <td>2.143811</td>\n",
              "      <td>-1.039599</td>\n",
              "      <td>ABBBBBCMBB</td>\n",
              "      <td>296.484562</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6db22645-e589-48ef-b063-ac5e151266af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6db22645-e589-48ef-b063-ac5e151266af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6db22645-e589-48ef-b063-ac5e151266af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
              "0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
              "1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
              "2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
              "3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
              "4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
              "\n",
              "   f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
              "0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
              "1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
              "2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
              "3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
              "4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
              "\n",
              "       f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
              "0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
              "1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
              "2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
              "3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
              "4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
              "\n",
              "         f_27        f_28  f_29  f_30  length  A  B  C  D  E  F  G  H  I  J  \\\n",
              "0  BDBBCACIBB   20.308715     1     0      10  1  5  2  1  0  0  0  0  1  0   \n",
              "1  ACBDCBCADA -449.291063     1     0      10  3  2  3  2  0  0  0  0  0  0   \n",
              "2  AABBABCLAF  -86.206118     0     1      10  4  3  1  0  0  1  0  0  0  0   \n",
              "3  ADBBABEEBA  -30.157403     0     2      10  3  4  0  1  2  0  0  0  0  0   \n",
              "4  ABBBBBCMBB  296.484562     0     2      10  1  7  1  0  0  0  0  0  0  0   \n",
              "\n",
              "   K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z  pos_0  pos_1  pos_2  pos_3  \\\n",
              "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      1      3      1      1   \n",
              "1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      2      1      3   \n",
              "2  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      0      1      1   \n",
              "3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      3      1      1   \n",
              "4  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0      0      1      1      1   \n",
              "\n",
              "   pos_4  pos_5  pos_6  pos_7  pos_8  pos_9  \n",
              "0      2      0      2      8      1      1  \n",
              "1      2      1      2      0      3      0  \n",
              "2      0      1      2     11      0      5  \n",
              "3      0      1      4      4      1      0  \n",
              "4      1      1      2     12      1      1  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Counts the number of times a letter appear in the code\n",
        "def add_letters_count(data):\n",
        "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    for char in letters:\n",
        "        data[char] = data['f_27'].str.count(char)\n",
        "    return data\n",
        "\n",
        "# For each of the 10 position of the letter code I assign which code letter was assigned to it\n",
        "def add_letter_position(data):\n",
        "    for i in range(10):\n",
        "        data['pos_' + str(i)] = (data['f_27'].str[i]).apply(lambda x: ord(x)) - 65\n",
        "    return data\n",
        "\n",
        "X = add_letters_count(X)\n",
        "X = add_letter_position(X)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "82iY4qFAPepq",
        "outputId": "82596049-2cf2-430e-8120-76250d2f7b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Total number of columns (66) exceeds max_columns (50). Falling back to pandas display.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2cf155ae-1c3e-4818-895e-b2505de39774\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>J</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>O</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>U</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>pos_0</th>\n",
              "      <th>pos_1</th>\n",
              "      <th>pos_2</th>\n",
              "      <th>pos_3</th>\n",
              "      <th>pos_4</th>\n",
              "      <th>pos_5</th>\n",
              "      <th>pos_6</th>\n",
              "      <th>pos_7</th>\n",
              "      <th>pos_8</th>\n",
              "      <th>pos_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.315471</td>\n",
              "      <td>-0.183690</td>\n",
              "      <td>0.664383</td>\n",
              "      <td>-1.186794</td>\n",
              "      <td>0.665098</td>\n",
              "      <td>0.946208</td>\n",
              "      <td>0.729857</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-3.690715</td>\n",
              "      <td>-0.628005</td>\n",
              "      <td>-2.832295</td>\n",
              "      <td>-1.409039</td>\n",
              "      <td>3.645067</td>\n",
              "      <td>0.233039</td>\n",
              "      <td>-3.754846</td>\n",
              "      <td>-1.061733</td>\n",
              "      <td>20.308715</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.286392</td>\n",
              "      <td>1.780592</td>\n",
              "      <td>0.576698</td>\n",
              "      <td>-2.690658</td>\n",
              "      <td>1.321997</td>\n",
              "      <td>-0.675894</td>\n",
              "      <td>0.371070</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.664517</td>\n",
              "      <td>-2.871912</td>\n",
              "      <td>3.826628</td>\n",
              "      <td>3.087653</td>\n",
              "      <td>0.494209</td>\n",
              "      <td>3.210875</td>\n",
              "      <td>-0.666457</td>\n",
              "      <td>0.123854</td>\n",
              "      <td>-449.291063</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.290303</td>\n",
              "      <td>-0.485907</td>\n",
              "      <td>0.808350</td>\n",
              "      <td>-0.156288</td>\n",
              "      <td>1.083632</td>\n",
              "      <td>-1.129914</td>\n",
              "      <td>0.767396</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.494988</td>\n",
              "      <td>-0.946303</td>\n",
              "      <td>2.333223</td>\n",
              "      <td>2.084169</td>\n",
              "      <td>-4.782668</td>\n",
              "      <td>-1.671375</td>\n",
              "      <td>2.774382</td>\n",
              "      <td>2.273130</td>\n",
              "      <td>-86.206118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.243590</td>\n",
              "      <td>0.035112</td>\n",
              "      <td>-1.013236</td>\n",
              "      <td>0.854267</td>\n",
              "      <td>0.019192</td>\n",
              "      <td>0.597892</td>\n",
              "      <td>-2.020416</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-3.066427</td>\n",
              "      <td>-2.430158</td>\n",
              "      <td>-0.185332</td>\n",
              "      <td>-0.701691</td>\n",
              "      <td>-2.769142</td>\n",
              "      <td>-6.534231</td>\n",
              "      <td>-0.557677</td>\n",
              "      <td>-0.429972</td>\n",
              "      <td>-30.157403</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.702716</td>\n",
              "      <td>0.817044</td>\n",
              "      <td>-0.064907</td>\n",
              "      <td>-1.045483</td>\n",
              "      <td>0.718374</td>\n",
              "      <td>0.164451</td>\n",
              "      <td>-0.936620</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.899984</td>\n",
              "      <td>1.427460</td>\n",
              "      <td>-4.992610</td>\n",
              "      <td>1.154162</td>\n",
              "      <td>-1.931443</td>\n",
              "      <td>2.325042</td>\n",
              "      <td>2.143811</td>\n",
              "      <td>-1.039599</td>\n",
              "      <td>296.484562</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cf155ae-1c3e-4818-895e-b2505de39774')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cf155ae-1c3e-4818-895e-b2505de39774 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cf155ae-1c3e-4818-895e-b2505de39774');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
              "0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
              "1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
              "2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
              "3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
              "4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
              "\n",
              "   f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
              "0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
              "1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
              "2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
              "3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
              "4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
              "\n",
              "       f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
              "0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
              "1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
              "2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
              "3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
              "4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
              "\n",
              "         f_28  f_29  f_30  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  \\\n",
              "0   20.308715     1     0  1  5  2  1  0  0  0  0  1  0  0  0  0  0  0  0  0   \n",
              "1 -449.291063     1     0  3  2  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
              "2  -86.206118     0     1  4  3  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0   \n",
              "3  -30.157403     0     2  3  4  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0   \n",
              "4  296.484562     0     2  1  7  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0   \n",
              "\n",
              "   R  S  T  U  V  W  X  Y  Z  pos_0  pos_1  pos_2  pos_3  pos_4  pos_5  pos_6  \\\n",
              "0  0  0  0  0  0  0  0  0  0      1      3      1      1      2      0      2   \n",
              "1  0  0  0  0  0  0  0  0  0      0      2      1      3      2      1      2   \n",
              "2  0  0  0  0  0  0  0  0  0      0      0      1      1      0      1      2   \n",
              "3  0  0  0  0  0  0  0  0  0      0      3      1      1      0      1      4   \n",
              "4  0  0  0  0  0  0  0  0  0      0      1      1      1      1      1      2   \n",
              "\n",
              "   pos_7  pos_8  pos_9  \n",
              "0      8      1      1  \n",
              "1      0      3      0  \n",
              "2     11      0      5  \n",
              "3      4      1      0  \n",
              "4     12      1      1  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drops unneded features\n",
        "X = X.drop(['f_27', 'length'], axis=1)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "YjKTv1ThPepq",
        "outputId": "0768866f-b249-4b2a-ab0e-0f16d65f428b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Total number of columns (66) exceeds max_columns (50). Falling back to pandas display.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6bf6a9be-fe97-4aa0-bd7c-b937786148b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>J</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>O</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>U</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>pos_0</th>\n",
              "      <th>pos_1</th>\n",
              "      <th>pos_2</th>\n",
              "      <th>pos_3</th>\n",
              "      <th>pos_4</th>\n",
              "      <th>pos_5</th>\n",
              "      <th>pos_6</th>\n",
              "      <th>pos_7</th>\n",
              "      <th>pos_8</th>\n",
              "      <th>pos_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.315508</td>\n",
              "      <td>-0.185365</td>\n",
              "      <td>0.663485</td>\n",
              "      <td>-1.184789</td>\n",
              "      <td>0.665919</td>\n",
              "      <td>0.945566</td>\n",
              "      <td>0.731168</td>\n",
              "      <td>-1.226551</td>\n",
              "      <td>1.220861</td>\n",
              "      <td>-0.832130</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>-0.522922</td>\n",
              "      <td>-0.477867</td>\n",
              "      <td>1.145009</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>1.847503</td>\n",
              "      <td>-0.061861</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>-1.727077</td>\n",
              "      <td>-0.187118</td>\n",
              "      <td>-1.076850</td>\n",
              "      <td>-0.571024</td>\n",
              "      <td>1.635696</td>\n",
              "      <td>0.241191</td>\n",
              "      <td>-1.626472</td>\n",
              "      <td>-0.572666</td>\n",
              "      <td>0.086945</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "      <td>-1.083869</td>\n",
              "      <td>1.234584</td>\n",
              "      <td>0.490294</td>\n",
              "      <td>-0.031583</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>3.705131</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.003054</td>\n",
              "      <td>0.689322</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>0.472564</td>\n",
              "      <td>-0.937307</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>-0.240395</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-0.777127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.288412</td>\n",
              "      <td>1.780094</td>\n",
              "      <td>0.575879</td>\n",
              "      <td>-2.688532</td>\n",
              "      <td>1.322818</td>\n",
              "      <td>-0.676539</td>\n",
              "      <td>0.372214</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>2.078555</td>\n",
              "      <td>0.089454</td>\n",
              "      <td>-0.155626</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>2.502279</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>0.154349</td>\n",
              "      <td>-1.121819</td>\n",
              "      <td>1.603352</td>\n",
              "      <td>1.263992</td>\n",
              "      <td>0.351786</td>\n",
              "      <td>1.488661</td>\n",
              "      <td>-0.348562</td>\n",
              "      <td>-0.093987</td>\n",
              "      <td>-1.880083</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "      <td>0.399822</td>\n",
              "      <td>-0.874097</td>\n",
              "      <td>1.430813</td>\n",
              "      <td>1.044383</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>0.022464</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>0.632781</td>\n",
              "      <td>0.472564</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>-1.642581</td>\n",
              "      <td>0.610917</td>\n",
              "      <td>-1.382270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.291044</td>\n",
              "      <td>-0.487763</td>\n",
              "      <td>0.807325</td>\n",
              "      <td>-0.154365</td>\n",
              "      <td>1.084453</td>\n",
              "      <td>-1.130560</td>\n",
              "      <td>0.768724</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-0.665105</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.777817</td>\n",
              "      <td>0.656775</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-1.343932</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>2.512803</td>\n",
              "      <td>-0.346548</td>\n",
              "      <td>-0.319705</td>\n",
              "      <td>1.002260</td>\n",
              "      <td>0.854489</td>\n",
              "      <td>-1.798432</td>\n",
              "      <td>-0.556603</td>\n",
              "      <td>1.075183</td>\n",
              "      <td>0.773779</td>\n",
              "      <td>-0.359217</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>-0.002898</td>\n",
              "      <td>1.141668</td>\n",
              "      <td>-0.171203</td>\n",
              "      <td>-0.450225</td>\n",
              "      <td>-1.107548</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>1.252204</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>4.303646</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>-1.311252</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-1.068348</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>0.285425</td>\n",
              "      <td>-1.264398</td>\n",
              "      <td>1.643445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.244818</td>\n",
              "      <td>0.033569</td>\n",
              "      <td>-1.012640</td>\n",
              "      <td>0.856109</td>\n",
              "      <td>0.020013</td>\n",
              "      <td>0.597249</td>\n",
              "      <td>-2.020385</td>\n",
              "      <td>-0.019367</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.999962</td>\n",
              "      <td>1.715054</td>\n",
              "      <td>-1.173291</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>0.572765</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>-0.584500</td>\n",
              "      <td>0.596595</td>\n",
              "      <td>-1.457389</td>\n",
              "      <td>-0.937806</td>\n",
              "      <td>-0.011453</td>\n",
              "      <td>-0.282369</td>\n",
              "      <td>-0.977962</td>\n",
              "      <td>-2.593742</td>\n",
              "      <td>-0.303552</td>\n",
              "      <td>-0.317593</td>\n",
              "      <td>-0.124444</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "      <td>0.399822</td>\n",
              "      <td>0.531690</td>\n",
              "      <td>-1.390745</td>\n",
              "      <td>-0.031583</td>\n",
              "      <td>1.909363</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>0.689322</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-1.068348</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>0.979742</td>\n",
              "      <td>-0.941488</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-1.382270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.703249</td>\n",
              "      <td>0.815969</td>\n",
              "      <td>-0.065154</td>\n",
              "      <td>-1.043488</td>\n",
              "      <td>0.719195</td>\n",
              "      <td>0.163808</td>\n",
              "      <td>-0.936084</td>\n",
              "      <td>-0.622959</td>\n",
              "      <td>-0.036450</td>\n",
              "      <td>-0.221433</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.127447</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-1.456260</td>\n",
              "      <td>1.091772</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>1.235331</td>\n",
              "      <td>-0.953495</td>\n",
              "      <td>0.669086</td>\n",
              "      <td>-1.946372</td>\n",
              "      <td>0.474970</td>\n",
              "      <td>-0.636617</td>\n",
              "      <td>1.117569</td>\n",
              "      <td>0.814266</td>\n",
              "      <td>-0.563729</td>\n",
              "      <td>1.243772</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "      <td>-1.083869</td>\n",
              "      <td>2.640371</td>\n",
              "      <td>-0.450225</td>\n",
              "      <td>-1.107548</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>4.307805</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>-0.644394</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-0.297892</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>0.460699</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-0.777127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bf6a9be-fe97-4aa0-bd7c-b937786148b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6bf6a9be-fe97-4aa0-bd7c-b937786148b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6bf6a9be-fe97-4aa0-bd7c-b937786148b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0  0.315508 -0.185365  0.663485 -1.184789  0.665919  0.945566  0.731168   \n",
              "1 -1.288412  1.780094  0.575879 -2.688532  1.322818 -0.676539  0.372214   \n",
              "2 -0.291044 -0.487763  0.807325 -0.154365  1.084453 -1.130560  0.768724   \n",
              "3  1.244818  0.033569 -1.012640  0.856109  0.020013  0.597249 -2.020385   \n",
              "4  0.703249  0.815969 -0.065154 -1.043488  0.719195  0.163808 -0.936084   \n",
              "\n",
              "       f_07      f_08      f_09      f_10      f_11      f_12      f_13  \\\n",
              "0 -1.226551  1.220861 -0.832130  0.499632 -0.522922 -0.477867  1.145009   \n",
              "1  0.584225 -1.293760  0.389265  0.499632  2.078555  0.089454 -0.155626   \n",
              "2  0.584225 -0.665105  0.389265 -0.108080  0.777817  0.656775 -0.805943   \n",
              "3 -0.019367 -1.293760  0.999962  1.715054 -1.173291  1.224096 -0.805943   \n",
              "4 -0.622959 -0.036450 -0.221433 -0.108080  0.127447  1.224096 -1.456260   \n",
              "\n",
              "       f_14      f_15      f_16      f_17      f_18      f_19      f_20  \\\n",
              "0 -0.379035  1.847503 -0.061861 -1.265929 -0.680877 -1.727077 -0.187118   \n",
              "1 -1.114439 -0.701972  2.502279 -1.265929 -0.680877  0.154349 -1.121819   \n",
              "2 -0.379035 -0.701972 -1.343932  0.096928  2.512803 -0.346548 -0.319705   \n",
              "3 -1.114439  0.572765 -0.702897 -0.584500  0.596595 -1.457389 -0.937806   \n",
              "4  1.091772 -0.701972 -0.702897  0.096928  1.235331 -0.953495  0.669086   \n",
              "\n",
              "       f_21      f_22      f_23      f_24      f_25      f_26      f_28  \\\n",
              "0 -1.076850 -0.571024  1.635696  0.241191 -1.626472 -0.572666  0.086945   \n",
              "1  1.603352  1.263992  0.351786  1.488661 -0.348562 -0.093987 -1.880083   \n",
              "2  1.002260  0.854489 -1.798432 -0.556603  1.075183  0.773779 -0.359217   \n",
              "3 -0.011453 -0.282369 -0.977962 -2.593742 -0.303552 -0.317593 -0.124444   \n",
              "4 -1.946372  0.474970 -0.636617  1.117569  0.814266 -0.563729  1.243772   \n",
              "\n",
              "       f_29      f_30         A         B         C         D         E  \\\n",
              "0  1.376157 -1.224123 -1.083869  1.234584  0.490294 -0.031583 -0.814115   \n",
              "1  1.376157 -1.224123  0.399822 -0.874097  1.430813  1.044383 -0.814115   \n",
              "2 -0.726661 -0.002898  1.141668 -0.171203 -0.450225 -1.107548 -0.814115   \n",
              "3 -0.726661  1.218327  0.399822  0.531690 -1.390745 -0.031583  1.909363   \n",
              "4 -0.726661  1.218327 -1.083869  2.640371 -0.450225 -1.107548 -0.814115   \n",
              "\n",
              "          F         G         H         I         J         K         L  \\\n",
              "0 -0.576194 -0.411349 -0.311859  3.705131 -0.241643 -0.234669 -0.232101   \n",
              "1 -0.576194 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669 -0.232101   \n",
              "2  1.252204 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669  4.303646   \n",
              "3 -0.576194 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669 -0.232101   \n",
              "4 -0.576194 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669 -0.232101   \n",
              "\n",
              "          M         N         O         P         Q         R      S  \\\n",
              "0 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "1 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "2 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "3 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "4  4.307805 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "\n",
              "          T    U    V    W    X    Y    Z     pos_0     pos_1     pos_2  \\\n",
              "0 -0.213519  0.0  0.0  0.0  0.0  0.0  0.0  1.003054  0.689322  0.703296   \n",
              "1 -0.213519  0.0  0.0  0.0  0.0  0.0  0.0 -0.996955  0.022464  0.703296   \n",
              "2 -0.213519  0.0  0.0  0.0  0.0  0.0  0.0 -0.996955 -1.311252  0.703296   \n",
              "3 -0.213519  0.0  0.0  0.0  0.0  0.0  0.0 -0.996955  0.689322  0.703296   \n",
              "4 -0.213519  0.0  0.0  0.0  0.0  0.0  0.0 -0.996955 -0.644394  0.703296   \n",
              "\n",
              "      pos_3     pos_4     pos_5     pos_6     pos_7     pos_8     pos_9  \n",
              "0 -0.620112  0.472564 -0.937307 -0.228211 -0.240395 -0.639293 -0.777127  \n",
              "1  0.632781  0.472564  1.066887 -0.228211 -1.642581  0.610917 -1.382270  \n",
              "2 -0.620112 -1.068348  1.066887 -0.228211  0.285425 -1.264398  1.643445  \n",
              "3 -0.620112 -1.068348  1.066887  0.979742 -0.941488 -0.639293 -1.382270  \n",
              "4 -0.620112 -0.297892  1.066887 -0.228211  0.460699 -0.639293 -0.777127  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncdcszL7Pepr",
        "outputId": "3ed3a764-d106-4782-8280-5e3e27ed8dda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([810000, 66])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Declare the hole dataset\n",
        "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
        "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
        "X_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDII01Oiwx6m",
        "outputId": "943718a3-f170-42f6-da23-4b1486586349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready for new training process... =)\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
        "\n",
        "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Net(input_features=X_tensor.shape[1])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criteria = nn.BCEWithLogitsLoss()\n",
        "resume = 1\n",
        "v1_path = Path('../Output/NetV1.pth')\n",
        "\n",
        "recover = False\n",
        "\n",
        "if v1_path.exists() and recover:\n",
        "    print('Recovering old training process...')\n",
        "    checkpoint = torch.load(v1_path)\n",
        "    resume = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criteria = checkpoint['criteria']\n",
        "else:\n",
        "    print('Ready for new training process... =)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3psdKfuPepr"
      },
      "outputs": [],
      "source": [
        "wanna_train = False \n",
        "if wanna_train: # Doen't learn with the previous model\n",
        "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v1_path)\n",
        "else:\n",
        "    print('Not training this model anymore...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgRW6cEWPepr",
        "outputId": "b397d8a4-47f6-4905-ac2b-2a606132634e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying last checkpoint of model NetV1 state to trained folder\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "v1_path_trained = Path('../Trained/NetV1.pth')\n",
        "if v1_path.exists():\n",
        "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV1'))\n",
        "    shutil.copy(v1_path, v1_path_trained)\n",
        "else:\n",
        "    print('Model {0} is not created yeat'.format('NetV1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "IjfeFG6DPeps",
        "outputId": "247661d6-f5ad-4c1c-ab33-d0f505d1bc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (90000, 1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 209540,\n            'f': \"209540\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 182173,\n            'f': \"182173\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 195991,\n            'f': \"195991\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 43111,\n            'f': \"43111\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 489479,\n            'f': \"489479\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"id\"], [\"number\", \"target\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "\n",
              "  <div id=\"df-4eed7016-d916-4c8f-9794-5c749aabc805\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>209540</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>182173</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>195991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>489479</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4eed7016-d916-4c8f-9794-5c749aabc805')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4eed7016-d916-4c8f-9794-5c749aabc805 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4eed7016-d916-4c8f-9794-5c749aabc805');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id  target\n",
              "0  209540       0\n",
              "1  182173       0\n",
              "2  195991       1\n",
              "3   43111       0\n",
              "4  489479       1"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validate_path = Path('/content/drive/MyDrive/validate.csv') if IN_COLAB else Path('../Data/validate.csv')\n",
        "validate_df = pd.read_csv(validate_path)\n",
        "X_val = validate_df.drop('id', axis=1)\n",
        "X_val = add_letters_count(X_val)\n",
        "X_val = add_letter_position(X_val)\n",
        "\n",
        "X_val = X_val.drop('f_27', axis=1)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_val_tensor = torch.from_numpy(X_val)\n",
        "X_val_tensor = X_val_tensor.type(torch.float)\n",
        "X_val_tensor = X_val_tensor.to(device)\n",
        "\n",
        "v1_path_trained = Path('../Trained/NetV1.pth')\n",
        "v1_state = torch.load(v1_path_trained)\n",
        "v1_state_dict = v1_state['model_state_dict']\n",
        "v1_model = Net(X_val_tensor.shape[1])\n",
        "v1_model.load_state_dict(v1_state_dict)\n",
        "\n",
        "\n",
        "v1_model = v1_model.to(device)\n",
        "v1_model.eval()\n",
        "with torch.inference_mode():\n",
        "    outputs = v1_model(X_val_tensor)\n",
        "    y_pred = torch.round(torch.sigmoid(outputs))\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    print(type(y_pred), y_pred.shape)\n",
        "    \n",
        "submision_df = pd.DataFrame({\n",
        "    'id': validate_df['id'],\n",
        "    'target': y_pred.flatten()\n",
        "})\n",
        "\n",
        "submision_df = submision_df.astype('int')\n",
        "\n",
        "submit_path_model_v1 = Path('../Submit/V1.csv')\n",
        "submision_df.to_csv(submit_path_model_v1, index=False)\n",
        "submision_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBOA193Ls131"
      },
      "source": [
        "## Study of the f_27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q4t_FRhtDcS"
      },
      "source": [
        "I wanna check if each character of the abecedary appear either in the train set and the validation set. I was amazed because letter after T are not pressent, so what I can do is to drop this columns from the training and the test set and maybe improve my model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "HtPf1UNAtNzy",
        "outputId": "9bf070ec-3d4d-493b-b4bd-7c44d155f74d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 1,\n            'f': \"1\",\n        },\n\"A\",\n\"2214818\"],\n [{\n            'v': 0,\n            'f': \"0\",\n        },\n\"B\",\n\"2919752\"],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n\"C\",\n\"1331088\"],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n\"D\",\n\"926075\"],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n\"E\",\n\"537776\"],\n [{\n            'v': 5,\n            'f': \"5\",\n        },\n\"F\",\n\"283771\"],\n [{\n            'v': 6,\n            'f': \"6\",\n        },\n\"G\",\n\"147484\"],\n [{\n            'v': 7,\n            'f': \"7\",\n        },\n\"H\",\n\"84653\"],\n [{\n            'v': 8,\n            'f': \"8\",\n        },\n\"I\",\n\"59478\"],\n [{\n            'v': 9,\n            'f': \"9\",\n        },\n\"J\",\n\"50043\"],\n [{\n            'v': 10,\n            'f': \"10\",\n        },\n\"K\",\n\"47008\"],\n [{\n            'v': 12,\n            'f': \"12\",\n        },\n\"L\",\n\"46099\"],\n [{\n            'v': 11,\n            'f': \"11\",\n        },\n\"M\",\n\"46104\"],\n [{\n            'v': 13,\n            'f': \"13\",\n        },\n\"N\",\n\"45943\"],\n [{\n            'v': 14,\n            'f': \"14\",\n        },\n\"O\",\n\"45442\"],\n [{\n            'v': 15,\n            'f': \"15\",\n        },\n\"P\",\n\"45277\"],\n [{\n            'v': 16,\n            'f': \"16\",\n        },\n\"Q\",\n\"44641\"],\n [{\n            'v': 17,\n            'f': \"17\",\n        },\n\"R\",\n\"43438\"],\n [{\n            'v': 18,\n            'f': \"18\",\n        },\n\"S\",\n\"41835\"],\n [{\n            'v': 19,\n            'f': \"19\",\n        },\n\"T\",\n\"39275\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"letter\"], [\"string\", \"freq\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "\n",
              "  <div id=\"df-00eef1fd-f8fe-49d7-bf4d-30fd3468418e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>letter</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>2214818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B</td>\n",
              "      <td>2919752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C</td>\n",
              "      <td>1331088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>D</td>\n",
              "      <td>926075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E</td>\n",
              "      <td>537776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>F</td>\n",
              "      <td>283771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>G</td>\n",
              "      <td>147484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>H</td>\n",
              "      <td>84653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I</td>\n",
              "      <td>59478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>J</td>\n",
              "      <td>50043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>K</td>\n",
              "      <td>47008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>L</td>\n",
              "      <td>46099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>M</td>\n",
              "      <td>46104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>N</td>\n",
              "      <td>45943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>O</td>\n",
              "      <td>45442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>P</td>\n",
              "      <td>45277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Q</td>\n",
              "      <td>44641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>R</td>\n",
              "      <td>43438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>S</td>\n",
              "      <td>41835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>T</td>\n",
              "      <td>39275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00eef1fd-f8fe-49d7-bf4d-30fd3468418e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00eef1fd-f8fe-49d7-bf4d-30fd3468418e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00eef1fd-f8fe-49d7-bf4d-30fd3468418e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   letter     freq\n",
              "1       A  2214818\n",
              "0       B  2919752\n",
              "2       C  1331088\n",
              "3       D   926075\n",
              "4       E   537776\n",
              "5       F   283771\n",
              "6       G   147484\n",
              "7       H    84653\n",
              "8       I    59478\n",
              "9       J    50043\n",
              "10      K    47008\n",
              "12      L    46099\n",
              "11      M    46104\n",
              "13      N    45943\n",
              "14      O    45442\n",
              "15      P    45277\n",
              "16      Q    44641\n",
              "17      R    43438\n",
              "18      S    41835\n",
              "19      T    39275"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_path = Path('/content/drive/MyDrive/train.csv') if IN_COLAB else Path('../Data/train.csv')\n",
        "train_df = pd.read_csv(train_path)\n",
        "validate_path = Path('/content/drive/MyDrive/validate.csv') if IN_COLAB else Path('../Data/validate.csv')\n",
        "validate_df = pd.read_csv(validate_path)\n",
        "\n",
        "codes = pd.concat([train_df['f_27'], validate_df['f_27']], axis=0)\n",
        "codes = codes.map(lambda code : [c for c in code])\n",
        "codes = codes.explode()\n",
        "codes = codes.value_counts()\n",
        "data = np.array([list(codes.index), codes.values]).T\n",
        "frequencies = pd.DataFrame(data=data, columns = ['letter', 'freq'])\n",
        "frequencies.sort_values('letter')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZP6Pb22xlIA"
      },
      "outputs": [],
      "source": [
        "letters_to_drop = ['U', 'V', 'W', 'X', 'Y', 'Z']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2udJbX6ZyBf0",
        "outputId": "50d476bf-e93d-4073-c352-6483090b2873"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((810000, 32), (810000,))"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['f_27'] = df['f_27'].str.upper()\n",
        "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-sgXXYryT43",
        "outputId": "962d4441-e8e1-4efb-c386-75d12c076d33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
              " 0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
              " 1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
              " 2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
              " 3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
              " 4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
              " \n",
              "    f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
              " 0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
              " 1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
              " 2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
              " 3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
              " 4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
              " \n",
              "        f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
              " 0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
              " 1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
              " 2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
              " 3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
              " 4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
              " \n",
              "          f_27        f_28  f_29  f_30  length  A  B  C  D  E  F  G  H  I  J  \\\n",
              " 0  BDBBCACIBB   20.308715     1     0      10  1  5  2  1  0  0  0  0  1  0   \n",
              " 1  ACBDCBCADA -449.291063     1     0      10  3  2  3  2  0  0  0  0  0  0   \n",
              " 2  AABBABCLAF  -86.206118     0     1      10  4  3  1  0  0  1  0  0  0  0   \n",
              " 3  ADBBABEEBA  -30.157403     0     2      10  3  4  0  1  2  0  0  0  0  0   \n",
              " 4  ABBBBBCMBB  296.484562     0     2      10  1  7  1  0  0  0  0  0  0  0   \n",
              " \n",
              "    K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z  pos_0  pos_1  pos_2  pos_3  \\\n",
              " 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      1      3      1      1   \n",
              " 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      2      1      3   \n",
              " 2  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      0      1      1   \n",
              " 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      3      1      1   \n",
              " 4  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0      0      1      1      1   \n",
              " \n",
              "    pos_4  pos_5  pos_6  pos_7  pos_8  pos_9  \n",
              " 0      2      0      2      8      1      1  \n",
              " 1      2      1      2      0      3      0  \n",
              " 2      0      1      2     11      0      5  \n",
              " 3      0      1      4      4      1      0  \n",
              " 4      1      1      2     12      1      1  , (810000, 68))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Counts the number of times a letter appear in the code\n",
        "def add_letters_count(data):\n",
        "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    for char in letters:\n",
        "        data[char] = data['f_27'].str.count(char)\n",
        "    return data\n",
        "\n",
        "# For each of the 10 position of the letter code I assign which code letter was assigned to it\n",
        "def add_letter_position(data):\n",
        "    for i in range(10):\n",
        "        data['pos_' + str(i)] = (data['f_27'].str[i]).apply(lambda x: ord(x)) - 65\n",
        "    return data\n",
        "\n",
        "X = add_letters_count(X)\n",
        "X = add_letter_position(X)\n",
        "X.head(), X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN7n3gCOwcGk",
        "outputId": "6b211e89-0708-4382-c28c-c31231febbd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
              " 0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
              " 1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
              " 2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
              " 3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
              " 4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
              " \n",
              "    f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
              " 0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
              " 1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
              " 2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
              " 3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
              " 4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
              " \n",
              "        f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
              " 0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
              " 1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
              " 2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
              " 3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
              " 4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
              " \n",
              "          f_28  f_29  f_30  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  \\\n",
              " 0   20.308715     1     0  1  5  2  1  0  0  0  0  1  0  0  0  0  0  0  0  0   \n",
              " 1 -449.291063     1     0  3  2  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
              " 2  -86.206118     0     1  4  3  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0   \n",
              " 3  -30.157403     0     2  3  4  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0   \n",
              " 4  296.484562     0     2  1  7  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0   \n",
              " \n",
              "    R  S  T  pos_0  pos_1  pos_2  pos_3  pos_4  pos_5  pos_6  pos_7  pos_8  \\\n",
              " 0  0  0  0      1      3      1      1      2      0      2      8      1   \n",
              " 1  0  0  0      0      2      1      3      2      1      2      0      3   \n",
              " 2  0  0  0      0      0      1      1      0      1      2     11      0   \n",
              " 3  0  0  0      0      3      1      1      0      1      4      4      1   \n",
              " 4  0  0  0      0      1      1      1      1      1      2     12      1   \n",
              " \n",
              "    pos_9  \n",
              " 0      1  \n",
              " 1      0  \n",
              " 2      5  \n",
              " 3      0  \n",
              " 4      1  , (810000, 60))"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = X.drop(['length', 'f_27'] + letters_to_drop, axis=1, errors='ignore')\n",
        "X.head(), X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "Ni-mPzFSzPt8",
        "outputId": "8e8c0659-898a-4285-a716-2219a4f3e81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Total number of columns (60) exceeds max_columns (50). Falling back to pandas display.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e7cc7d53-8e68-4303-bf7a-d112c8b4a6e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "      <th>E</th>\n",
              "      <th>F</th>\n",
              "      <th>G</th>\n",
              "      <th>H</th>\n",
              "      <th>I</th>\n",
              "      <th>J</th>\n",
              "      <th>K</th>\n",
              "      <th>L</th>\n",
              "      <th>M</th>\n",
              "      <th>N</th>\n",
              "      <th>O</th>\n",
              "      <th>P</th>\n",
              "      <th>Q</th>\n",
              "      <th>R</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>pos_0</th>\n",
              "      <th>pos_1</th>\n",
              "      <th>pos_2</th>\n",
              "      <th>pos_3</th>\n",
              "      <th>pos_4</th>\n",
              "      <th>pos_5</th>\n",
              "      <th>pos_6</th>\n",
              "      <th>pos_7</th>\n",
              "      <th>pos_8</th>\n",
              "      <th>pos_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.315508</td>\n",
              "      <td>-0.185365</td>\n",
              "      <td>0.663485</td>\n",
              "      <td>-1.184789</td>\n",
              "      <td>0.665919</td>\n",
              "      <td>0.945566</td>\n",
              "      <td>0.731168</td>\n",
              "      <td>-1.226551</td>\n",
              "      <td>1.220861</td>\n",
              "      <td>-0.832130</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>-0.522922</td>\n",
              "      <td>-0.477867</td>\n",
              "      <td>1.145009</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>1.847503</td>\n",
              "      <td>-0.061861</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>-1.727077</td>\n",
              "      <td>-0.187118</td>\n",
              "      <td>-1.076850</td>\n",
              "      <td>-0.571024</td>\n",
              "      <td>1.635696</td>\n",
              "      <td>0.241191</td>\n",
              "      <td>-1.626472</td>\n",
              "      <td>-0.572666</td>\n",
              "      <td>0.086945</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "      <td>-1.083869</td>\n",
              "      <td>1.234584</td>\n",
              "      <td>0.490294</td>\n",
              "      <td>-0.031583</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>3.705131</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>1.003054</td>\n",
              "      <td>0.689322</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>0.472564</td>\n",
              "      <td>-0.937307</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>-0.240395</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-0.777127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.288412</td>\n",
              "      <td>1.780094</td>\n",
              "      <td>0.575879</td>\n",
              "      <td>-2.688532</td>\n",
              "      <td>1.322818</td>\n",
              "      <td>-0.676539</td>\n",
              "      <td>0.372214</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>2.078555</td>\n",
              "      <td>0.089454</td>\n",
              "      <td>-0.155626</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>2.502279</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>0.154349</td>\n",
              "      <td>-1.121819</td>\n",
              "      <td>1.603352</td>\n",
              "      <td>1.263992</td>\n",
              "      <td>0.351786</td>\n",
              "      <td>1.488661</td>\n",
              "      <td>-0.348562</td>\n",
              "      <td>-0.093987</td>\n",
              "      <td>-1.880083</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "      <td>0.399822</td>\n",
              "      <td>-0.874097</td>\n",
              "      <td>1.430813</td>\n",
              "      <td>1.044383</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>0.022464</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>0.632781</td>\n",
              "      <td>0.472564</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>-1.642581</td>\n",
              "      <td>0.610917</td>\n",
              "      <td>-1.382270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.291044</td>\n",
              "      <td>-0.487763</td>\n",
              "      <td>0.807325</td>\n",
              "      <td>-0.154365</td>\n",
              "      <td>1.084453</td>\n",
              "      <td>-1.130560</td>\n",
              "      <td>0.768724</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-0.665105</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.777817</td>\n",
              "      <td>0.656775</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-1.343932</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>2.512803</td>\n",
              "      <td>-0.346548</td>\n",
              "      <td>-0.319705</td>\n",
              "      <td>1.002260</td>\n",
              "      <td>0.854489</td>\n",
              "      <td>-1.798432</td>\n",
              "      <td>-0.556603</td>\n",
              "      <td>1.075183</td>\n",
              "      <td>0.773779</td>\n",
              "      <td>-0.359217</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>-0.002898</td>\n",
              "      <td>1.141668</td>\n",
              "      <td>-0.171203</td>\n",
              "      <td>-0.450225</td>\n",
              "      <td>-1.107548</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>1.252204</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>4.303646</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>-1.311252</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-1.068348</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>0.285425</td>\n",
              "      <td>-1.264398</td>\n",
              "      <td>1.643445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.244818</td>\n",
              "      <td>0.033569</td>\n",
              "      <td>-1.012640</td>\n",
              "      <td>0.856109</td>\n",
              "      <td>0.020013</td>\n",
              "      <td>0.597249</td>\n",
              "      <td>-2.020385</td>\n",
              "      <td>-0.019367</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.999962</td>\n",
              "      <td>1.715054</td>\n",
              "      <td>-1.173291</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>0.572765</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>-0.584500</td>\n",
              "      <td>0.596595</td>\n",
              "      <td>-1.457389</td>\n",
              "      <td>-0.937806</td>\n",
              "      <td>-0.011453</td>\n",
              "      <td>-0.282369</td>\n",
              "      <td>-0.977962</td>\n",
              "      <td>-2.593742</td>\n",
              "      <td>-0.303552</td>\n",
              "      <td>-0.317593</td>\n",
              "      <td>-0.124444</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "      <td>0.399822</td>\n",
              "      <td>0.531690</td>\n",
              "      <td>-1.390745</td>\n",
              "      <td>-0.031583</td>\n",
              "      <td>1.909363</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>-0.232078</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>0.689322</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-1.068348</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>0.979742</td>\n",
              "      <td>-0.941488</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-1.382270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.703249</td>\n",
              "      <td>0.815969</td>\n",
              "      <td>-0.065154</td>\n",
              "      <td>-1.043488</td>\n",
              "      <td>0.719195</td>\n",
              "      <td>0.163808</td>\n",
              "      <td>-0.936084</td>\n",
              "      <td>-0.622959</td>\n",
              "      <td>-0.036450</td>\n",
              "      <td>-0.221433</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.127447</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-1.456260</td>\n",
              "      <td>1.091772</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>1.235331</td>\n",
              "      <td>-0.953495</td>\n",
              "      <td>0.669086</td>\n",
              "      <td>-1.946372</td>\n",
              "      <td>0.474970</td>\n",
              "      <td>-0.636617</td>\n",
              "      <td>1.117569</td>\n",
              "      <td>0.814266</td>\n",
              "      <td>-0.563729</td>\n",
              "      <td>1.243772</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "      <td>-1.083869</td>\n",
              "      <td>2.640371</td>\n",
              "      <td>-0.450225</td>\n",
              "      <td>-1.107548</td>\n",
              "      <td>-0.814115</td>\n",
              "      <td>-0.576194</td>\n",
              "      <td>-0.411349</td>\n",
              "      <td>-0.311859</td>\n",
              "      <td>-0.262396</td>\n",
              "      <td>-0.241643</td>\n",
              "      <td>-0.234669</td>\n",
              "      <td>-0.232101</td>\n",
              "      <td>4.307805</td>\n",
              "      <td>-0.231841</td>\n",
              "      <td>-0.230273</td>\n",
              "      <td>-0.229907</td>\n",
              "      <td>-0.228613</td>\n",
              "      <td>-0.225631</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.213519</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>-0.644394</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-0.297892</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>0.460699</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-0.777127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7cc7d53-8e68-4303-bf7a-d112c8b4a6e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e7cc7d53-8e68-4303-bf7a-d112c8b4a6e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e7cc7d53-8e68-4303-bf7a-d112c8b4a6e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0  0.315508 -0.185365  0.663485 -1.184789  0.665919  0.945566  0.731168   \n",
              "1 -1.288412  1.780094  0.575879 -2.688532  1.322818 -0.676539  0.372214   \n",
              "2 -0.291044 -0.487763  0.807325 -0.154365  1.084453 -1.130560  0.768724   \n",
              "3  1.244818  0.033569 -1.012640  0.856109  0.020013  0.597249 -2.020385   \n",
              "4  0.703249  0.815969 -0.065154 -1.043488  0.719195  0.163808 -0.936084   \n",
              "\n",
              "       f_07      f_08      f_09      f_10      f_11      f_12      f_13  \\\n",
              "0 -1.226551  1.220861 -0.832130  0.499632 -0.522922 -0.477867  1.145009   \n",
              "1  0.584225 -1.293760  0.389265  0.499632  2.078555  0.089454 -0.155626   \n",
              "2  0.584225 -0.665105  0.389265 -0.108080  0.777817  0.656775 -0.805943   \n",
              "3 -0.019367 -1.293760  0.999962  1.715054 -1.173291  1.224096 -0.805943   \n",
              "4 -0.622959 -0.036450 -0.221433 -0.108080  0.127447  1.224096 -1.456260   \n",
              "\n",
              "       f_14      f_15      f_16      f_17      f_18      f_19      f_20  \\\n",
              "0 -0.379035  1.847503 -0.061861 -1.265929 -0.680877 -1.727077 -0.187118   \n",
              "1 -1.114439 -0.701972  2.502279 -1.265929 -0.680877  0.154349 -1.121819   \n",
              "2 -0.379035 -0.701972 -1.343932  0.096928  2.512803 -0.346548 -0.319705   \n",
              "3 -1.114439  0.572765 -0.702897 -0.584500  0.596595 -1.457389 -0.937806   \n",
              "4  1.091772 -0.701972 -0.702897  0.096928  1.235331 -0.953495  0.669086   \n",
              "\n",
              "       f_21      f_22      f_23      f_24      f_25      f_26      f_28  \\\n",
              "0 -1.076850 -0.571024  1.635696  0.241191 -1.626472 -0.572666  0.086945   \n",
              "1  1.603352  1.263992  0.351786  1.488661 -0.348562 -0.093987 -1.880083   \n",
              "2  1.002260  0.854489 -1.798432 -0.556603  1.075183  0.773779 -0.359217   \n",
              "3 -0.011453 -0.282369 -0.977962 -2.593742 -0.303552 -0.317593 -0.124444   \n",
              "4 -1.946372  0.474970 -0.636617  1.117569  0.814266 -0.563729  1.243772   \n",
              "\n",
              "       f_29      f_30         A         B         C         D         E  \\\n",
              "0  1.376157 -1.224123 -1.083869  1.234584  0.490294 -0.031583 -0.814115   \n",
              "1  1.376157 -1.224123  0.399822 -0.874097  1.430813  1.044383 -0.814115   \n",
              "2 -0.726661 -0.002898  1.141668 -0.171203 -0.450225 -1.107548 -0.814115   \n",
              "3 -0.726661  1.218327  0.399822  0.531690 -1.390745 -0.031583  1.909363   \n",
              "4 -0.726661  1.218327 -1.083869  2.640371 -0.450225 -1.107548 -0.814115   \n",
              "\n",
              "          F         G         H         I         J         K         L  \\\n",
              "0 -0.576194 -0.411349 -0.311859  3.705131 -0.241643 -0.234669 -0.232101   \n",
              "1 -0.576194 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669 -0.232101   \n",
              "2  1.252204 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669  4.303646   \n",
              "3 -0.576194 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669 -0.232101   \n",
              "4 -0.576194 -0.411349 -0.311859 -0.262396 -0.241643 -0.234669 -0.232101   \n",
              "\n",
              "          M         N         O         P         Q         R      S  \\\n",
              "0 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "1 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "2 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "3 -0.232078 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "4  4.307805 -0.231841 -0.230273 -0.229907 -0.228613 -0.225631 -0.221   \n",
              "\n",
              "          T     pos_0     pos_1     pos_2     pos_3     pos_4     pos_5  \\\n",
              "0 -0.213519  1.003054  0.689322  0.703296 -0.620112  0.472564 -0.937307   \n",
              "1 -0.213519 -0.996955  0.022464  0.703296  0.632781  0.472564  1.066887   \n",
              "2 -0.213519 -0.996955 -1.311252  0.703296 -0.620112 -1.068348  1.066887   \n",
              "3 -0.213519 -0.996955  0.689322  0.703296 -0.620112 -1.068348  1.066887   \n",
              "4 -0.213519 -0.996955 -0.644394  0.703296 -0.620112 -0.297892  1.066887   \n",
              "\n",
              "      pos_6     pos_7     pos_8     pos_9  \n",
              "0 -0.228211 -0.240395 -0.639293 -0.777127  \n",
              "1 -0.228211 -1.642581  0.610917 -1.382270  \n",
              "2 -0.228211  0.285425 -1.264398  1.643445  \n",
              "3  0.979742 -0.941488 -0.639293 -1.382270  \n",
              "4 -0.228211  0.460699 -0.639293 -0.777127  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKZAlLKKz18V",
        "outputId": "c953e23d-9802-4496-8ce8-e7fd33369d38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([810000, 60]), torch.Size([810000]))"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Declare the hole dataset\n",
        "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
        "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
        "X_tensor.shape, y_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABoJ-K2guCM5",
        "outputId": "e6455881-2408-416c-c7b8-2688bf0f4fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "810000 728999 81001\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
        "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
        "\n",
        "print(len(dataset), len(train_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK1jW-XLtPPZ",
        "outputId": "8db4c80a-749c-451b-f7fd-c3cd7b968766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready for new training process... =) | Running with cuda device\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Net(input_features=X_tensor.shape[1])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criteria = nn.BCEWithLogitsLoss()\n",
        "resume = 1\n",
        "v2_path = Path('../Output/NetV2.pth')\n",
        "\n",
        "recover = False\n",
        "\n",
        "if v2_path.exists() and recover:\n",
        "    print('Recovering old training process... | Running with {} device'.format(device))\n",
        "    checkpoint = torch.load(v2_path)\n",
        "    resume = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criteria = checkpoint['criteria']\n",
        "else:\n",
        "    print('Ready for new training process... =) | Running with {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhQenQkttPNP",
        "outputId": "89006d4f-1c87-46ab-c6eb-61e8e7d803f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/100 | Train loss: 92.207586 | Train accuracy: 0.84111775 | Test loss: 92.49268 | Test accuracy: 0.84047111]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[2/100 | Train loss: 73.618109 | Train accuracy: 0.87625909 | Test loss: 74.436386 | Test accuracy: 0.87298922]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[3/100 | Train loss: 62.744496 | Train accuracy: 0.89661166 | Test loss: 63.49813 | Test accuracy: 0.89465562]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[4/100 | Train loss: 52.449189 | Train accuracy: 0.91646491 | Test loss: 53.361474 | Test accuracy: 0.91443316]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[5/100 | Train loss: 48.506334 | Train accuracy: 0.92234009 | Test loss: 49.900975 | Test accuracy: 0.91947013]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[6/100 | Train loss: 43.573783 | Train accuracy: 0.9307379 | Test loss: 45.2047 | Test accuracy: 0.92767991]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[7/100 | Train loss: 42.506599 | Train accuracy: 0.93275437 | Test loss: 44.262214 | Test accuracy: 0.92979099]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[8/100 | Train loss: 38.790805 | Train accuracy: 0.9392482 | Test loss: 40.498677 | Test accuracy: 0.93640819]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[9/100 | Train loss: 38.879853 | Train accuracy: 0.93828661 | Test loss: 41.327781 | Test accuracy: 0.93397612]\n",
            "Trigger times: 1/5\n",
            "[10/100 | Train loss: 36.008898 | Train accuracy: 0.94293682 | Test loss: 37.551597 | Test accuracy: 0.94091431]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[11/100 | Train loss: 34.592485 | Train accuracy: 0.94653079 | Test loss: 36.796864 | Test accuracy: 0.94265503]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[12/100 | Train loss: 36.14857 | Train accuracy: 0.94375301 | Test loss: 38.420721 | Test accuracy: 0.94076616]\n",
            "Trigger times: 1/5\n",
            "[13/100 | Train loss: 33.57733 | Train accuracy: 0.9473072 | Test loss: 35.672504 | Test accuracy: 0.94416118]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[14/100 | Train loss: 31.27242 | Train accuracy: 0.95135659 | Test loss: 33.831651 | Test accuracy: 0.94791422]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[15/100 | Train loss: 31.169248 | Train accuracy: 0.95231818 | Test loss: 33.250141 | Test accuracy: 0.94811175]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[16/100 | Train loss: 30.343742 | Train accuracy: 0.95321393 | Test loss: 32.313635 | Test accuracy: 0.94970432]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[17/100 | Train loss: 30.331869 | Train accuracy: 0.95238402 | Test loss: 32.659846 | Test accuracy: 0.94903767]\n",
            "Trigger times: 1/5\n",
            "[18/100 | Train loss: 29.060042 | Train accuracy: 0.95464466 | Test loss: 31.370674 | Test accuracy: 0.9515685]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[19/100 | Train loss: 29.365379 | Train accuracy: 0.95445536 | Test loss: 31.522023 | Test accuracy: 0.9510253]\n",
            "Trigger times: 1/5\n",
            "[20/100 | Train loss: 29.153596 | Train accuracy: 0.95421256 | Test loss: 31.544265 | Test accuracy: 0.95091419]\n",
            "Trigger times: 2/5\n",
            "[21/100 | Train loss: 27.165004 | Train accuracy: 0.95775028 | Test loss: 29.89615 | Test accuracy: 0.95428452]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[22/100 | Train loss: 28.808145 | Train accuracy: 0.95607813 | Test loss: 30.813536 | Test accuracy: 0.95264256]\n",
            "Trigger times: 1/5\n",
            "[23/100 | Train loss: 26.366404 | Train accuracy: 0.95938952 | Test loss: 29.11834 | Test accuracy: 0.95513636]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[24/100 | Train loss: 28.95092 | Train accuracy: 0.95431269 | Test loss: 31.150035 | Test accuracy: 0.95160554]\n",
            "Trigger times: 1/5\n",
            "[25/100 | Train loss: 26.240696 | Train accuracy: 0.95996154 | Test loss: 28.519473 | Test accuracy: 0.95611165]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[26/100 | Train loss: 27.244846 | Train accuracy: 0.95734151 | Test loss: 29.578517 | Test accuracy: 0.95469192]\n",
            "Trigger times: 1/5\n",
            "[27/100 | Train loss: 27.28624 | Train accuracy: 0.95811928 | Test loss: 29.529931 | Test accuracy: 0.95456846]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[28/100 | Train loss: 25.207369 | Train accuracy: 0.9609149 | Test loss: 27.793184 | Test accuracy: 0.95738324]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[29/100 | Train loss: 24.682063 | Train accuracy: 0.96162821 | Test loss: 27.610418 | Test accuracy: 0.95719806]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[30/100 | Train loss: 24.40863 | Train accuracy: 0.96243753 | Test loss: 27.071664 | Test accuracy: 0.95863014]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[31/100 | Train loss: 25.220026 | Train accuracy: 0.96145262 | Test loss: 27.715667 | Test accuracy: 0.95754373]\n",
            "Trigger times: 1/5\n",
            "[32/100 | Train loss: 24.734672 | Train accuracy: 0.96186826 | Test loss: 27.38979 | Test accuracy: 0.95753139]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[33/100 | Train loss: 24.861893 | Train accuracy: 0.96161586 | Test loss: 27.386813 | Test accuracy: 0.95749435]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[34/100 | Train loss: 24.786658 | Train accuracy: 0.96147731 | Test loss: 28.014776 | Test accuracy: 0.95698819]\n",
            "Trigger times: 1/5\n",
            "[35/100 | Train loss: 24.777611 | Train accuracy: 0.96147731 | Test loss: 27.715066 | Test accuracy: 0.95714868]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[36/100 | Train loss: 24.581127 | Train accuracy: 0.96227018 | Test loss: 27.10161 | Test accuracy: 0.95797583]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[37/100 | Train loss: 24.054915 | Train accuracy: 0.96237992 | Test loss: 26.804082 | Test accuracy: 0.9584573]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[38/100 | Train loss: 24.095034 | Train accuracy: 0.96289707 | Test loss: 26.947975 | Test accuracy: 0.95922273]\n",
            "Trigger times: 1/5\n",
            "[39/100 | Train loss: 23.388777 | Train accuracy: 0.96354865 | Test loss: 26.298244 | Test accuracy: 0.95976593]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[40/100 | Train loss: 23.889056 | Train accuracy: 0.96331134 | Test loss: 26.67088 | Test accuracy: 0.95865483]\n",
            "Trigger times: 1/5\n",
            "[41/100 | Train loss: 23.96254 | Train accuracy: 0.96258431 | Test loss: 27.024202 | Test accuracy: 0.95811163]\n",
            "Trigger times: 2/5\n",
            "[42/100 | Train loss: 22.858834 | Train accuracy: 0.96499584 | Test loss: 25.870218 | Test accuracy: 0.9602721]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[43/100 | Train loss: 22.599213 | Train accuracy: 0.96529899 | Test loss: 26.066631 | Test accuracy: 0.96088937]\n",
            "Trigger times: 1/5\n",
            "[44/100 | Train loss: 23.08042 | Train accuracy: 0.96402876 | Test loss: 26.254916 | Test accuracy: 0.95972889]\n",
            "Trigger times: 2/5\n",
            "[45/100 | Train loss: 22.789393 | Train accuracy: 0.96507128 | Test loss: 26.215615 | Test accuracy: 0.95949433]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[46/100 | Train loss: 22.392284 | Train accuracy: 0.96553768 | Test loss: 25.812755 | Test accuracy: 0.96058073]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[47/100 | Train loss: 22.318455 | Train accuracy: 0.96541833 | Test loss: 25.678426 | Test accuracy: 0.96137085]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[48/100 | Train loss: 22.087922 | Train accuracy: 0.96593411 | Test loss: 25.65687 | Test accuracy: 0.96150665]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[49/100 | Train loss: 22.325036 | Train accuracy: 0.96554728 | Test loss: 25.919047 | Test accuracy: 0.96106221]\n",
            "Trigger times: 1/5\n",
            "[50/100 | Train loss: 22.437341 | Train accuracy: 0.96607951 | Test loss: 25.464439 | Test accuracy: 0.96111159]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[51/100 | Train loss: 22.947373 | Train accuracy: 0.96549652 | Test loss: 26.017546 | Test accuracy: 0.9609511]\n",
            "Trigger times: 1/5\n",
            "[52/100 | Train loss: 25.069621 | Train accuracy: 0.96098897 | Test loss: 28.677447 | Test accuracy: 0.9559882]\n",
            "Trigger times: 2/5\n",
            "[53/100 | Train loss: 22.002842 | Train accuracy: 0.96623727 | Test loss: 25.495727 | Test accuracy: 0.96138319]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[54/100 | Train loss: 23.57151 | Train accuracy: 0.96322492 | Test loss: 26.936943 | Test accuracy: 0.95893878]\n",
            "Trigger times: 1/5\n",
            "[55/100 | Train loss: 22.049796 | Train accuracy: 0.96580379 | Test loss: 25.849191 | Test accuracy: 0.9609511]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[56/100 | Train loss: 22.135467 | Train accuracy: 0.96579008 | Test loss: 25.577288 | Test accuracy: 0.96137085]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[57/100 | Train loss: 21.314735 | Train accuracy: 0.96717005 | Test loss: 25.217551 | Test accuracy: 0.96202516]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[58/100 | Train loss: 21.151813 | Train accuracy: 0.96725647 | Test loss: 25.185824 | Test accuracy: 0.96181529]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[59/100 | Train loss: 21.573041 | Train accuracy: 0.96686552 | Test loss: 25.615889 | Test accuracy: 0.96159307]\n",
            "Trigger times: 1/5\n",
            "[60/100 | Train loss: 21.367898 | Train accuracy: 0.9672016 | Test loss: 25.550822 | Test accuracy: 0.9620622]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[61/100 | Train loss: 21.930332 | Train accuracy: 0.9666433 | Test loss: 25.997144 | Test accuracy: 0.96174121]\n",
            "Trigger times: 1/5\n",
            "[62/100 | Train loss: 21.754558 | Train accuracy: 0.96701779 | Test loss: 26.065559 | Test accuracy: 0.96204985]\n",
            "Trigger times: 2/5\n",
            "[63/100 | Train loss: 21.589322 | Train accuracy: 0.9664979 | Test loss: 25.242497 | Test accuracy: 0.96202516]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[64/100 | Train loss: 21.832936 | Train accuracy: 0.96672561 | Test loss: 25.595772 | Test accuracy: 0.9613585]\n",
            "Trigger times: 1/5\n",
            "[65/100 | Train loss: 23.054515 | Train accuracy: 0.9646337 | Test loss: 27.162341 | Test accuracy: 0.95987704]\n",
            "Trigger times: 2/5\n",
            "[66/100 | Train loss: 21.686724 | Train accuracy: 0.9665898 | Test loss: 25.609423 | Test accuracy: 0.96090172]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[67/100 | Train loss: 20.944273 | Train accuracy: 0.96803699 | Test loss: 25.098022 | Test accuracy: 0.96248194]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[68/100 | Train loss: 20.789235 | Train accuracy: 0.96806306 | Test loss: 25.099174 | Test accuracy: 0.96256836]\n",
            "Trigger times: 1/5\n",
            "[69/100 | Train loss: 21.239688 | Train accuracy: 0.96723864 | Test loss: 25.584105 | Test accuracy: 0.96111159]\n",
            "Trigger times: 2/5\n",
            "[70/100 | Train loss: 21.333736 | Train accuracy: 0.96776813 | Test loss: 25.197545 | Test accuracy: 0.96172887]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[71/100 | Train loss: 21.097154 | Train accuracy: 0.96779419 | Test loss: 26.391512 | Test accuracy: 0.96069184]\n",
            "Trigger times: 1/5\n",
            "[72/100 | Train loss: 21.660168 | Train accuracy: 0.96636758 | Test loss: 26.094855 | Test accuracy: 0.9610869]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[73/100 | Train loss: 21.364755 | Train accuracy: 0.96707814 | Test loss: 25.423309 | Test accuracy: 0.96182763]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[74/100 | Train loss: 21.91453 | Train accuracy: 0.96703562 | Test loss: 25.926544 | Test accuracy: 0.96051901]\n",
            "Trigger times: 1/5\n",
            "[75/100 | Train loss: 20.984243 | Train accuracy: 0.96755277 | Test loss: 25.142697 | Test accuracy: 0.96188936]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[76/100 | Train loss: 20.982397 | Train accuracy: 0.96781203 | Test loss: 25.561196 | Test accuracy: 0.96235849]\n",
            "Trigger times: 1/5\n",
            "[77/100 | Train loss: 21.613677 | Train accuracy: 0.96641559 | Test loss: 26.323202 | Test accuracy: 0.96049431]\n",
            "Trigger times: 2/5\n",
            "[78/100 | Train loss: 20.935084 | Train accuracy: 0.96827568 | Test loss: 24.937358 | Test accuracy: 0.9626054]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[79/100 | Train loss: 21.038434 | Train accuracy: 0.96772286 | Test loss: 25.290349 | Test accuracy: 0.9620622]\n",
            "Trigger times: 1/5\n",
            "[80/100 | Train loss: 21.008207 | Train accuracy: 0.96798898 | Test loss: 25.351579 | Test accuracy: 0.96216096]\n",
            "Trigger times: 2/5\n",
            "[81/100 | Train loss: 20.48852 | Train accuracy: 0.96833055 | Test loss: 24.881732 | Test accuracy: 0.96238318]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[82/100 | Train loss: 20.292265 | Train accuracy: 0.96891079 | Test loss: 25.466272 | Test accuracy: 0.96212392]\n",
            "Trigger times: 1/5\n",
            "[83/100 | Train loss: 20.394404 | Train accuracy: 0.96884907 | Test loss: 24.989625 | Test accuracy: 0.96261775]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[84/100 | Train loss: 20.957247 | Train accuracy: 0.96755414 | Test loss: 26.136728 | Test accuracy: 0.96116097]\n",
            "Trigger times: 1/5\n",
            "[85/100 | Train loss: 20.561382 | Train accuracy: 0.96855826 | Test loss: 25.414735 | Test accuracy: 0.96281527]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[86/100 | Train loss: 21.168411 | Train accuracy: 0.96807815 | Test loss: 25.763369 | Test accuracy: 0.96185232]\n",
            "Trigger times: 1/5\n",
            "[87/100 | Train loss: 19.890809 | Train accuracy: 0.9695171 | Test loss: 24.837924 | Test accuracy: 0.96329675]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[88/100 | Train loss: 20.224213 | Train accuracy: 0.96891354 | Test loss: 25.017547 | Test accuracy: 0.96282762]\n",
            "Trigger times: 1/5\n",
            "[89/100 | Train loss: 21.641241 | Train accuracy: 0.96645949 | Test loss: 26.022565 | Test accuracy: 0.96155603]\n",
            "Trigger times: 2/5\n",
            "[90/100 | Train loss: 20.540888 | Train accuracy: 0.96837307 | Test loss: 25.804297 | Test accuracy: 0.96235849]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[91/100 | Train loss: 20.398042 | Train accuracy: 0.96896566 | Test loss: 26.058721 | Test accuracy: 0.96207454]\n",
            "Trigger times: 1/5\n",
            "[92/100 | Train loss: 19.956176 | Train accuracy: 0.96981066 | Test loss: 24.810587 | Test accuracy: 0.9631486]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[93/100 | Train loss: 20.181493 | Train accuracy: 0.96907678 | Test loss: 25.076827 | Test accuracy: 0.96303749]\n",
            "Trigger times: 1/5\n",
            "[94/100 | Train loss: 20.180069 | Train accuracy: 0.96880791 | Test loss: 25.145205 | Test accuracy: 0.9623338]\n",
            "Trigger times: 2/5\n",
            "[95/100 | Train loss: 19.86251 | Train accuracy: 0.96928116 | Test loss: 24.677318 | Test accuracy: 0.96367946]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[96/100 | Train loss: 19.680086 | Train accuracy: 0.96985867 | Test loss: 24.560617 | Test accuracy: 0.96387699]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[97/100 | Train loss: 20.433588 | Train accuracy: 0.96869543 | Test loss: 25.072216 | Test accuracy: 0.96330909]\n",
            "Trigger times: 1/5\n",
            "[98/100 | Train loss: 20.477747 | Train accuracy: 0.96840325 | Test loss: 25.267429 | Test accuracy: 0.96242022]\n",
            "Trigger times: 2/5\n",
            "[99/100 | Train loss: 20.521263 | Train accuracy: 0.96868994 | Test loss: 25.153458 | Test accuracy: 0.96332144]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[100/100 | Train loss: 20.405391 | Train accuracy: 0.96861724 | Test loss: 25.124538 | Test accuracy: 0.96293873]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n"
          ]
        }
      ],
      "source": [
        "wanna_train = False\n",
        "if wanna_train:\n",
        "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v2_path)\n",
        "else:\n",
        "    print('Not training this model anymore...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5I-LzlGtPK3",
        "outputId": "a1ca2a77-b060-4e9b-addc-f79d39624a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying last checkpoint of model NetV2 state to trained folder\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "v2_path_trained = Path('../Trained/NetV2.pth')\n",
        "if v2_path.exists():\n",
        "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV2'))\n",
        "    shutil.copy(v2_path, v2_path_trained)\n",
        "else:\n",
        "    print('Model {0} is not created yeat'.format('NetV2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "xsTbxkKdtPIh",
        "outputId": "7902376e-5315-4e1c-e196-4304abf07e9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 209540,\n            'f': \"209540\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 182173,\n            'f': \"182173\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 195991,\n            'f': \"195991\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 43111,\n            'f': \"43111\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 489479,\n            'f': \"489479\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"id\"], [\"number\", \"target\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "\n",
              "  <div id=\"df-d752fd88-ed66-4495-9ed8-58137e45d3c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>209540</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>182173</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>195991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>489479</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d752fd88-ed66-4495-9ed8-58137e45d3c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d752fd88-ed66-4495-9ed8-58137e45d3c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d752fd88-ed66-4495-9ed8-58137e45d3c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id  target\n",
              "0  209540       0\n",
              "1  182173       0\n",
              "2  195991       1\n",
              "3   43111       0\n",
              "4  489479       1"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validate_path = Path('/content/drive/MyDrive/validate.csv') if IN_COLAB else Path('../Data/validate.csv')\n",
        "validate_df = pd.read_csv(validate_path)\n",
        "X_val = validate_df.drop('id', axis=1)\n",
        "X_val = add_letters_count(X_val)\n",
        "X_val = add_letter_position(X_val)\n",
        "\n",
        "X_val = X_val.drop(['f_27'] + letters_to_drop, axis=1)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_val_tensor = torch.from_numpy(X_val)\n",
        "X_val_tensor = X_val_tensor.type(torch.float)\n",
        "X_val_tensor = X_val_tensor.to(device)\n",
        "\n",
        "v2_path_trained = Path('../Trained/NetV2.pth')\n",
        "v2_state = torch.load(v2_path_trained)\n",
        "v2_state_dict = v2_state['model_state_dict']\n",
        "v2_model = Net(X_val_tensor.shape[1])\n",
        "v2_model.load_state_dict(v2_state_dict)\n",
        "\n",
        "\n",
        "v2_model = v2_model.to(device)\n",
        "v2_model.eval()\n",
        "with torch.inference_mode():\n",
        "    outputs = v2_model(X_val_tensor)\n",
        "    y_pred = torch.round(torch.sigmoid(outputs))\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    \n",
        "submision_df = pd.DataFrame({\n",
        "    'id': validate_df['id'],\n",
        "    'target': y_pred.flatten()\n",
        "})\n",
        "\n",
        "submision_df = submision_df.astype('int')\n",
        "\n",
        "submit_path_model_v2 = Path('../Submit/V2.csv')\n",
        "submision_df.to_csv(submit_path_model_v2, index=False)\n",
        "submision_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH0yZsTttPGI"
      },
      "source": [
        "## What about if I only use the position of the letters in the code?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVmww5GYyIGO",
        "outputId": "f2ef48a8-251f-4b3d-c259-25257e7e5deb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((810000, 31), (810000,))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['f_27'] = df['f_27'].str.upper()\n",
        "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYp0dBa1tPD2",
        "outputId": "20773bce-8328-4f47-9cf3-9048bbde3d91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
              " 0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
              " 1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
              " 2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
              " 3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
              " 4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
              " \n",
              "    f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
              " 0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
              " 1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
              " 2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
              " 3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
              " 4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
              " \n",
              "        f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
              " 0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
              " 1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
              " 2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
              " 3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
              " 4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
              " \n",
              "          f_27        f_28  f_29  f_30  pos_0  pos_1  pos_2  pos_3  pos_4  \\\n",
              " 0  BDBBCACIBB   20.308715     1     0      1      3      1      1      2   \n",
              " 1  ACBDCBCADA -449.291063     1     0      0      2      1      3      2   \n",
              " 2  AABBABCLAF  -86.206118     0     1      0      0      1      1      0   \n",
              " 3  ADBBABEEBA  -30.157403     0     2      0      3      1      1      0   \n",
              " 4  ABBBBBCMBB  296.484562     0     2      0      1      1      1      1   \n",
              " \n",
              "    pos_5  pos_6  pos_7  pos_8  pos_9  \n",
              " 0      0      2      8      1      1  \n",
              " 1      1      2      0      3      0  \n",
              " 2      1      2     11      0      5  \n",
              " 3      1      4      4      1      0  \n",
              " 4      1      2     12      1      1  , (810000, 41))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For each of the 10 position of the letter code I assign which code letter was assigned to it\n",
        "def add_letter_position(data):\n",
        "    for i in range(10):\n",
        "        data['pos_' + str(i)] = (data['f_27'].str[i]).apply(lambda x: ord(x)) - 65\n",
        "    return data\n",
        "\n",
        "X = add_letter_position(X)\n",
        "X.head(), X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyo038SitPBM",
        "outputId": "f6f76975-6337-4c06-acfd-378d293a09e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
              " 0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
              " 1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
              " 2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
              " 3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
              " 4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
              " \n",
              "    f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
              " 0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
              " 1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
              " 2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
              " 3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
              " 4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
              " \n",
              "        f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
              " 0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
              " 1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
              " 2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
              " 3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
              " 4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
              " \n",
              "          f_28  f_29  f_30  pos_0  pos_1  pos_2  pos_3  pos_4  pos_5  pos_6  \\\n",
              " 0   20.308715     1     0      1      3      1      1      2      0      2   \n",
              " 1 -449.291063     1     0      0      2      1      3      2      1      2   \n",
              " 2  -86.206118     0     1      0      0      1      1      0      1      2   \n",
              " 3  -30.157403     0     2      0      3      1      1      0      1      4   \n",
              " 4  296.484562     0     2      0      1      1      1      1      1      2   \n",
              " \n",
              "    pos_7  pos_8  pos_9  \n",
              " 0      8      1      1  \n",
              " 1      0      3      0  \n",
              " 2     11      0      5  \n",
              " 3      4      1      0  \n",
              " 4     12      1      1  , (810000, 40))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = X.drop(['length', 'f_27'], axis=1, errors='ignore')\n",
        "X.head(), X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "FpGk4ag0tO-1",
        "outputId": "83eb0e7b-2098-458b-a28c-fadd7be2c76f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0.31550764835322115,\n            'f': \"0.31550764835322115\",\n        },\n{\n            'v': -0.18536497026221213,\n            'f': \"-0.18536497026221213\",\n        },\n{\n            'v': 0.6634854232504735,\n            'f': \"0.6634854232504735\",\n        },\n{\n            'v': -1.1847888970944762,\n            'f': \"-1.1847888970944762\",\n        },\n{\n            'v': 0.6659193383336577,\n            'f': \"0.6659193383336577\",\n        },\n{\n            'v': 0.9455656574854795,\n            'f': \"0.9455656574854795\",\n        },\n{\n            'v': 0.7311678619266998,\n            'f': \"0.7311678619266998\",\n        },\n{\n            'v': -1.2265510871085281,\n            'f': \"-1.2265510871085281\",\n        },\n{\n            'v': 1.2208608024620462,\n            'f': \"1.2208608024620462\",\n        },\n{\n            'v': -0.8321304841873368,\n            'f': \"-0.8321304841873368\",\n        },\n{\n            'v': 0.49963164280533257,\n            'f': \"0.49963164280533257\",\n        },\n{\n            'v': -0.5229216637347978,\n            'f': \"-0.5229216637347978\",\n        },\n{\n            'v': -0.47786702855742275,\n            'f': \"-0.47786702855742275\",\n        },\n{\n            'v': 1.14500870459034,\n            'f': \"1.14500870459034\",\n        },\n{\n            'v': -0.3790352285628641,\n            'f': \"-0.3790352285628641\",\n        },\n{\n            'v': 1.8475026617875556,\n            'f': \"1.8475026617875556\",\n        },\n{\n            'v': -0.061861472075263206,\n            'f': \"-0.061861472075263206\",\n        },\n{\n            'v': -1.2659285164833685,\n            'f': \"-1.2659285164833685\",\n        },\n{\n            'v': -0.6808766785869232,\n            'f': \"-0.6808766785869232\",\n        },\n{\n            'v': -1.7270770550645105,\n            'f': \"-1.7270770550645105\",\n        },\n{\n            'v': -0.18711828160447244,\n            'f': \"-0.18711828160447244\",\n        },\n{\n            'v': -1.0768500656162188,\n            'f': \"-1.0768500656162188\",\n        },\n{\n            'v': -0.5710238513176027,\n            'f': \"-0.5710238513176027\",\n        },\n{\n            'v': 1.6356960574419916,\n            'f': \"1.6356960574419916\",\n        },\n{\n            'v': 0.24119106729421572,\n            'f': \"0.24119106729421572\",\n        },\n{\n            'v': -1.6264720992256974,\n            'f': \"-1.6264720992256974\",\n        },\n{\n            'v': -0.5726656038308031,\n            'f': \"-0.5726656038308031\",\n        },\n{\n            'v': 0.0869446914680189,\n            'f': \"0.0869446914680189\",\n        },\n{\n            'v': 1.3761569552468826,\n            'f': \"1.3761569552468826\",\n        },\n{\n            'v': -1.2241226274703867,\n            'f': \"-1.2241226274703867\",\n        },\n{\n            'v': 1.0030540462937485,\n            'f': \"1.0030540462937485\",\n        },\n{\n            'v': 0.6893219644276584,\n            'f': \"0.6893219644276584\",\n        },\n{\n            'v': 0.7032957315794812,\n            'f': \"0.7032957315794812\",\n        },\n{\n            'v': -0.6201117720209613,\n            'f': \"-0.6201117720209613\",\n        },\n{\n            'v': 0.47256440799822125,\n            'f': \"0.47256440799822125\",\n        },\n{\n            'v': -0.937306786719727,\n            'f': \"-0.937306786719727\",\n        },\n{\n            'v': -0.2282105768478739,\n            'f': \"-0.2282105768478739\",\n        },\n{\n            'v': -0.24039468536733807,\n            'f': \"-0.24039468536733807\",\n        },\n{\n            'v': -0.6392932999446418,\n            'f': \"-0.6392932999446418\",\n        },\n{\n            'v': -0.7771268733560895,\n            'f': \"-0.7771268733560895\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': -1.2884117772666464,\n            'f': \"-1.2884117772666464\",\n        },\n{\n            'v': 1.7800942046412855,\n            'f': \"1.7800942046412855\",\n        },\n{\n            'v': 0.5758786414402264,\n            'f': \"0.5758786414402264\",\n        },\n{\n            'v': -2.6885320363815723,\n            'f': \"-2.6885320363815723\",\n        },\n{\n            'v': 1.322818321180399,\n            'f': \"1.322818321180399\",\n        },\n{\n            'v': -0.6765386848032976,\n            'f': \"-0.6765386848032976\",\n        },\n{\n            'v': 0.3722139595018628,\n            'f': \"0.3722139595018628\",\n        },\n{\n            'v': 0.5842248846007309,\n            'f': \"0.5842248846007309\",\n        },\n{\n            'v': -1.2937599690759416,\n            'f': \"-1.2937599690759416\",\n        },\n{\n            'v': 0.38926466592028836,\n            'f': \"0.38926466592028836\",\n        },\n{\n            'v': 0.49963164280533257,\n            'f': \"0.49963164280533257\",\n        },\n{\n            'v': 2.0785548191743826,\n            'f': \"2.0785548191743826\",\n        },\n{\n            'v': 0.08945390391982994,\n            'f': \"0.08945390391982994\",\n        },\n{\n            'v': -0.1556257268913447,\n            'f': \"-0.1556257268913447\",\n        },\n{\n            'v': -1.1144389099964074,\n            'f': \"-1.1144389099964074\",\n        },\n{\n            'v': -0.7019718361323636,\n            'f': \"-0.7019718361323636\",\n        },\n{\n            'v': 2.50227901590372,\n            'f': \"2.50227901590372\",\n        },\n{\n            'v': -1.2659285164833685,\n            'f': \"-1.2659285164833685\",\n        },\n{\n            'v': -0.6808766785869232,\n            'f': \"-0.6808766785869232\",\n        },\n{\n            'v': 0.15434921822141226,\n            'f': \"0.15434921822141226\",\n        },\n{\n            'v': -1.1218186290172945,\n            'f': \"-1.1218186290172945\",\n        },\n{\n            'v': 1.6033519093878472,\n            'f': \"1.6033519093878472\",\n        },\n{\n            'v': 1.2639916745860336,\n            'f': \"1.2639916745860336\",\n        },\n{\n            'v': 0.3517863709805076,\n            'f': \"0.3517863709805076\",\n        },\n{\n            'v': 1.488660926449109,\n            'f': \"1.488660926449109\",\n        },\n{\n            'v': -0.34856246652786793,\n            'f': \"-0.34856246652786793\",\n        },\n{\n            'v': -0.09398689069440648,\n            'f': \"-0.09398689069440648\",\n        },\n{\n            'v': -1.8800833118411049,\n            'f': \"-1.8800833118411049\",\n        },\n{\n            'v': 1.3761569552468826,\n            'f': \"1.3761569552468826\",\n        },\n{\n            'v': -1.2241226274703867,\n            'f': \"-1.2241226274703867\",\n        },\n{\n            'v': -0.9969552525060504,\n            'f': \"-0.9969552525060504\",\n        },\n{\n            'v': 0.022464055437894665,\n            'f': \"0.022464055437894665\",\n        },\n{\n            'v': 0.7032957315794812,\n            'f': \"0.7032957315794812\",\n        },\n{\n            'v': 0.6327814614971925,\n            'f': \"0.6327814614971925\",\n        },\n{\n            'v': 0.47256440799822125,\n            'f': \"0.47256440799822125\",\n        },\n{\n            'v': 1.0668865457591317,\n            'f': \"1.0668865457591317\",\n        },\n{\n            'v': -0.2282105768478739,\n            'f': \"-0.2282105768478739\",\n        },\n{\n            'v': -1.6425810694230878,\n            'f': \"-1.6425810694230878\",\n        },\n{\n            'v': 0.6109166216622436,\n            'f': \"0.6109166216622436\",\n        },\n{\n            'v': -1.3822698667260818,\n            'f': \"-1.3822698667260818\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': -0.2910442534566729,\n            'f': \"-0.2910442534566729\",\n        },\n{\n            'v': -0.48776291976580644,\n            'f': \"-0.48776291976580644\",\n        },\n{\n            'v': 0.8073247158112421,\n            'f': \"0.8073247158112421\",\n        },\n{\n            'v': -0.15436534749129568,\n            'f': \"-0.15436534749129568\",\n        },\n{\n            'v': 1.0844530742281455,\n            'f': \"1.0844530742281455\",\n        },\n{\n            'v': -1.1305596629638797,\n            'f': \"-1.1305596629638797\",\n        },\n{\n            'v': 0.7687241748129573,\n            'f': \"0.7687241748129573\",\n        },\n{\n            'v': 0.5842248846007309,\n            'f': \"0.5842248846007309\",\n        },\n{\n            'v': -0.6651047761914446,\n            'f': \"-0.6651047761914446\",\n        },\n{\n            'v': 0.38926466592028836,\n            'f': \"0.38926466592028836\",\n        },\n{\n            'v': -0.10807956214931722,\n            'f': \"-0.10807956214931722\",\n        },\n{\n            'v': 0.7778165777197923,\n            'f': \"0.7778165777197923\",\n        },\n{\n            'v': 0.6567748363970827,\n            'f': \"0.6567748363970827\",\n        },\n{\n            'v': -0.8059429426321871,\n            'f': \"-0.8059429426321871\",\n        },\n{\n            'v': -0.3790352285628641,\n            'f': \"-0.3790352285628641\",\n        },\n{\n            'v': -0.7019718361323636,\n            'f': \"-0.7019718361323636\",\n        },\n{\n            'v': -1.3439317160647546,\n            'f': \"-1.3439317160647546\",\n        },\n{\n            'v': 0.09692767862706389,\n            'f': \"0.09692767862706389\",\n        },\n{\n            'v': 2.5128027296835285,\n            'f': \"2.5128027296835285\",\n        },\n{\n            'v': -0.34654770670219204,\n            'f': \"-0.34654770670219204\",\n        },\n{\n            'v': -0.31970543988918615,\n            'f': \"-0.31970543988918615\",\n        },\n{\n            'v': 1.0022597323215872,\n            'f': \"1.0022597323215872\",\n        },\n{\n            'v': 0.8544887025247351,\n            'f': \"0.8544887025247351\",\n        },\n{\n            'v': -1.7984324468781616,\n            'f': \"-1.7984324468781616\",\n        },\n{\n            'v': -0.5566027081034819,\n            'f': \"-0.5566027081034819\",\n        },\n{\n            'v': 1.0751831227554074,\n            'f': \"1.0751831227554074\",\n        },\n{\n            'v': 0.773779456904109,\n            'f': \"0.773779456904109\",\n        },\n{\n            'v': -0.3592174851565914,\n            'f': \"-0.3592174851565914\",\n        },\n{\n            'v': -0.7266612984712926,\n            'f': \"-0.7266612984712926\",\n        },\n{\n            'v': -0.0028977705863347525,\n            'f': \"-0.0028977705863347525\",\n        },\n{\n            'v': -0.9969552525060504,\n            'f': \"-0.9969552525060504\",\n        },\n{\n            'v': -1.311251762541633,\n            'f': \"-1.311251762541633\",\n        },\n{\n            'v': 0.7032957315794812,\n            'f': \"0.7032957315794812\",\n        },\n{\n            'v': -0.6201117720209613,\n            'f': \"-0.6201117720209613\",\n        },\n{\n            'v': -1.0683475558298898,\n            'f': \"-1.0683475558298898\",\n        },\n{\n            'v': 1.0668865457591317,\n            'f': \"1.0668865457591317\",\n        },\n{\n            'v': -0.2282105768478739,\n            'f': \"-0.2282105768478739\",\n        },\n{\n            'v': 0.28542520865356813,\n            'f': \"0.28542520865356813\",\n        },\n{\n            'v': -1.2643982607480846,\n            'f': \"-1.2643982607480846\",\n        },\n{\n            'v': 1.643445100123879,\n            'f': \"1.643445100123879\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 1.2448177158946498,\n            'f': \"1.2448177158946498\",\n        },\n{\n            'v': 0.033569034820743686,\n            'f': \"0.033569034820743686\",\n        },\n{\n            'v': -1.0126395392612821,\n            'f': \"-1.0126395392612821\",\n        },\n{\n            'v': 0.8561085875502743,\n            'f': \"0.8561085875502743\",\n        },\n{\n            'v': 0.020012794956395803,\n            'f': \"0.020012794956395803\",\n        },\n{\n            'v': 0.5972493623459518,\n            'f': \"0.5972493623459518\",\n        },\n{\n            'v': -2.020385147963608,\n            'f': \"-2.020385147963608\",\n        },\n{\n            'v': -0.019367105969022167,\n            'f': \"-0.019367105969022167\",\n        },\n{\n            'v': -1.2937599690759416,\n            'f': \"-1.2937599690759416\",\n        },\n{\n            'v': 0.9999622409741009,\n            'f': \"0.9999622409741009\",\n        },\n{\n            'v': 1.715054052714632,\n            'f': \"1.715054052714632\",\n        },\n{\n            'v': -1.1732907844620928,\n            'f': \"-1.1732907844620928\",\n        },\n{\n            'v': 1.2240957688743352,\n            'f': \"1.2240957688743352\",\n        },\n{\n            'v': -0.8059429426321871,\n            'f': \"-0.8059429426321871\",\n        },\n{\n            'v': -1.1144389099964074,\n            'f': \"-1.1144389099964074\",\n        },\n{\n            'v': 0.572765412827596,\n            'f': \"0.572765412827596\",\n        },\n{\n            'v': -0.7028965940700089,\n            'f': \"-0.7028965940700089\",\n        },\n{\n            'v': -0.5845004189281523,\n            'f': \"-0.5845004189281523\",\n        },\n{\n            'v': 0.5965950847212574,\n            'f': \"0.5965950847212574\",\n        },\n{\n            'v': -1.4573892398693922,\n            'f': \"-1.4573892398693922\",\n        },\n{\n            'v': -0.9378058052081131,\n            'f': \"-0.9378058052081131\",\n        },\n{\n            'v': -0.011453277639074824,\n            'f': \"-0.011453277639074824\",\n        },\n{\n            'v': -0.28236871142875064,\n            'f': \"-0.28236871142875064\",\n        },\n{\n            'v': -0.9779621819274648,\n            'f': \"-0.9779621819274648\",\n        },\n{\n            'v': -2.593742140096376,\n            'f': \"-2.593742140096376\",\n        },\n{\n            'v': -0.3035518714866955,\n            'f': \"-0.3035518714866955\",\n        },\n{\n            'v': -0.317593228350196,\n            'f': \"-0.317593228350196\",\n        },\n{\n            'v': -0.12444439315892475,\n            'f': \"-0.12444439315892475\",\n        },\n{\n            'v': -0.7266612984712926,\n            'f': \"-0.7266612984712926\",\n        },\n{\n            'v': 1.2183270862977171,\n            'f': \"1.2183270862977171\",\n        },\n{\n            'v': -0.9969552525060504,\n            'f': \"-0.9969552525060504\",\n        },\n{\n            'v': 0.6893219644276584,\n            'f': \"0.6893219644276584\",\n        },\n{\n            'v': 0.7032957315794812,\n            'f': \"0.7032957315794812\",\n        },\n{\n            'v': -0.6201117720209613,\n            'f': \"-0.6201117720209613\",\n        },\n{\n            'v': -1.0683475558298898,\n            'f': \"-1.0683475558298898\",\n        },\n{\n            'v': 1.0668865457591317,\n            'f': \"1.0668865457591317\",\n        },\n{\n            'v': 0.9797420020708721,\n            'f': \"0.9797420020708721\",\n        },\n{\n            'v': -0.941487877395213,\n            'f': \"-0.941487877395213\",\n        },\n{\n            'v': -0.6392932999446418,\n            'f': \"-0.6392932999446418\",\n        },\n{\n            'v': -1.3822698667260818,\n            'f': \"-1.3822698667260818\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 0.7032490180108356,\n            'f': \"0.7032490180108356\",\n        },\n{\n            'v': 0.8159691698853119,\n            'f': \"0.8159691698853119\",\n        },\n{\n            'v': -0.0651543810081058,\n            'f': \"-0.0651543810081058\",\n        },\n{\n            'v': -1.0434884303490919,\n            'f': \"-1.0434884303490919\",\n        },\n{\n            'v': 0.7191947269407284,\n            'f': \"0.7191947269407284\",\n        },\n{\n            'v': 0.1638081472105642,\n            'f': \"0.1638081472105642\",\n        },\n{\n            'v': -0.9360843183881183,\n            'f': \"-0.9360843183881183\",\n        },\n{\n            'v': -0.6229590965387751,\n            'f': \"-0.6229590965387751\",\n        },\n{\n            'v': -0.03644958330694764,\n            'f': \"-0.03644958330694764\",\n        },\n{\n            'v': -0.22143290913352423,\n            'f': \"-0.22143290913352423\",\n        },\n{\n            'v': -0.10807956214931722,\n            'f': \"-0.10807956214931722\",\n        },\n{\n            'v': 0.12744745699249727,\n            'f': \"0.12744745699249727\",\n        },\n{\n            'v': 1.2240957688743352,\n            'f': \"1.2240957688743352\",\n        },\n{\n            'v': -1.4562601583730295,\n            'f': \"-1.4562601583730295\",\n        },\n{\n            'v': 1.0917721343042226,\n            'f': \"1.0917721343042226\",\n        },\n{\n            'v': -0.7019718361323636,\n            'f': \"-0.7019718361323636\",\n        },\n{\n            'v': -0.7028965940700089,\n            'f': \"-0.7028965940700089\",\n        },\n{\n            'v': 0.09692767862706389,\n            'f': \"0.09692767862706389\",\n        },\n{\n            'v': 1.2353309663753478,\n            'f': \"1.2353309663753478\",\n        },\n{\n            'v': -0.9534950949698254,\n            'f': \"-0.9534950949698254\",\n        },\n{\n            'v': 0.6690861769777094,\n            'f': \"0.6690861769777094\",\n        },\n{\n            'v': -1.9463720192154812,\n            'f': \"-1.9463720192154812\",\n        },\n{\n            'v': 0.4749702249150923,\n            'f': \"0.4749702249150923\",\n        },\n{\n            'v': -0.6366169800052985,\n            'f': \"-0.6366169800052985\",\n        },\n{\n            'v': 1.117569417358218,\n            'f': \"1.117569417358218\",\n        },\n{\n            'v': 0.8142662051005235,\n            'f': \"0.8142662051005235\",\n        },\n{\n            'v': -0.5637287894278201,\n            'f': \"-0.5637287894278201\",\n        },\n{\n            'v': 1.243771526333478,\n            'f': \"1.243771526333478\",\n        },\n{\n            'v': -0.7266612984712926,\n            'f': \"-0.7266612984712926\",\n        },\n{\n            'v': 1.2183270862977171,\n            'f': \"1.2183270862977171\",\n        },\n{\n            'v': -0.9969552525060504,\n            'f': \"-0.9969552525060504\",\n        },\n{\n            'v': -0.6443938535518691,\n            'f': \"-0.6443938535518691\",\n        },\n{\n            'v': 0.7032957315794812,\n            'f': \"0.7032957315794812\",\n        },\n{\n            'v': -0.6201117720209613,\n            'f': \"-0.6201117720209613\",\n        },\n{\n            'v': -0.29789157391583426,\n            'f': \"-0.29789157391583426\",\n        },\n{\n            'v': 1.0668865457591317,\n            'f': \"1.0668865457591317\",\n        },\n{\n            'v': -0.2282105768478739,\n            'f': \"-0.2282105768478739\",\n        },\n{\n            'v': 0.46069850666053686,\n            'f': \"0.46069850666053686\",\n        },\n{\n            'v': -0.6392932999446418,\n            'f': \"-0.6392932999446418\",\n        },\n{\n            'v': -0.7771268733560895,\n            'f': \"-0.7771268733560895\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"f_00\"], [\"number\", \"f_01\"], [\"number\", \"f_02\"], [\"number\", \"f_03\"], [\"number\", \"f_04\"], [\"number\", \"f_05\"], [\"number\", \"f_06\"], [\"number\", \"f_07\"], [\"number\", \"f_08\"], [\"number\", \"f_09\"], [\"number\", \"f_10\"], [\"number\", \"f_11\"], [\"number\", \"f_12\"], [\"number\", \"f_13\"], [\"number\", \"f_14\"], [\"number\", \"f_15\"], [\"number\", \"f_16\"], [\"number\", \"f_17\"], [\"number\", \"f_18\"], [\"number\", \"f_19\"], [\"number\", \"f_20\"], [\"number\", \"f_21\"], [\"number\", \"f_22\"], [\"number\", \"f_23\"], [\"number\", \"f_24\"], [\"number\", \"f_25\"], [\"number\", \"f_26\"], [\"number\", \"f_28\"], [\"number\", \"f_29\"], [\"number\", \"f_30\"], [\"number\", \"pos_0\"], [\"number\", \"pos_1\"], [\"number\", \"pos_2\"], [\"number\", \"pos_3\"], [\"number\", \"pos_4\"], [\"number\", \"pos_5\"], [\"number\", \"pos_6\"], [\"number\", \"pos_7\"], [\"number\", \"pos_8\"], [\"number\", \"pos_9\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "\n",
              "  <div id=\"df-0a98f2d2-b7e9-4297-aff2-79ee651a14d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>pos_0</th>\n",
              "      <th>pos_1</th>\n",
              "      <th>pos_2</th>\n",
              "      <th>pos_3</th>\n",
              "      <th>pos_4</th>\n",
              "      <th>pos_5</th>\n",
              "      <th>pos_6</th>\n",
              "      <th>pos_7</th>\n",
              "      <th>pos_8</th>\n",
              "      <th>pos_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.315508</td>\n",
              "      <td>-0.185365</td>\n",
              "      <td>0.663485</td>\n",
              "      <td>-1.184789</td>\n",
              "      <td>0.665919</td>\n",
              "      <td>0.945566</td>\n",
              "      <td>0.731168</td>\n",
              "      <td>-1.226551</td>\n",
              "      <td>1.220861</td>\n",
              "      <td>-0.832130</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>-0.522922</td>\n",
              "      <td>-0.477867</td>\n",
              "      <td>1.145009</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>1.847503</td>\n",
              "      <td>-0.061861</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>-1.727077</td>\n",
              "      <td>-0.187118</td>\n",
              "      <td>-1.076850</td>\n",
              "      <td>-0.571024</td>\n",
              "      <td>1.635696</td>\n",
              "      <td>0.241191</td>\n",
              "      <td>-1.626472</td>\n",
              "      <td>-0.572666</td>\n",
              "      <td>0.086945</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "      <td>1.003054</td>\n",
              "      <td>0.689322</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>0.472564</td>\n",
              "      <td>-0.937307</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>-0.240395</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-0.777127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.288412</td>\n",
              "      <td>1.780094</td>\n",
              "      <td>0.575879</td>\n",
              "      <td>-2.688532</td>\n",
              "      <td>1.322818</td>\n",
              "      <td>-0.676539</td>\n",
              "      <td>0.372214</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>0.499632</td>\n",
              "      <td>2.078555</td>\n",
              "      <td>0.089454</td>\n",
              "      <td>-0.155626</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>2.502279</td>\n",
              "      <td>-1.265929</td>\n",
              "      <td>-0.680877</td>\n",
              "      <td>0.154349</td>\n",
              "      <td>-1.121819</td>\n",
              "      <td>1.603352</td>\n",
              "      <td>1.263992</td>\n",
              "      <td>0.351786</td>\n",
              "      <td>1.488661</td>\n",
              "      <td>-0.348562</td>\n",
              "      <td>-0.093987</td>\n",
              "      <td>-1.880083</td>\n",
              "      <td>1.376157</td>\n",
              "      <td>-1.224123</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>0.022464</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>0.632781</td>\n",
              "      <td>0.472564</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>-1.642581</td>\n",
              "      <td>0.610917</td>\n",
              "      <td>-1.382270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.291044</td>\n",
              "      <td>-0.487763</td>\n",
              "      <td>0.807325</td>\n",
              "      <td>-0.154365</td>\n",
              "      <td>1.084453</td>\n",
              "      <td>-1.130560</td>\n",
              "      <td>0.768724</td>\n",
              "      <td>0.584225</td>\n",
              "      <td>-0.665105</td>\n",
              "      <td>0.389265</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.777817</td>\n",
              "      <td>0.656775</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-0.379035</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-1.343932</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>2.512803</td>\n",
              "      <td>-0.346548</td>\n",
              "      <td>-0.319705</td>\n",
              "      <td>1.002260</td>\n",
              "      <td>0.854489</td>\n",
              "      <td>-1.798432</td>\n",
              "      <td>-0.556603</td>\n",
              "      <td>1.075183</td>\n",
              "      <td>0.773779</td>\n",
              "      <td>-0.359217</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>-0.002898</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>-1.311252</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-1.068348</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>0.285425</td>\n",
              "      <td>-1.264398</td>\n",
              "      <td>1.643445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.244818</td>\n",
              "      <td>0.033569</td>\n",
              "      <td>-1.012640</td>\n",
              "      <td>0.856109</td>\n",
              "      <td>0.020013</td>\n",
              "      <td>0.597249</td>\n",
              "      <td>-2.020385</td>\n",
              "      <td>-0.019367</td>\n",
              "      <td>-1.293760</td>\n",
              "      <td>0.999962</td>\n",
              "      <td>1.715054</td>\n",
              "      <td>-1.173291</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-0.805943</td>\n",
              "      <td>-1.114439</td>\n",
              "      <td>0.572765</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>-0.584500</td>\n",
              "      <td>0.596595</td>\n",
              "      <td>-1.457389</td>\n",
              "      <td>-0.937806</td>\n",
              "      <td>-0.011453</td>\n",
              "      <td>-0.282369</td>\n",
              "      <td>-0.977962</td>\n",
              "      <td>-2.593742</td>\n",
              "      <td>-0.303552</td>\n",
              "      <td>-0.317593</td>\n",
              "      <td>-0.124444</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>0.689322</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-1.068348</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>0.979742</td>\n",
              "      <td>-0.941488</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-1.382270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.703249</td>\n",
              "      <td>0.815969</td>\n",
              "      <td>-0.065154</td>\n",
              "      <td>-1.043488</td>\n",
              "      <td>0.719195</td>\n",
              "      <td>0.163808</td>\n",
              "      <td>-0.936084</td>\n",
              "      <td>-0.622959</td>\n",
              "      <td>-0.036450</td>\n",
              "      <td>-0.221433</td>\n",
              "      <td>-0.108080</td>\n",
              "      <td>0.127447</td>\n",
              "      <td>1.224096</td>\n",
              "      <td>-1.456260</td>\n",
              "      <td>1.091772</td>\n",
              "      <td>-0.701972</td>\n",
              "      <td>-0.702897</td>\n",
              "      <td>0.096928</td>\n",
              "      <td>1.235331</td>\n",
              "      <td>-0.953495</td>\n",
              "      <td>0.669086</td>\n",
              "      <td>-1.946372</td>\n",
              "      <td>0.474970</td>\n",
              "      <td>-0.636617</td>\n",
              "      <td>1.117569</td>\n",
              "      <td>0.814266</td>\n",
              "      <td>-0.563729</td>\n",
              "      <td>1.243772</td>\n",
              "      <td>-0.726661</td>\n",
              "      <td>1.218327</td>\n",
              "      <td>-0.996955</td>\n",
              "      <td>-0.644394</td>\n",
              "      <td>0.703296</td>\n",
              "      <td>-0.620112</td>\n",
              "      <td>-0.297892</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>-0.228211</td>\n",
              "      <td>0.460699</td>\n",
              "      <td>-0.639293</td>\n",
              "      <td>-0.777127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a98f2d2-b7e9-4297-aff2-79ee651a14d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a98f2d2-b7e9-4297-aff2-79ee651a14d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a98f2d2-b7e9-4297-aff2-79ee651a14d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0  0.315508 -0.185365  0.663485 -1.184789  0.665919  0.945566  0.731168   \n",
              "1 -1.288412  1.780094  0.575879 -2.688532  1.322818 -0.676539  0.372214   \n",
              "2 -0.291044 -0.487763  0.807325 -0.154365  1.084453 -1.130560  0.768724   \n",
              "3  1.244818  0.033569 -1.012640  0.856109  0.020013  0.597249 -2.020385   \n",
              "4  0.703249  0.815969 -0.065154 -1.043488  0.719195  0.163808 -0.936084   \n",
              "\n",
              "       f_07      f_08      f_09      f_10      f_11      f_12      f_13  \\\n",
              "0 -1.226551  1.220861 -0.832130  0.499632 -0.522922 -0.477867  1.145009   \n",
              "1  0.584225 -1.293760  0.389265  0.499632  2.078555  0.089454 -0.155626   \n",
              "2  0.584225 -0.665105  0.389265 -0.108080  0.777817  0.656775 -0.805943   \n",
              "3 -0.019367 -1.293760  0.999962  1.715054 -1.173291  1.224096 -0.805943   \n",
              "4 -0.622959 -0.036450 -0.221433 -0.108080  0.127447  1.224096 -1.456260   \n",
              "\n",
              "       f_14      f_15      f_16      f_17      f_18      f_19      f_20  \\\n",
              "0 -0.379035  1.847503 -0.061861 -1.265929 -0.680877 -1.727077 -0.187118   \n",
              "1 -1.114439 -0.701972  2.502279 -1.265929 -0.680877  0.154349 -1.121819   \n",
              "2 -0.379035 -0.701972 -1.343932  0.096928  2.512803 -0.346548 -0.319705   \n",
              "3 -1.114439  0.572765 -0.702897 -0.584500  0.596595 -1.457389 -0.937806   \n",
              "4  1.091772 -0.701972 -0.702897  0.096928  1.235331 -0.953495  0.669086   \n",
              "\n",
              "       f_21      f_22      f_23      f_24      f_25      f_26      f_28  \\\n",
              "0 -1.076850 -0.571024  1.635696  0.241191 -1.626472 -0.572666  0.086945   \n",
              "1  1.603352  1.263992  0.351786  1.488661 -0.348562 -0.093987 -1.880083   \n",
              "2  1.002260  0.854489 -1.798432 -0.556603  1.075183  0.773779 -0.359217   \n",
              "3 -0.011453 -0.282369 -0.977962 -2.593742 -0.303552 -0.317593 -0.124444   \n",
              "4 -1.946372  0.474970 -0.636617  1.117569  0.814266 -0.563729  1.243772   \n",
              "\n",
              "       f_29      f_30     pos_0     pos_1     pos_2     pos_3     pos_4  \\\n",
              "0  1.376157 -1.224123  1.003054  0.689322  0.703296 -0.620112  0.472564   \n",
              "1  1.376157 -1.224123 -0.996955  0.022464  0.703296  0.632781  0.472564   \n",
              "2 -0.726661 -0.002898 -0.996955 -1.311252  0.703296 -0.620112 -1.068348   \n",
              "3 -0.726661  1.218327 -0.996955  0.689322  0.703296 -0.620112 -1.068348   \n",
              "4 -0.726661  1.218327 -0.996955 -0.644394  0.703296 -0.620112 -0.297892   \n",
              "\n",
              "      pos_5     pos_6     pos_7     pos_8     pos_9  \n",
              "0 -0.937307 -0.228211 -0.240395 -0.639293 -0.777127  \n",
              "1  1.066887 -0.228211 -1.642581  0.610917 -1.382270  \n",
              "2  1.066887 -0.228211  0.285425 -1.264398  1.643445  \n",
              "3  1.066887  0.979742 -0.941488 -0.639293 -1.382270  \n",
              "4  1.066887 -0.228211  0.460699 -0.639293 -0.777127  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya5LTGletO8W",
        "outputId": "bac257ce-8ab2-4c20-f005-1814d47f7baa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([810000, 40]), torch.Size([810000]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Declare the hole dataset\n",
        "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
        "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
        "X_tensor.shape, y_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrEn_2twzUn9",
        "outputId": "bf00a882-a852-4339-eff3-d9de8ed1619c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([688500, 40]),\n",
              " torch.Size([121500, 40]),\n",
              " torch.Size([688500]),\n",
              " torch.Size([121500]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.15, random_state=42, stratify=y_tensor)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvnQvQdXtO58",
        "outputId": "1047abc4-971f-4a7d-df47-fe605caacb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "688500 121500\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CustomDataset(data=X_train, labels=y_train)\n",
        "test_dataset = CustomDataset(data=X_test, labels=y_test)\n",
        "print(len(train_dataset), len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYvfqGhGzJZH",
        "outputId": "a8b62404-c55f-4f67-a4fd-755f484a2f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready for new training process... =) | Running with cuda device\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Net(input_features=X_train.shape[1])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criteria = nn.BCEWithLogitsLoss()\n",
        "resume = 1\n",
        "v3_path = Path('../Output/NetV3.pth')\n",
        "\n",
        "recover = False\n",
        "\n",
        "if v3_path.exists() and recover:\n",
        "    print('Recovering old training process... | Running with {} device'.format(device))\n",
        "    checkpoint = torch.load(v3_path)\n",
        "    resume = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criteria = checkpoint['criteria']\n",
        "else:\n",
        "    print('Ready for new training process... =) | Running with {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nl7DKawzJWa",
        "outputId": "16089ac7-43e2-4d54-9ecf-571fc0ef8abe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/100 | Train loss: 92.009393 | Train accuracy: 0.84544517 | Test loss: 92.622972 | Test accuracy: 0.84427984]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[2/100 | Train loss: 75.252651 | Train accuracy: 0.87585766 | Test loss: 76.534899 | Test accuracy: 0.87415638]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[3/100 | Train loss: 69.002815 | Train accuracy: 0.88682498 | Test loss: 69.864343 | Test accuracy: 0.88487243]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[4/100 | Train loss: 62.288842 | Train accuracy: 0.89854611 | Test loss: 63.40238 | Test accuracy: 0.89647737]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[5/100 | Train loss: 58.209559 | Train accuracy: 0.90586638 | Test loss: 59.324719 | Test accuracy: 0.90409053]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[6/100 | Train loss: 56.701488 | Train accuracy: 0.90842847 | Test loss: 58.233108 | Test accuracy: 0.90571193]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[7/100 | Train loss: 56.149025 | Train accuracy: 0.90984168 | Test loss: 57.803654 | Test accuracy: 0.90681481]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[8/100 | Train loss: 53.482328 | Train accuracy: 0.91375454 | Test loss: 55.048944 | Test accuracy: 0.9111358]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[9/100 | Train loss: 52.913991 | Train accuracy: 0.91474219 | Test loss: 54.71325 | Test accuracy: 0.91172016]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[10/100 | Train loss: 51.507587 | Train accuracy: 0.91714597 | Test loss: 53.549776 | Test accuracy: 0.91413992]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[11/100 | Train loss: 51.346266 | Train accuracy: 0.91756718 | Test loss: 53.23129 | Test accuracy: 0.91488889]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[12/100 | Train loss: 50.891157 | Train accuracy: 0.91851561 | Test loss: 52.952851 | Test accuracy: 0.91476543]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[13/100 | Train loss: 49.897902 | Train accuracy: 0.91937691 | Test loss: 51.843466 | Test accuracy: 0.91683951]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[14/100 | Train loss: 49.260342 | Train accuracy: 0.92177197 | Test loss: 51.732743 | Test accuracy: 0.91832922]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[15/100 | Train loss: 48.106283 | Train accuracy: 0.92318519 | Test loss: 50.577837 | Test accuracy: 0.92019753]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[16/100 | Train loss: 48.648713 | Train accuracy: 0.92405519 | Test loss: 50.856673 | Test accuracy: 0.92076543]\n",
            "Trigger times: 1/5\n",
            "[17/100 | Train loss: 46.781434 | Train accuracy: 0.92586057 | Test loss: 49.753248 | Test accuracy: 0.92155556]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[18/100 | Train loss: 47.632543 | Train accuracy: 0.92425708 | Test loss: 49.980999 | Test accuracy: 0.91982716]\n",
            "Trigger times: 1/5\n",
            "[19/100 | Train loss: 47.13421 | Train accuracy: 0.92506318 | Test loss: 49.618479 | Test accuracy: 0.9207572]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[20/100 | Train loss: 45.949301 | Train accuracy: 0.9270167 | Test loss: 48.611913 | Test accuracy: 0.9228642]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[21/100 | Train loss: 46.804387 | Train accuracy: 0.92506318 | Test loss: 49.649904 | Test accuracy: 0.9215144]\n",
            "Trigger times: 1/5\n",
            "[22/100 | Train loss: 45.633705 | Train accuracy: 0.92756863 | Test loss: 48.482045 | Test accuracy: 0.92302058]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[23/100 | Train loss: 45.360618 | Train accuracy: 0.92809731 | Test loss: 48.453244 | Test accuracy: 0.92293004]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[24/100 | Train loss: 45.518652 | Train accuracy: 0.92730428 | Test loss: 48.233491 | Test accuracy: 0.92397531]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[25/100 | Train loss: 45.532669 | Train accuracy: 0.92813508 | Test loss: 48.434014 | Test accuracy: 0.92378601]\n",
            "Trigger times: 1/5\n",
            "[26/100 | Train loss: 44.604795 | Train accuracy: 0.9294045 | Test loss: 47.71997 | Test accuracy: 0.9246749]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[27/100 | Train loss: 43.776062 | Train accuracy: 0.9308337 | Test loss: 46.988512 | Test accuracy: 0.92604115]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[28/100 | Train loss: 44.508531 | Train accuracy: 0.92911837 | Test loss: 47.672909 | Test accuracy: 0.92451029]\n",
            "Trigger times: 1/5\n",
            "[29/100 | Train loss: 43.483799 | Train accuracy: 0.9314626 | Test loss: 46.799467 | Test accuracy: 0.92661728]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[30/100 | Train loss: 44.148867 | Train accuracy: 0.93010603 | Test loss: 47.205454 | Test accuracy: 0.92540741]\n",
            "Trigger times: 1/5\n",
            "[31/100 | Train loss: 43.446284 | Train accuracy: 0.93092375 | Test loss: 46.946986 | Test accuracy: 0.9257284]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[32/100 | Train loss: 43.79539 | Train accuracy: 0.93076543 | Test loss: 47.306373 | Test accuracy: 0.92573663]\n",
            "Trigger times: 1/5\n",
            "[33/100 | Train loss: 43.645702 | Train accuracy: 0.93061728 | Test loss: 47.147977 | Test accuracy: 0.92534979]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[34/100 | Train loss: 44.132844 | Train accuracy: 0.92996514 | Test loss: 47.319916 | Test accuracy: 0.92464198]\n",
            "Trigger times: 1/5\n",
            "[35/100 | Train loss: 42.940077 | Train accuracy: 0.93166449 | Test loss: 46.299926 | Test accuracy: 0.92658436]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[36/100 | Train loss: 42.745893 | Train accuracy: 0.93210312 | Test loss: 46.467254 | Test accuracy: 0.92701235]\n",
            "Trigger times: 1/5\n",
            "[37/100 | Train loss: 43.436498 | Train accuracy: 0.9313406 | Test loss: 46.637064 | Test accuracy: 0.9262963]\n",
            "Trigger times: 2/5\n",
            "[38/100 | Train loss: 43.148135 | Train accuracy: 0.93141757 | Test loss: 46.544768 | Test accuracy: 0.92658436]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[39/100 | Train loss: 43.19133 | Train accuracy: 0.93203776 | Test loss: 47.135461 | Test accuracy: 0.92604938]\n",
            "Trigger times: 1/5\n",
            "[40/100 | Train loss: 42.619446 | Train accuracy: 0.9325374 | Test loss: 46.050015 | Test accuracy: 0.92777778]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[41/100 | Train loss: 42.539694 | Train accuracy: 0.93258678 | Test loss: 46.586668 | Test accuracy: 0.9264856]\n",
            "Trigger times: 1/5\n",
            "[42/100 | Train loss: 42.634324 | Train accuracy: 0.9320915 | Test loss: 46.940693 | Test accuracy: 0.92578601]\n",
            "Trigger times: 2/5\n",
            "[43/100 | Train loss: 42.571126 | Train accuracy: 0.93214379 | Test loss: 46.287995 | Test accuracy: 0.92659259]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[44/100 | Train loss: 41.96444 | Train accuracy: 0.93314887 | Test loss: 46.266001 | Test accuracy: 0.92653498]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[45/100 | Train loss: 41.962256 | Train accuracy: 0.93348293 | Test loss: 46.126191 | Test accuracy: 0.92706173]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[46/100 | Train loss: 41.773465 | Train accuracy: 0.93315904 | Test loss: 46.126151 | Test accuracy: 0.92702058]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[47/100 | Train loss: 41.582097 | Train accuracy: 0.9343907 | Test loss: 45.724927 | Test accuracy: 0.92882305]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[48/100 | Train loss: 41.619863 | Train accuracy: 0.93419463 | Test loss: 45.647579 | Test accuracy: 0.92801646]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[49/100 | Train loss: 43.103682 | Train accuracy: 0.93170225 | Test loss: 47.056622 | Test accuracy: 0.92558025]\n",
            "Trigger times: 1/5\n",
            "[50/100 | Train loss: 42.647642 | Train accuracy: 0.93172113 | Test loss: 47.159414 | Test accuracy: 0.92495473]\n",
            "Trigger times: 2/5\n",
            "[51/100 | Train loss: 42.531744 | Train accuracy: 0.93186492 | Test loss: 46.949409 | Test accuracy: 0.9249465]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[52/100 | Train loss: 40.98862 | Train accuracy: 0.93499492 | Test loss: 45.423651 | Test accuracy: 0.92901235]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[53/100 | Train loss: 40.528829 | Train accuracy: 0.93594336 | Test loss: 45.002705 | Test accuracy: 0.92979424]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[54/100 | Train loss: 41.439841 | Train accuracy: 0.93388381 | Test loss: 45.895511 | Test accuracy: 0.9272428]\n",
            "Trigger times: 1/5\n",
            "[55/100 | Train loss: 41.46094 | Train accuracy: 0.93494989 | Test loss: 45.400577 | Test accuracy: 0.92828807]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[56/100 | Train loss: 42.740335 | Train accuracy: 0.93147858 | Test loss: 47.229852 | Test accuracy: 0.9248642]\n",
            "Trigger times: 1/5\n",
            "[57/100 | Train loss: 41.621682 | Train accuracy: 0.93326797 | Test loss: 46.063896 | Test accuracy: 0.92716049]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[58/100 | Train loss: 41.938198 | Train accuracy: 0.93323312 | Test loss: 46.40159 | Test accuracy: 0.92709465]\n",
            "Trigger times: 1/5\n",
            "[59/100 | Train loss: 41.19146 | Train accuracy: 0.93487291 | Test loss: 45.74455 | Test accuracy: 0.92790947]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[60/100 | Train loss: 40.793613 | Train accuracy: 0.93498911 | Test loss: 45.32579 | Test accuracy: 0.92812346]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[61/100 | Train loss: 41.926368 | Train accuracy: 0.93331881 | Test loss: 46.678873 | Test accuracy: 0.92649383]\n",
            "Trigger times: 1/5\n",
            "[62/100 | Train loss: 41.717454 | Train accuracy: 0.93384023 | Test loss: 46.042599 | Test accuracy: 0.92715226]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[63/100 | Train loss: 41.279215 | Train accuracy: 0.93464924 | Test loss: 46.352855 | Test accuracy: 0.92706996]\n",
            "Trigger times: 1/5\n",
            "[64/100 | Train loss: 41.435978 | Train accuracy: 0.93433115 | Test loss: 45.578888 | Test accuracy: 0.92851852]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[65/100 | Train loss: 40.676599 | Train accuracy: 0.93514742 | Test loss: 45.37063 | Test accuracy: 0.92802469]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[66/100 | Train loss: 40.715291 | Train accuracy: 0.93493101 | Test loss: 45.311669 | Test accuracy: 0.92786008]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[67/100 | Train loss: 40.494365 | Train accuracy: 0.93586202 | Test loss: 45.856509 | Test accuracy: 0.92821399]\n",
            "Trigger times: 1/5\n",
            "[68/100 | Train loss: 40.144747 | Train accuracy: 0.9365955 | Test loss: 45.596569 | Test accuracy: 0.92892181]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[69/100 | Train loss: 40.890286 | Train accuracy: 0.93498911 | Test loss: 45.840065 | Test accuracy: 0.9268642]\n",
            "Trigger times: 1/5\n",
            "[70/100 | Train loss: 40.533913 | Train accuracy: 0.93558606 | Test loss: 45.55449 | Test accuracy: 0.92870782]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[71/100 | Train loss: 40.732054 | Train accuracy: 0.93534495 | Test loss: 45.185654 | Test accuracy: 0.92859259]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[72/100 | Train loss: 40.339451 | Train accuracy: 0.93587509 | Test loss: 45.795078 | Test accuracy: 0.9286749]\n",
            "Trigger times: 1/5\n",
            "[73/100 | Train loss: 40.124425 | Train accuracy: 0.9363297 | Test loss: 45.80922 | Test accuracy: 0.92907819]\n",
            "Trigger times: 2/5\n",
            "[74/100 | Train loss: 40.332545 | Train accuracy: 0.93604503 | Test loss: 45.769174 | Test accuracy: 0.92795885]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[75/100 | Train loss: 40.807392 | Train accuracy: 0.93533769 | Test loss: 45.776145 | Test accuracy: 0.92729218]\n",
            "Trigger times: 1/5\n",
            "[76/100 | Train loss: 40.08383 | Train accuracy: 0.93623675 | Test loss: 45.231459 | Test accuracy: 0.92888889]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[77/100 | Train loss: 40.315864 | Train accuracy: 0.93604067 | Test loss: 46.037925 | Test accuracy: 0.9278107]\n",
            "Trigger times: 1/5\n",
            "[78/100 | Train loss: 39.821995 | Train accuracy: 0.93682789 | Test loss: 45.107965 | Test accuracy: 0.92857613]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[79/100 | Train loss: 39.834499 | Train accuracy: 0.93623239 | Test loss: 45.654698 | Test accuracy: 0.92824691]\n",
            "Trigger times: 1/5\n",
            "[80/100 | Train loss: 39.99899 | Train accuracy: 0.93635875 | Test loss: 45.688988 | Test accuracy: 0.92792593]\n",
            "Trigger times: 2/5\n",
            "[81/100 | Train loss: 39.239897 | Train accuracy: 0.93775309 | Test loss: 44.778808 | Test accuracy: 0.92967901]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[82/100 | Train loss: 40.246278 | Train accuracy: 0.93612781 | Test loss: 45.847391 | Test accuracy: 0.92833745]\n",
            "Trigger times: 1/5\n",
            "[83/100 | Train loss: 41.494183 | Train accuracy: 0.93473057 | Test loss: 46.56504 | Test accuracy: 0.92632922]\n",
            "Trigger times: 2/5\n",
            "[84/100 | Train loss: 40.631328 | Train accuracy: 0.93606391 | Test loss: 45.81797 | Test accuracy: 0.92742387]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[85/100 | Train loss: 40.041899 | Train accuracy: 0.93649673 | Test loss: 45.770751 | Test accuracy: 0.92832099]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[86/100 | Train loss: 39.514654 | Train accuracy: 0.93699201 | Test loss: 45.639837 | Test accuracy: 0.9282716]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[87/100 | Train loss: 40.450799 | Train accuracy: 0.9351968 | Test loss: 46.348744 | Test accuracy: 0.92671605]\n",
            "Trigger times: 1/5\n",
            "[88/100 | Train loss: 39.829674 | Train accuracy: 0.93640232 | Test loss: 45.425157 | Test accuracy: 0.92797531]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[89/100 | Train loss: 39.55287 | Train accuracy: 0.93727669 | Test loss: 45.710236 | Test accuracy: 0.92784362]\n",
            "Trigger times: 1/5\n",
            "[90/100 | Train loss: 39.155801 | Train accuracy: 0.93765577 | Test loss: 44.917091 | Test accuracy: 0.92997531]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[91/100 | Train loss: 39.260317 | Train accuracy: 0.93761075 | Test loss: 45.263908 | Test accuracy: 0.92914403]\n",
            "Trigger times: 1/5\n",
            "[92/100 | Train loss: 39.685454 | Train accuracy: 0.93625272 | Test loss: 45.771366 | Test accuracy: 0.92707819]\n",
            "Trigger times: 2/5\n",
            "[93/100 | Train loss: 40.259458 | Train accuracy: 0.93532462 | Test loss: 46.010756 | Test accuracy: 0.92748971]\n",
            "Trigger times: 3/5\n",
            "[94/100 | Train loss: 39.693537 | Train accuracy: 0.93690777 | Test loss: 45.61396 | Test accuracy: 0.92874897]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[95/100 | Train loss: 39.223641 | Train accuracy: 0.93713435 | Test loss: 45.330337 | Test accuracy: 0.92803292]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[96/100 | Train loss: 39.513554 | Train accuracy: 0.93720697 | Test loss: 45.957679 | Test accuracy: 0.92795885]\n",
            "Trigger times: 1/5\n",
            "[97/100 | Train loss: 39.240446 | Train accuracy: 0.93810167 | Test loss: 45.43778 | Test accuracy: 0.92868313]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[98/100 | Train loss: 40.030208 | Train accuracy: 0.93584604 | Test loss: 45.719214 | Test accuracy: 0.92708642]\n",
            "Trigger times: 1/5\n",
            "[99/100 | Train loss: 40.033455 | Train accuracy: 0.93657952 | Test loss: 46.780259 | Test accuracy: 0.92814815]\n",
            "Trigger times: 2/5\n",
            "[100/100 | Train loss: 40.274587 | Train accuracy: 0.93606391 | Test loss: 46.325833 | Test accuracy: 0.92688066]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n"
          ]
        }
      ],
      "source": [
        "wanna_train = False\n",
        "if wanna_train:\n",
        "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v3_path)\n",
        "else:\n",
        "    print('Not training this model anymore...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L_b1yykzJUC"
      },
      "source": [
        "## Making the Neural Network architecture bigger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-rivQMdzJR3"
      },
      "outputs": [],
      "source": [
        "class Net2(nn.Module):\n",
        "\n",
        "    def __init__(self, input_features=30):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=input_features, out_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=128, out_features=256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=256, out_features=256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=256, out_features=256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=256, out_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=128, out_features=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.main(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp-Yum2FCE1Y"
      },
      "outputs": [],
      "source": [
        "# Counts the number of times a letter appear in the code\n",
        "def add_letters_count(data):\n",
        "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    for char in letters:\n",
        "        data[char] = data['f_27'].str.count(char)\n",
        "    return data\n",
        "\n",
        "# For each of the 10 position of the letter code I assign which code letter was assigned to it\n",
        "def add_letter_position(data):\n",
        "    for i in range(10):\n",
        "        data['pos_' + str(i)] = (data['f_27'].str[i]).apply(lambda x: ord(x)) - 65\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U10hq0KzJPX"
      },
      "outputs": [],
      "source": [
        "letters_to_drop = ['U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "df['f_27'] = df['f_27'].str.upper()\n",
        "\n",
        "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
        "\n",
        "X = add_letters_count(X)\n",
        "X = add_letter_position(X)\n",
        "\n",
        "X = X.drop(['length', 'f_27'] + letters_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "X_train, X_test = torch.from_numpy(X_train.to_numpy()), torch.from_numpy(X_test.to_numpy())\n",
        "y_train, y_test = torch.from_numpy(y_train.to_numpy()), torch.from_numpy(y_test.to_numpy())\n",
        "\n",
        "X_train, X_test = X_train.type(torch.float), X_test.type(torch.float)\n",
        "y_train, y_test = y_train.type(torch.float), y_test.type(torch.float)\n",
        "\n",
        "train_dataset = CustomDataset(data=X_train, labels=y_train)\n",
        "test_dataset = CustomDataset(data=X_test, labels=y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7L4YHm4zJM3",
        "outputId": "dfdd93ff-0e83-42ed-c842-aa6e6b94a879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready for new training process... =) | Running with cuda device\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Net2(input_features=X_train.shape[1])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criteria = nn.BCEWithLogitsLoss()\n",
        "resume = 1\n",
        "v4_path = Path('../Output/NetV4.pth')\n",
        "\n",
        "recover = False\n",
        "\n",
        "if v4_path.exists() and recover:\n",
        "    print('Recovering old training process... | Running with {} device'.format(device))\n",
        "    checkpoint = torch.load(v4_path)\n",
        "    resume = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criteria = checkpoint['criteria']\n",
        "else:\n",
        "    print('Ready for new training process... =) | Running with {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRerrkwgC3wC",
        "outputId": "1cf04b8f-3ab7-4184-ea95-db3fa051dd8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/100 | Train loss: 83.314789 | Train accuracy: 0.85891907 | Test loss: 85.215976 | Test accuracy: 0.85476543]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[2/100 | Train loss: 58.620393 | Train accuracy: 0.90247462 | Test loss: 60.777092 | Test accuracy: 0.89754321]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[3/100 | Train loss: 41.070859 | Train accuracy: 0.93459259 | Test loss: 44.557603 | Test accuracy: 0.92879012]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[4/100 | Train loss: 34.442221 | Train accuracy: 0.94661866 | Test loss: 37.276193 | Test accuracy: 0.9418642]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[5/100 | Train loss: 29.8427 | Train accuracy: 0.95437037 | Test loss: 33.445415 | Test accuracy: 0.94839506]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[6/100 | Train loss: 30.214552 | Train accuracy: 0.95351578 | Test loss: 34.530396 | Test accuracy: 0.94618519]\n",
            "Trigger times: 1/5\n",
            "[7/100 | Train loss: 28.456232 | Train accuracy: 0.95597942 | Test loss: 33.924868 | Test accuracy: 0.94840741]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[8/100 | Train loss: 26.422958 | Train accuracy: 0.95984088 | Test loss: 32.049447 | Test accuracy: 0.95125926]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[9/100 | Train loss: 25.760407 | Train accuracy: 0.96041975 | Test loss: 31.308514 | Test accuracy: 0.9518642]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[10/100 | Train loss: 26.801086 | Train accuracy: 0.95877366 | Test loss: 32.450512 | Test accuracy: 0.94938272]\n",
            "Trigger times: 1/5\n",
            "[11/100 | Train loss: 24.318713 | Train accuracy: 0.96236351 | Test loss: 31.002656 | Test accuracy: 0.95159259]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[12/100 | Train loss: 22.669924 | Train accuracy: 0.96519342 | Test loss: 29.591875 | Test accuracy: 0.95504938]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[13/100 | Train loss: 22.831558 | Train accuracy: 0.9651166 | Test loss: 31.02479 | Test accuracy: 0.95401235]\n",
            "Trigger times: 1/5\n",
            "[14/100 | Train loss: 23.875966 | Train accuracy: 0.9641262 | Test loss: 32.020828 | Test accuracy: 0.95154321]\n",
            "Trigger times: 2/5\n",
            "[15/100 | Train loss: 22.769411 | Train accuracy: 0.96539643 | Test loss: 30.857672 | Test accuracy: 0.95344444]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[16/100 | Train loss: 22.145105 | Train accuracy: 0.96658162 | Test loss: 30.413029 | Test accuracy: 0.95358025]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[17/100 | Train loss: 21.933278 | Train accuracy: 0.96722222 | Test loss: 31.552778 | Test accuracy: 0.95333333]\n",
            "Trigger times: 1/5\n",
            "[18/100 | Train loss: 20.55681 | Train accuracy: 0.96883402 | Test loss: 30.282405 | Test accuracy: 0.95485185]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[19/100 | Train loss: 20.09463 | Train accuracy: 0.9696845 | Test loss: 31.058672 | Test accuracy: 0.95469136]\n",
            "Trigger times: 1/5\n",
            "[20/100 | Train loss: 19.423456 | Train accuracy: 0.97013717 | Test loss: 30.913549 | Test accuracy: 0.95450617]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[21/100 | Train loss: 19.830809 | Train accuracy: 0.97001235 | Test loss: 30.46701 | Test accuracy: 0.95420988]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[22/100 | Train loss: 19.013559 | Train accuracy: 0.97161728 | Test loss: 29.812971 | Test accuracy: 0.95583951]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[23/100 | Train loss: 19.469016 | Train accuracy: 0.97012894 | Test loss: 31.122293 | Test accuracy: 0.95402469]\n",
            "Trigger times: 1/5\n",
            "[24/100 | Train loss: 19.411756 | Train accuracy: 0.97060357 | Test loss: 31.453179 | Test accuracy: 0.95391358]\n",
            "Trigger times: 2/5\n",
            "[25/100 | Train loss: 19.131834 | Train accuracy: 0.97083676 | Test loss: 31.710691 | Test accuracy: 0.95217284]\n",
            "Trigger times: 3/5\n",
            "[26/100 | Train loss: 17.949609 | Train accuracy: 0.97296433 | Test loss: 31.683703 | Test accuracy: 0.9551358]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[27/100 | Train loss: 17.796301 | Train accuracy: 0.97288066 | Test loss: 31.874368 | Test accuracy: 0.95449383]\n",
            "Trigger times: 1/5\n",
            "[28/100 | Train loss: 17.727737 | Train accuracy: 0.97294376 | Test loss: 31.636783 | Test accuracy: 0.95375309]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[29/100 | Train loss: 16.568129 | Train accuracy: 0.97521811 | Test loss: 31.170011 | Test accuracy: 0.95571605]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[30/100 | Train loss: 17.18603 | Train accuracy: 0.9733594 | Test loss: 32.456835 | Test accuracy: 0.95341975]\n",
            "Trigger times: 1/5\n",
            "[31/100 | Train loss: 17.5333 | Train accuracy: 0.97311385 | Test loss: 33.012654 | Test accuracy: 0.95292593]\n",
            "Trigger times: 2/5\n",
            "[32/100 | Train loss: 16.776339 | Train accuracy: 0.97464883 | Test loss: 32.031837 | Test accuracy: 0.95464198]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[33/100 | Train loss: 16.362839 | Train accuracy: 0.97579012 | Test loss: 31.944838 | Test accuracy: 0.95440741]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[34/100 | Train loss: 16.294447 | Train accuracy: 0.9755775 | Test loss: 33.412146 | Test accuracy: 0.95338272]\n",
            "Trigger times: 1/5\n",
            "[35/100 | Train loss: 16.225987 | Train accuracy: 0.97582579 | Test loss: 31.872906 | Test accuracy: 0.9537037]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[36/100 | Train loss: 16.075191 | Train accuracy: 0.97572154 | Test loss: 34.093041 | Test accuracy: 0.95385185]\n",
            "Trigger times: 1/5\n",
            "[37/100 | Train loss: 15.441421 | Train accuracy: 0.9769177 | Test loss: 33.132023 | Test accuracy: 0.95365432]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[38/100 | Train loss: 15.558276 | Train accuracy: 0.97662003 | Test loss: 33.409076 | Test accuracy: 0.95233333]\n",
            "Trigger times: 1/5\n",
            "[39/100 | Train loss: 14.88546 | Train accuracy: 0.97728258 | Test loss: 33.632118 | Test accuracy: 0.95308642]\n",
            "Trigger times: 2/5\n",
            "[40/100 | Train loss: 15.191579 | Train accuracy: 0.97776955 | Test loss: 33.146007 | Test accuracy: 0.95308642]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[41/100 | Train loss: 14.59328 | Train accuracy: 0.97819204 | Test loss: 32.881363 | Test accuracy: 0.95334568]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[42/100 | Train loss: 14.657941 | Train accuracy: 0.9780535 | Test loss: 34.103329 | Test accuracy: 0.95280247]\n",
            "Trigger times: 1/5\n",
            "[43/100 | Train loss: 14.653147 | Train accuracy: 0.97768587 | Test loss: 35.589168 | Test accuracy: 0.95277778]\n",
            "Trigger times: 2/5\n",
            "[44/100 | Train loss: 14.39359 | Train accuracy: 0.97817833 | Test loss: 34.751155 | Test accuracy: 0.95261728]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[45/100 | Train loss: 14.650437 | Train accuracy: 0.97820439 | Test loss: 34.149789 | Test accuracy: 0.9531358]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[46/100 | Train loss: 14.720891 | Train accuracy: 0.9780439 | Test loss: 35.957396 | Test accuracy: 0.95185185]\n",
            "Trigger times: 1/5\n",
            "[47/100 | Train loss: 14.277982 | Train accuracy: 0.97858711 | Test loss: 36.16735 | Test accuracy: 0.9528642]\n",
            "Trigger times: 2/5\n",
            "[48/100 | Train loss: 13.912227 | Train accuracy: 0.97871331 | Test loss: 34.942755 | Test accuracy: 0.95295062]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[49/100 | Train loss: 13.60182 | Train accuracy: 0.97975446 | Test loss: 34.80321 | Test accuracy: 0.95335802]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[50/100 | Train loss: 13.857484 | Train accuracy: 0.9793251 | Test loss: 35.265972 | Test accuracy: 0.95222222]\n",
            "Trigger times: 1/5\n",
            "[51/100 | Train loss: 14.056432 | Train accuracy: 0.97881893 | Test loss: 35.736634 | Test accuracy: 0.95209877]\n",
            "Trigger times: 2/5\n",
            "[52/100 | Train loss: 13.010816 | Train accuracy: 0.98070233 | Test loss: 36.031622 | Test accuracy: 0.95250617]\n",
            "Trigger times: 3/5\n",
            "[53/100 | Train loss: 13.78192 | Train accuracy: 0.97965021 | Test loss: 35.169479 | Test accuracy: 0.95198765]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[54/100 | Train loss: 13.480004 | Train accuracy: 0.97970782 | Test loss: 37.833542 | Test accuracy: 0.95169136]\n",
            "Trigger times: 1/5\n",
            "[55/100 | Train loss: 12.805433 | Train accuracy: 0.98090398 | Test loss: 36.063749 | Test accuracy: 0.95293827]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[56/100 | Train loss: 12.450528 | Train accuracy: 0.98172565 | Test loss: 35.862887 | Test accuracy: 0.95323457]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[57/100 | Train loss: 12.450716 | Train accuracy: 0.98134019 | Test loss: 36.023469 | Test accuracy: 0.95196296]\n",
            "Trigger times: 1/5\n"
          ]
        }
      ],
      "source": [
        "wanna_train = False\n",
        "if wanna_train:\n",
        "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v4_path)\n",
        "else:\n",
        "    print('Not training this model anymore...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVgHfNY8AT2M"
      },
      "source": [
        "## Neural network with Drop out rectification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhArfjXKzJHY"
      },
      "outputs": [],
      "source": [
        "class Net3(nn.Module):\n",
        "\n",
        "    def __init__(self, input_features=30):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features=input_features, out_features=1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=1024, out_features=512),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(in_features=512, out_features=512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=512, out_features=256),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(in_features=256, out_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(in_features=128, out_features=64),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(in_features=64, out_features=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.main(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts the number of times a letter appear in the code\n",
        "def add_letters_count(data):\n",
        "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    for char in letters:\n",
        "        data[char] = data['f_27'].str.count(char)\n",
        "    return data\n",
        "\n",
        "# For each of the 10 position of the letter code I assign which code letter was assigned to it\n",
        "def add_letter_position(data):\n",
        "    for i in range(10):\n",
        "        data['pos_' + str(i)] = (data['f_27'].str[i]).apply(lambda x: ord(x)) - 65\n",
        "    return data"
      ],
      "metadata": {
        "id": "Z0mBaG2xjgiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diG1n0eSzJAm"
      },
      "outputs": [],
      "source": [
        "letters_to_drop = ['U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "df['f_27'] = df['f_27'].str.upper()\n",
        "\n",
        "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
        "\n",
        "X = add_letters_count(X)\n",
        "X = add_letter_position(X)\n",
        "\n",
        "X = X.drop(['length', 'f_27'] + letters_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
        "\n",
        "X_train, X_test = torch.from_numpy(X_train.to_numpy()), torch.from_numpy(X_test.to_numpy())\n",
        "y_train, y_test = torch.from_numpy(y_train.to_numpy()), torch.from_numpy(y_test.to_numpy())\n",
        "\n",
        "X_train, X_test = X_train.type(torch.float), X_test.type(torch.float)\n",
        "y_train, y_test = y_train.type(torch.float), y_test.type(torch.float)\n",
        "\n",
        "train_dataset = CustomDataset(data=X_train, labels=y_train)\n",
        "test_dataset = CustomDataset(data=X_test, labels=y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjgDI1srzIoD",
        "outputId": "cc5d0086-9326-446d-df4d-ebab787732db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready for new training process... =) | Running with cuda device\n"
          ]
        }
      ],
      "source": [
        "batch_size = 256 \n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Net3(input_features=X_train.shape[1])\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "epochs = 120\n",
        "learning_rate, l2_factor = 0.001, 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_factor)\n",
        "criteria = nn.BCEWithLogitsLoss()\n",
        "resume = 1\n",
        "v5_path = Path('../Output/NetV5.pth')\n",
        "\n",
        "recover = False\n",
        "\n",
        "if v5_path.exists() and recover:\n",
        "    print('Recovering old training process... | Running with {} device'.format(device))\n",
        "    checkpoint = torch.load(v5_path)\n",
        "    resume = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    criteria = checkpoint['criteria']\n",
        "else:\n",
        "    print('Ready for new training process... =) | Running with {} device'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrnijA4wtO3l",
        "outputId": "d40190d5-caa0-4fee-db05-4bfbdf25bd4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/120 | Train loss: 75.086867 | Train accuracy: 0.87244993 | Test loss: 76.651278 | Test accuracy: 0.86939506]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[2/120 | Train loss: 60.159328 | Train accuracy: 0.89995199 | Test loss: 61.539475 | Test accuracy: 0.8962716]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[3/120 | Train loss: 44.881606 | Train accuracy: 0.92938683 | Test loss: 46.041412 | Test accuracy: 0.92758025]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[4/120 | Train loss: 40.902025 | Train accuracy: 0.93658848 | Test loss: 42.020412 | Test accuracy: 0.93459259]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[5/120 | Train loss: 35.732119 | Train accuracy: 0.94591632 | Test loss: 36.474231 | Test accuracy: 0.94483951]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[6/120 | Train loss: 35.029692 | Train accuracy: 0.9469177 | Test loss: 36.309234 | Test accuracy: 0.94520988]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[7/120 | Train loss: 35.480283 | Train accuracy: 0.94617421 | Test loss: 36.481334 | Test accuracy: 0.94455556]\n",
            "Trigger times: 1/5\n",
            "[8/120 | Train loss: 33.46295 | Train accuracy: 0.94816461 | Test loss: 34.9763 | Test accuracy: 0.94508642]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[9/120 | Train loss: 31.809721 | Train accuracy: 0.95136488 | Test loss: 33.053327 | Test accuracy: 0.94933333]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[10/120 | Train loss: 31.370339 | Train accuracy: 0.950893 | Test loss: 32.974491 | Test accuracy: 0.94818519]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[11/120 | Train loss: 30.019252 | Train accuracy: 0.95424005 | Test loss: 31.802129 | Test accuracy: 0.95053086]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[12/120 | Train loss: 29.674646 | Train accuracy: 0.95451852 | Test loss: 31.337636 | Test accuracy: 0.95224691]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[13/120 | Train loss: 30.615425 | Train accuracy: 0.95340604 | Test loss: 32.387668 | Test accuracy: 0.95082716]\n",
            "Trigger times: 1/5\n",
            "[14/120 | Train loss: 28.81135 | Train accuracy: 0.95660768 | Test loss: 30.28382 | Test accuracy: 0.95391358]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[15/120 | Train loss: 31.307287 | Train accuracy: 0.95336351 | Test loss: 32.737499 | Test accuracy: 0.95103704]\n",
            "Trigger times: 1/5\n",
            "[16/120 | Train loss: 28.483407 | Train accuracy: 0.95721125 | Test loss: 30.138837 | Test accuracy: 0.95366667]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[17/120 | Train loss: 28.679787 | Train accuracy: 0.9565144 | Test loss: 30.481618 | Test accuracy: 0.95346914]\n",
            "Trigger times: 1/5\n",
            "[18/120 | Train loss: 29.125441 | Train accuracy: 0.95553086 | Test loss: 30.695077 | Test accuracy: 0.95228395]\n",
            "Trigger times: 2/5\n",
            "[19/120 | Train loss: 33.04817 | Train accuracy: 0.94917558 | Test loss: 34.560282 | Test accuracy: 0.94676543]\n",
            "Trigger times: 3/5\n",
            "[20/120 | Train loss: 27.726701 | Train accuracy: 0.95712071 | Test loss: 29.07994 | Test accuracy: 0.95495062]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[21/120 | Train loss: 27.949812 | Train accuracy: 0.9565048 | Test loss: 29.651444 | Test accuracy: 0.95333333]\n",
            "Trigger times: 1/5\n",
            "[22/120 | Train loss: 28.841283 | Train accuracy: 0.95633608 | Test loss: 30.30281 | Test accuracy: 0.95377778]\n",
            "Trigger times: 2/5\n",
            "[23/120 | Train loss: 26.907617 | Train accuracy: 0.95871605 | Test loss: 28.664665 | Test accuracy: 0.95564198]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[24/120 | Train loss: 27.929478 | Train accuracy: 0.9572428 | Test loss: 29.825436 | Test accuracy: 0.95381481]\n",
            "Trigger times: 1/5\n",
            "[25/120 | Train loss: 30.494991 | Train accuracy: 0.95420439 | Test loss: 32.249742 | Test accuracy: 0.95085185]\n",
            "Trigger times: 2/5\n",
            "[26/120 | Train loss: 26.832338 | Train accuracy: 0.9591454 | Test loss: 28.406044 | Test accuracy: 0.95597531]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[27/120 | Train loss: 30.418754 | Train accuracy: 0.95247874 | Test loss: 32.341715 | Test accuracy: 0.94853086]\n",
            "Trigger times: 1/5\n",
            "[28/120 | Train loss: 28.064502 | Train accuracy: 0.95659396 | Test loss: 30.11315 | Test accuracy: 0.95319753]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[29/120 | Train loss: 27.61307 | Train accuracy: 0.95761043 | Test loss: 29.234801 | Test accuracy: 0.95503704]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[30/120 | Train loss: 27.001512 | Train accuracy: 0.95921262 | Test loss: 28.82273 | Test accuracy: 0.95609877]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[31/120 | Train loss: 27.160239 | Train accuracy: 0.95828532 | Test loss: 28.732997 | Test accuracy: 0.95562963]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[32/120 | Train loss: 26.907945 | Train accuracy: 0.95835117 | Test loss: 28.852271 | Test accuracy: 0.95490123]\n",
            "Trigger times: 1/5\n",
            "[33/120 | Train loss: 28.296902 | Train accuracy: 0.9558546 | Test loss: 30.172659 | Test accuracy: 0.95234568]\n",
            "Trigger times: 2/5\n",
            "[34/120 | Train loss: 25.872272 | Train accuracy: 0.96086145 | Test loss: 27.556035 | Test accuracy: 0.95722222]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[35/120 | Train loss: 25.82451 | Train accuracy: 0.96021536 | Test loss: 27.580471 | Test accuracy: 0.95687654]\n",
            "Trigger times: 1/5\n",
            "[36/120 | Train loss: 26.415357 | Train accuracy: 0.95969822 | Test loss: 28.045858 | Test accuracy: 0.9568642]\n",
            "Trigger times: 2/5\n",
            "[37/120 | Train loss: 26.510284 | Train accuracy: 0.95983951 | Test loss: 28.340665 | Test accuracy: 0.95719753]\n",
            "Trigger times: 3/5\n",
            "[38/120 | Train loss: 26.518792 | Train accuracy: 0.95947737 | Test loss: 28.04713 | Test accuracy: 0.95671605]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[39/120 | Train loss: 26.035859 | Train accuracy: 0.96009877 | Test loss: 27.728258 | Test accuracy: 0.95680247]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[40/120 | Train loss: 26.347295 | Train accuracy: 0.9594321 | Test loss: 28.001844 | Test accuracy: 0.95664198]\n",
            "Trigger times: 1/5\n",
            "[41/120 | Train loss: 26.025143 | Train accuracy: 0.96040329 | Test loss: 27.991656 | Test accuracy: 0.95714815]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[42/120 | Train loss: 26.679147 | Train accuracy: 0.95910562 | Test loss: 28.441127 | Test accuracy: 0.95538272]\n",
            "Trigger times: 1/5\n",
            "[43/120 | Train loss: 26.593748 | Train accuracy: 0.95931276 | Test loss: 28.656232 | Test accuracy: 0.95574074]\n",
            "Trigger times: 2/5\n",
            "[44/120 | Train loss: 26.719286 | Train accuracy: 0.95841975 | Test loss: 28.765708 | Test accuracy: 0.95476543]\n",
            "Trigger times: 3/5\n",
            "[45/120 | Train loss: 25.2243 | Train accuracy: 0.96172016 | Test loss: 26.970867 | Test accuracy: 0.95790123]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[46/120 | Train loss: 25.270111 | Train accuracy: 0.96142798 | Test loss: 26.941677 | Test accuracy: 0.95841975]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[47/120 | Train loss: 26.374463 | Train accuracy: 0.96006036 | Test loss: 28.232449 | Test accuracy: 0.95681481]\n",
            "Trigger times: 1/5\n",
            "[48/120 | Train loss: 24.570338 | Train accuracy: 0.96241427 | Test loss: 26.517144 | Test accuracy: 0.95883951]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[49/120 | Train loss: 28.778636 | Train accuracy: 0.9551797 | Test loss: 30.636514 | Test accuracy: 0.95180247]\n",
            "Trigger times: 1/5\n",
            "[50/120 | Train loss: 25.36644 | Train accuracy: 0.96176818 | Test loss: 27.265728 | Test accuracy: 0.95823457]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[51/120 | Train loss: 24.714925 | Train accuracy: 0.96251852 | Test loss: 26.693062 | Test accuracy: 0.95877778]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[52/120 | Train loss: 25.036565 | Train accuracy: 0.96228395 | Test loss: 26.492279 | Test accuracy: 0.95981481]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[53/120 | Train loss: 26.038235 | Train accuracy: 0.96055418 | Test loss: 27.854437 | Test accuracy: 0.95648148]\n",
            "Trigger times: 1/5\n",
            "[54/120 | Train loss: 28.186315 | Train accuracy: 0.95672977 | Test loss: 30.441355 | Test accuracy: 0.9528642]\n",
            "Trigger times: 2/5\n",
            "[55/120 | Train loss: 27.495027 | Train accuracy: 0.95785871 | Test loss: 29.320451 | Test accuracy: 0.95449383]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[56/120 | Train loss: 29.007193 | Train accuracy: 0.95571056 | Test loss: 30.87478 | Test accuracy: 0.95292593]\n",
            "Trigger times: 1/5\n",
            "[57/120 | Train loss: 25.824841 | Train accuracy: 0.96039781 | Test loss: 27.627834 | Test accuracy: 0.95748148]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[58/120 | Train loss: 25.669464 | Train accuracy: 0.96060357 | Test loss: 26.891489 | Test accuracy: 0.95816049]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[59/120 | Train loss: 25.794179 | Train accuracy: 0.9602716 | Test loss: 27.630029 | Test accuracy: 0.95677778]\n",
            "Trigger times: 1/5\n",
            "[60/120 | Train loss: 26.498318 | Train accuracy: 0.9594513 | Test loss: 28.161358 | Test accuracy: 0.9571358]\n",
            "Trigger times: 2/5\n",
            "[61/120 | Train loss: 27.318598 | Train accuracy: 0.95602881 | Test loss: 29.27784 | Test accuracy: 0.95230864]\n",
            "Trigger times: 3/5\n",
            "[62/120 | Train loss: 25.0256 | Train accuracy: 0.96207407 | Test loss: 27.21845 | Test accuracy: 0.95781481]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[63/120 | Train loss: 24.758713 | Train accuracy: 0.96327572 | Test loss: 26.286219 | Test accuracy: 0.96014815]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[64/120 | Train loss: 25.657358 | Train accuracy: 0.96077503 | Test loss: 27.564606 | Test accuracy: 0.95706173]\n",
            "Trigger times: 1/5\n",
            "[65/120 | Train loss: 26.215628 | Train accuracy: 0.9609273 | Test loss: 27.625236 | Test accuracy: 0.95804938]\n",
            "Trigger times: 2/5\n",
            "[66/120 | Train loss: 25.888822 | Train accuracy: 0.96006173 | Test loss: 27.632054 | Test accuracy: 0.95716049]\n",
            "Trigger times: 3/5\n",
            "[67/120 | Train loss: 26.059175 | Train accuracy: 0.95996433 | Test loss: 27.886715 | Test accuracy: 0.95754321]\n",
            "Trigger times: 4/5\n",
            "[68/120 | Train loss: 25.049408 | Train accuracy: 0.96192455 | Test loss: 27.066508 | Test accuracy: 0.95816049]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[69/120 | Train loss: 25.884746 | Train accuracy: 0.96018656 | Test loss: 27.976827 | Test accuracy: 0.95681481]\n",
            "Trigger times: 1/5\n",
            "[70/120 | Train loss: 26.000718 | Train accuracy: 0.96074074 | Test loss: 27.708966 | Test accuracy: 0.95817284]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[71/120 | Train loss: 25.108296 | Train accuracy: 0.96146914 | Test loss: 27.041453 | Test accuracy: 0.9578642]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[72/120 | Train loss: 24.320353 | Train accuracy: 0.96208505 | Test loss: 26.522174 | Test accuracy: 0.95781481]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[73/120 | Train loss: 26.667752 | Train accuracy: 0.95914678 | Test loss: 28.887815 | Test accuracy: 0.95502469]\n",
            "Trigger times: 1/5\n",
            "[74/120 | Train loss: 24.254187 | Train accuracy: 0.96274897 | Test loss: 25.856326 | Test accuracy: 0.9592963]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[75/120 | Train loss: 25.180209 | Train accuracy: 0.96254733 | Test loss: 26.726255 | Test accuracy: 0.95925926]\n",
            "Trigger times: 1/5\n",
            "[76/120 | Train loss: 24.950149 | Train accuracy: 0.96166392 | Test loss: 26.721938 | Test accuracy: 0.95804938]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[77/120 | Train loss: 25.686617 | Train accuracy: 0.96043347 | Test loss: 27.391126 | Test accuracy: 0.95753086]\n",
            "Trigger times: 1/5\n",
            "[78/120 | Train loss: 24.063933 | Train accuracy: 0.96320988 | Test loss: 25.879048 | Test accuracy: 0.96060494]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n",
            "[79/120 | Train loss: 28.612931 | Train accuracy: 0.95562277 | Test loss: 30.095425 | Test accuracy: 0.9531358]\n",
            "Trigger times: 1/5\n",
            "[80/120 | Train loss: 25.980416 | Train accuracy: 0.95974211 | Test loss: 28.17013 | Test accuracy: 0.95583951]\n",
            "Trigger times: 0/5\n",
            "New checkpoint...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-c15400183317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwanna_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwanna_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_as\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv5_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not training this model anymore...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-27b343bc6253>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(device, model, epochs, optimizer, criteria, train_loader, valid_loader, resume, save_as)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "wanna_train = True\n",
        "if wanna_train:\n",
        "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v5_path)\n",
        "else:\n",
        "    print('Not training this model anymore...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co9EYZJKtOxY",
        "outputId": "99d55a71-026f-46e2-a8a7-04569b4af040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying last checkpoint of model NetV5 state to trained folder\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "v5_path_trained = Path('../Trained/NetV5.pth')\n",
        "if v5_path.exists():\n",
        "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV5'))\n",
        "    shutil.copy(v5_path, v5_path_trained)\n",
        "else:\n",
        "    print('Model {0} is not created yeat'.format('NetV5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOe8QZzvfDQB"
      },
      "outputs": [],
      "source": [
        "v5_path_trained = Path('../Trained/NetV5.pth')\n",
        "v5_state = torch.load(v5_path_trained)\n",
        "print(v5_state['best_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDDmY59NtOzs",
        "outputId": "f7a34a5a-1165-4a50-a1de-8f8281037e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id  target\n",
              "0  209540       0\n",
              "1  182173       0\n",
              "2  195991       1\n",
              "3   43111       0\n",
              "4  489479       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28a62206-d45c-48dc-9059-e8b8cc98c106\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>209540</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>182173</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>195991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43111</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>489479</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28a62206-d45c-48dc-9059-e8b8cc98c106')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-28a62206-d45c-48dc-9059-e8b8cc98c106 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-28a62206-d45c-48dc-9059-e8b8cc98c106');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/fb998edc550c7947/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 209540,\n            'f': \"209540\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': 182173,\n            'f': \"182173\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 2,\n            'f': \"2\",\n        },\n{\n            'v': 195991,\n            'f': \"195991\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [{\n            'v': 3,\n            'f': \"3\",\n        },\n{\n            'v': 43111,\n            'f': \"43111\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        }],\n [{\n            'v': 4,\n            'f': \"4\",\n        },\n{\n            'v': 489479,\n            'f': \"489479\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }]],\n        columns: [[\"number\", \"index\"], [\"number\", \"id\"], [\"number\", \"target\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    "
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "validate_path = Path('/content/drive/MyDrive/validate.csv') if IN_COLAB else Path('../Data/validate.csv')\n",
        "validate_df = pd.read_csv(validate_path)\n",
        "X_val = validate_df.drop('id', axis=1)\n",
        "X_val = add_letters_count(X_val)\n",
        "X_val = add_letter_position(X_val)\n",
        "\n",
        "X_val = X_val.drop(['f_27'] + letters_to_drop, axis=1)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_val_tensor = torch.from_numpy(X_val)\n",
        "X_val_tensor = X_val_tensor.type(torch.float)\n",
        "X_val_tensor = X_val_tensor.to(device)\n",
        "\n",
        "v5_path_trained = Path('../Trained/NetV5.pth')\n",
        "v5_state = torch.load(v5_path_trained)\n",
        "v5_state_dict = v5_state['best_state_dict']\n",
        "v5_model = Net3(X_val_tensor.shape[1])\n",
        "v5_model.load_state_dict(v5_state_dict)\n",
        "\n",
        "\n",
        "v5_model = v5_model.to(device)\n",
        "v5_model.eval()\n",
        "with torch.inference_mode():\n",
        "    outputs = v5_model(X_val_tensor)\n",
        "    y_pred = torch.round(torch.sigmoid(outputs))\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    \n",
        "submision_df = pd.DataFrame({\n",
        "    'id': validate_df['id'],\n",
        "    'target': y_pred.flatten()\n",
        "})\n",
        "\n",
        "submision_df = submision_df.astype('int')\n",
        "\n",
        "submit_path_model_v5 = Path('../Submit/V5.csv')\n",
        "submision_df.to_csv(submit_path_model_v5, index=False)\n",
        "submision_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJGKF8lttOrU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tgrt-b0tOcx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}