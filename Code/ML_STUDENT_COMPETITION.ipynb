{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wilberquito/NeuralNetworksCompetition2023/blob/main/Code/ML_STUDENT_COMPETITION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M.L Competition Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frSDoMZ6nROc",
    "outputId": "2e41768c-89ed-4f1a-842a-d7102161648d"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    from google.colab import data_table\n",
    "    data_table.DataTable.max_columns = 50\n",
    "    data_table.enable_dataframe_formatter()\n",
    "else:\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "    InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "-6Meh_vIqdTe",
    "outputId": "ecb278f0-fb78-4ac1-bf9b-861c9cc3cd65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20f9080d2f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>493553</td>\n",
       "      <td>0.315471</td>\n",
       "      <td>-0.183690</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>-1.186794</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.946208</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.690715</td>\n",
       "      <td>-0.628005</td>\n",
       "      <td>-2.832295</td>\n",
       "      <td>-1.409039</td>\n",
       "      <td>3.645067</td>\n",
       "      <td>0.233039</td>\n",
       "      <td>-3.754846</td>\n",
       "      <td>-1.061733</td>\n",
       "      <td>BDBBCACIBB</td>\n",
       "      <td>20.308715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>237346</td>\n",
       "      <td>-1.286392</td>\n",
       "      <td>1.780592</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>-2.690658</td>\n",
       "      <td>1.321997</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>0.371070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664517</td>\n",
       "      <td>-2.871912</td>\n",
       "      <td>3.826628</td>\n",
       "      <td>3.087653</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>3.210875</td>\n",
       "      <td>-0.666457</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>ACBDCBCADA</td>\n",
       "      <td>-449.291063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37368</td>\n",
       "      <td>-0.290303</td>\n",
       "      <td>-0.485907</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>1.083632</td>\n",
       "      <td>-1.129914</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.494988</td>\n",
       "      <td>-0.946303</td>\n",
       "      <td>2.333223</td>\n",
       "      <td>2.084169</td>\n",
       "      <td>-4.782668</td>\n",
       "      <td>-1.671375</td>\n",
       "      <td>2.774382</td>\n",
       "      <td>2.273130</td>\n",
       "      <td>AABBABCLAF</td>\n",
       "      <td>-86.206118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>665220</td>\n",
       "      <td>1.243590</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.854267</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>-2.020416</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.066427</td>\n",
       "      <td>-2.430158</td>\n",
       "      <td>-0.185332</td>\n",
       "      <td>-0.701691</td>\n",
       "      <td>-2.769142</td>\n",
       "      <td>-6.534231</td>\n",
       "      <td>-0.557677</td>\n",
       "      <td>-0.429972</td>\n",
       "      <td>ADBBABEEBA</td>\n",
       "      <td>-30.157403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41499</td>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-1.045483</td>\n",
       "      <td>0.718374</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>-0.936620</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.899984</td>\n",
       "      <td>1.427460</td>\n",
       "      <td>-4.992610</td>\n",
       "      <td>1.154162</td>\n",
       "      <td>-1.931443</td>\n",
       "      <td>2.325042</td>\n",
       "      <td>2.143811</td>\n",
       "      <td>-1.039599</td>\n",
       "      <td>ABBBBBCMBB</td>\n",
       "      <td>296.484562</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
       "0  493553  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208   \n",
       "1  237346 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894   \n",
       "2   37368 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914   \n",
       "3  665220  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892   \n",
       "4   41499  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451   \n",
       "\n",
       "       f_06  f_07  f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  \\\n",
       "0  0.729857     0     4     1     3     1     2     4     1     5     2     0   \n",
       "1  0.371070     3     0     3     3     5     3     2     0     1     6     0   \n",
       "2  0.767396     3     1     3     2     3     4     1     1     1     0     2   \n",
       "3 -2.020416     2     0     4     5     0     5     1     0     3     1     1   \n",
       "4 -0.936620     1     2     2     2     2     5     0     3     1     1     2   \n",
       "\n",
       "   f_18      f_19      f_20      f_21      f_22      f_23      f_24      f_25  \\\n",
       "0     1 -3.690715 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846   \n",
       "1     1  0.664517 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457   \n",
       "2     6 -0.494988 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382   \n",
       "3     3 -3.066427 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677   \n",
       "4     4 -1.899984  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811   \n",
       "\n",
       "       f_26        f_27        f_28  f_29  f_30  target  \n",
       "0 -1.061733  BDBBCACIBB   20.308715     1     0       0  \n",
       "1  0.123854  ACBDCBCADA -449.291063     1     0       0  \n",
       "2  2.273130  AABBABCLAF  -86.206118     0     1       1  \n",
       "3 -0.429972  ADBBABEEBA  -30.157403     0     2       1  \n",
       "4 -1.039599  ABBBBBCMBB  296.484562     0     2       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = Path('/content/drive/MyDrive/train.csv') if IN_COLAB else Path('../Data/train.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approuch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlPl0dVDrByn",
    "outputId": "f7521bab-1e56-406e-cc68-be2e4b7dd827"
   },
   "outputs": [],
   "source": [
    "X, y = df.drop(['target', 'f_27', 'id'], axis=1), df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "26pKNWPurBwZ",
    "outputId": "d86646d1-3361-4e42-d119-a949102f62ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "      <td>810000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>-0.001910</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>-0.000971</td>\n",
       "      <td>2.032086</td>\n",
       "      <td>2.057980</td>\n",
       "      <td>2.362590</td>\n",
       "      <td>2.177847</td>\n",
       "      <td>1.804038</td>\n",
       "      <td>2.842322</td>\n",
       "      <td>2.239307</td>\n",
       "      <td>1.515411</td>\n",
       "      <td>2.101359</td>\n",
       "      <td>2.096502</td>\n",
       "      <td>1.857758</td>\n",
       "      <td>2.065975</td>\n",
       "      <td>0.307220</td>\n",
       "      <td>-0.178796</td>\n",
       "      <td>-0.156877</td>\n",
       "      <td>-0.009749</td>\n",
       "      <td>-0.369114</td>\n",
       "      <td>-0.342708</td>\n",
       "      <td>0.175932</td>\n",
       "      <td>0.356640</td>\n",
       "      <td>-0.448086</td>\n",
       "      <td>0.345565</td>\n",
       "      <td>1.002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.998719</td>\n",
       "      <td>0.999402</td>\n",
       "      <td>1.000892</td>\n",
       "      <td>1.000081</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999535</td>\n",
       "      <td>1.656749</td>\n",
       "      <td>1.590698</td>\n",
       "      <td>1.637473</td>\n",
       "      <td>1.645519</td>\n",
       "      <td>1.537589</td>\n",
       "      <td>1.762672</td>\n",
       "      <td>1.537712</td>\n",
       "      <td>1.359798</td>\n",
       "      <td>1.568952</td>\n",
       "      <td>1.559978</td>\n",
       "      <td>1.467507</td>\n",
       "      <td>1.565593</td>\n",
       "      <td>2.314858</td>\n",
       "      <td>2.400672</td>\n",
       "      <td>2.484487</td>\n",
       "      <td>2.450494</td>\n",
       "      <td>2.454113</td>\n",
       "      <td>2.387102</td>\n",
       "      <td>2.416753</td>\n",
       "      <td>2.476792</td>\n",
       "      <td>238.735832</td>\n",
       "      <td>0.475553</td>\n",
       "      <td>0.818851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.599856</td>\n",
       "      <td>-4.682199</td>\n",
       "      <td>-4.642676</td>\n",
       "      <td>-4.628484</td>\n",
       "      <td>-4.748501</td>\n",
       "      <td>-4.750214</td>\n",
       "      <td>-4.842919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-11.280941</td>\n",
       "      <td>-11.257917</td>\n",
       "      <td>-12.183785</td>\n",
       "      <td>-11.853530</td>\n",
       "      <td>-12.301097</td>\n",
       "      <td>-11.416189</td>\n",
       "      <td>-11.918306</td>\n",
       "      <td>-14.300577</td>\n",
       "      <td>-1229.753052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.674386</td>\n",
       "      <td>-0.674814</td>\n",
       "      <td>-0.674948</td>\n",
       "      <td>-0.676899</td>\n",
       "      <td>-0.676212</td>\n",
       "      <td>-0.672953</td>\n",
       "      <td>-0.675204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.236465</td>\n",
       "      <td>-1.804803</td>\n",
       "      <td>-1.819358</td>\n",
       "      <td>-1.645711</td>\n",
       "      <td>-2.019762</td>\n",
       "      <td>-1.956021</td>\n",
       "      <td>-1.440454</td>\n",
       "      <td>-1.262614</td>\n",
       "      <td>-159.456219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>-0.002556</td>\n",
       "      <td>-0.001907</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>-0.001630</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.329103</td>\n",
       "      <td>-0.189919</td>\n",
       "      <td>-0.153055</td>\n",
       "      <td>0.030148</td>\n",
       "      <td>-0.390120</td>\n",
       "      <td>-0.342316</td>\n",
       "      <td>0.160621</td>\n",
       "      <td>0.405216</td>\n",
       "      <td>-0.573352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.674684</td>\n",
       "      <td>0.675737</td>\n",
       "      <td>0.676736</td>\n",
       "      <td>0.671851</td>\n",
       "      <td>0.672968</td>\n",
       "      <td>0.675826</td>\n",
       "      <td>0.673790</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.878621</td>\n",
       "      <td>1.444271</td>\n",
       "      <td>1.506658</td>\n",
       "      <td>1.660455</td>\n",
       "      <td>1.257267</td>\n",
       "      <td>1.266450</td>\n",
       "      <td>1.794780</td>\n",
       "      <td>2.028263</td>\n",
       "      <td>158.941960</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.749301</td>\n",
       "      <td>4.815699</td>\n",
       "      <td>4.961982</td>\n",
       "      <td>4.454920</td>\n",
       "      <td>4.948983</td>\n",
       "      <td>4.971881</td>\n",
       "      <td>4.822668</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.108150</td>\n",
       "      <td>11.475325</td>\n",
       "      <td>12.029242</td>\n",
       "      <td>11.344080</td>\n",
       "      <td>12.247100</td>\n",
       "      <td>12.389844</td>\n",
       "      <td>12.529179</td>\n",
       "      <td>12.913041</td>\n",
       "      <td>1229.562577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f_00           f_01           f_02           f_03  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        0.000368       0.001564       0.000306      -0.001910   \n",
       "std         0.998719       0.999402       1.000892       1.000081   \n",
       "min        -4.599856      -4.682199      -4.642676      -4.628484   \n",
       "25%        -0.674386      -0.674814      -0.674948      -0.676899   \n",
       "50%         0.002005       0.002531       0.001148      -0.002556   \n",
       "75%         0.674684       0.675737       0.676736       0.671851   \n",
       "max         4.749301       4.815699       4.961982       4.454920   \n",
       "\n",
       "                f_04           f_05           f_06           f_07  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean       -0.000820       0.000644      -0.000971       2.032086   \n",
       "std         1.000000       0.999999       0.999535       1.656749   \n",
       "min        -4.748501      -4.750214      -4.842919       0.000000   \n",
       "25%        -0.676212      -0.672953      -0.675204       1.000000   \n",
       "50%        -0.001907       0.000071      -0.001630       2.000000   \n",
       "75%         0.672968       0.675826       0.673790       3.000000   \n",
       "max         4.948983       4.971881       4.822668      15.000000   \n",
       "\n",
       "                f_08           f_09           f_10           f_11  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        2.057980       2.362590       2.177847       1.804038   \n",
       "std         1.590698       1.637473       1.645519       1.537589   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         1.000000       1.000000       1.000000       1.000000   \n",
       "50%         2.000000       2.000000       2.000000       2.000000   \n",
       "75%         3.000000       3.000000       3.000000       3.000000   \n",
       "max        16.000000      14.000000      14.000000      13.000000   \n",
       "\n",
       "                f_12           f_13           f_14           f_15  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        2.842322       2.239307       1.515411       2.101359   \n",
       "std         1.762672       1.537712       1.359798       1.568952   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         2.000000       1.000000       0.000000       1.000000   \n",
       "50%         3.000000       2.000000       1.000000       2.000000   \n",
       "75%         4.000000       3.000000       2.000000       3.000000   \n",
       "max        16.000000      12.000000      14.000000      14.000000   \n",
       "\n",
       "                f_16           f_17           f_18           f_19  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean        2.096502       1.857758       2.065975       0.307220   \n",
       "std         1.559978       1.467507       1.565593       2.314858   \n",
       "min         0.000000       0.000000       0.000000     -11.280941   \n",
       "25%         1.000000       1.000000       1.000000      -1.236465   \n",
       "50%         2.000000       2.000000       2.000000       0.329103   \n",
       "75%         3.000000       3.000000       3.000000       1.878621   \n",
       "max        15.000000      12.000000      13.000000      11.108150   \n",
       "\n",
       "                f_20           f_21           f_22           f_23  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean       -0.178796      -0.156877      -0.009749      -0.369114   \n",
       "std         2.400672       2.484487       2.450494       2.454113   \n",
       "min       -11.257917     -12.183785     -11.853530     -12.301097   \n",
       "25%        -1.804803      -1.819358      -1.645711      -2.019762   \n",
       "50%        -0.189919      -0.153055       0.030148      -0.390120   \n",
       "75%         1.444271       1.506658       1.660455       1.257267   \n",
       "max        11.475325      12.029242      11.344080      12.247100   \n",
       "\n",
       "                f_24           f_25           f_26           f_28  \\\n",
       "count  810000.000000  810000.000000  810000.000000  810000.000000   \n",
       "mean       -0.342708       0.175932       0.356640      -0.448086   \n",
       "std         2.387102       2.416753       2.476792     238.735832   \n",
       "min       -11.416189     -11.918306     -14.300577   -1229.753052   \n",
       "25%        -1.956021      -1.440454      -1.262614    -159.456219   \n",
       "50%        -0.342316       0.160621       0.405216      -0.573352   \n",
       "75%         1.266450       1.794780       2.028263     158.941960   \n",
       "max        12.389844      12.529179      12.913041    1229.562577   \n",
       "\n",
       "                f_29           f_30  \n",
       "count  810000.000000  810000.000000  \n",
       "mean        0.345565       1.002373  \n",
       "std         0.475553       0.818851  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       1.000000  \n",
       "75%         1.000000       2.000000  \n",
       "max         1.000000       2.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmznjs5grBnV",
    "outputId": "c7388ce9-7b1c-4dfc-c48a-4b832222cc6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    415945\n",
       "1    394055\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nc_P-uEg4BZ",
    "outputId": "da2894c5-9823-461f-ec2f-33f130e7e10b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6xuWymPQrBd_"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features=30):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x)\n",
    "\n",
    "# Define a custom dataset by extending the PyTorch Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        return item, label\n",
    "    \n",
    "def datasets(dataset, stratify, test_size):\n",
    "    train_size = 1 - test_size\n",
    "\n",
    "    # Stratify by label\n",
    "    labels = np.array(stratify)\n",
    "    positive_indices = np.where(labels == 1)[0]\n",
    "    negative_indices = np.where(labels == 0)[0]\n",
    "\n",
    "    positive_split = int(train_size * len(positive_indices))\n",
    "    negative_split = int(train_size * len(negative_indices))\n",
    "\n",
    "    positive_train_indices = positive_indices[:positive_split]\n",
    "    positive_test_indices = positive_indices[positive_split:]\n",
    "    negative_train_indices = negative_indices[:negative_split]\n",
    "    negative_test_indices = negative_indices[negative_split:]\n",
    "\n",
    "    train_indices = np.concatenate([positive_train_indices, negative_train_indices])\n",
    "    test_indices = np.concatenate([positive_test_indices, negative_test_indices])\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315508</td>\n",
       "      <td>-0.185365</td>\n",
       "      <td>0.663485</td>\n",
       "      <td>-1.184789</td>\n",
       "      <td>0.665919</td>\n",
       "      <td>0.945566</td>\n",
       "      <td>0.731168</td>\n",
       "      <td>-1.226551</td>\n",
       "      <td>1.220861</td>\n",
       "      <td>-0.832130</td>\n",
       "      <td>0.499632</td>\n",
       "      <td>-0.522922</td>\n",
       "      <td>-0.477867</td>\n",
       "      <td>1.145009</td>\n",
       "      <td>-0.379035</td>\n",
       "      <td>1.847503</td>\n",
       "      <td>-0.061861</td>\n",
       "      <td>-1.265929</td>\n",
       "      <td>-0.680877</td>\n",
       "      <td>-1.727077</td>\n",
       "      <td>-0.187118</td>\n",
       "      <td>-1.076850</td>\n",
       "      <td>-0.571024</td>\n",
       "      <td>1.635696</td>\n",
       "      <td>0.241191</td>\n",
       "      <td>-1.626472</td>\n",
       "      <td>-0.572666</td>\n",
       "      <td>0.086945</td>\n",
       "      <td>1.376157</td>\n",
       "      <td>-1.224123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.288412</td>\n",
       "      <td>1.780094</td>\n",
       "      <td>0.575879</td>\n",
       "      <td>-2.688532</td>\n",
       "      <td>1.322818</td>\n",
       "      <td>-0.676539</td>\n",
       "      <td>0.372214</td>\n",
       "      <td>0.584225</td>\n",
       "      <td>-1.293760</td>\n",
       "      <td>0.389265</td>\n",
       "      <td>0.499632</td>\n",
       "      <td>2.078555</td>\n",
       "      <td>0.089454</td>\n",
       "      <td>-0.155626</td>\n",
       "      <td>-1.114439</td>\n",
       "      <td>-0.701972</td>\n",
       "      <td>2.502279</td>\n",
       "      <td>-1.265929</td>\n",
       "      <td>-0.680877</td>\n",
       "      <td>0.154349</td>\n",
       "      <td>-1.121819</td>\n",
       "      <td>1.603352</td>\n",
       "      <td>1.263992</td>\n",
       "      <td>0.351786</td>\n",
       "      <td>1.488661</td>\n",
       "      <td>-0.348562</td>\n",
       "      <td>-0.093987</td>\n",
       "      <td>-1.880083</td>\n",
       "      <td>1.376157</td>\n",
       "      <td>-1.224123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.291044</td>\n",
       "      <td>-0.487763</td>\n",
       "      <td>0.807325</td>\n",
       "      <td>-0.154365</td>\n",
       "      <td>1.084453</td>\n",
       "      <td>-1.130560</td>\n",
       "      <td>0.768724</td>\n",
       "      <td>0.584225</td>\n",
       "      <td>-0.665105</td>\n",
       "      <td>0.389265</td>\n",
       "      <td>-0.108080</td>\n",
       "      <td>0.777817</td>\n",
       "      <td>0.656775</td>\n",
       "      <td>-0.805943</td>\n",
       "      <td>-0.379035</td>\n",
       "      <td>-0.701972</td>\n",
       "      <td>-1.343932</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>2.512803</td>\n",
       "      <td>-0.346548</td>\n",
       "      <td>-0.319705</td>\n",
       "      <td>1.002260</td>\n",
       "      <td>0.854489</td>\n",
       "      <td>-1.798432</td>\n",
       "      <td>-0.556603</td>\n",
       "      <td>1.075183</td>\n",
       "      <td>0.773779</td>\n",
       "      <td>-0.359217</td>\n",
       "      <td>-0.726661</td>\n",
       "      <td>-0.002898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244818</td>\n",
       "      <td>0.033569</td>\n",
       "      <td>-1.012640</td>\n",
       "      <td>0.856109</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.597249</td>\n",
       "      <td>-2.020385</td>\n",
       "      <td>-0.019367</td>\n",
       "      <td>-1.293760</td>\n",
       "      <td>0.999962</td>\n",
       "      <td>1.715054</td>\n",
       "      <td>-1.173291</td>\n",
       "      <td>1.224096</td>\n",
       "      <td>-0.805943</td>\n",
       "      <td>-1.114439</td>\n",
       "      <td>0.572765</td>\n",
       "      <td>-0.702897</td>\n",
       "      <td>-0.584500</td>\n",
       "      <td>0.596595</td>\n",
       "      <td>-1.457389</td>\n",
       "      <td>-0.937806</td>\n",
       "      <td>-0.011453</td>\n",
       "      <td>-0.282369</td>\n",
       "      <td>-0.977962</td>\n",
       "      <td>-2.593742</td>\n",
       "      <td>-0.303552</td>\n",
       "      <td>-0.317593</td>\n",
       "      <td>-0.124444</td>\n",
       "      <td>-0.726661</td>\n",
       "      <td>1.218327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.703249</td>\n",
       "      <td>0.815969</td>\n",
       "      <td>-0.065154</td>\n",
       "      <td>-1.043488</td>\n",
       "      <td>0.719195</td>\n",
       "      <td>0.163808</td>\n",
       "      <td>-0.936084</td>\n",
       "      <td>-0.622959</td>\n",
       "      <td>-0.036450</td>\n",
       "      <td>-0.221433</td>\n",
       "      <td>-0.108080</td>\n",
       "      <td>0.127447</td>\n",
       "      <td>1.224096</td>\n",
       "      <td>-1.456260</td>\n",
       "      <td>1.091772</td>\n",
       "      <td>-0.701972</td>\n",
       "      <td>-0.702897</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>1.235331</td>\n",
       "      <td>-0.953495</td>\n",
       "      <td>0.669086</td>\n",
       "      <td>-1.946372</td>\n",
       "      <td>0.474970</td>\n",
       "      <td>-0.636617</td>\n",
       "      <td>1.117569</td>\n",
       "      <td>0.814266</td>\n",
       "      <td>-0.563729</td>\n",
       "      <td>1.243772</td>\n",
       "      <td>-0.726661</td>\n",
       "      <td>1.218327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "0  0.315508 -0.185365  0.663485 -1.184789  0.665919  0.945566  0.731168   \n",
       "1 -1.288412  1.780094  0.575879 -2.688532  1.322818 -0.676539  0.372214   \n",
       "2 -0.291044 -0.487763  0.807325 -0.154365  1.084453 -1.130560  0.768724   \n",
       "3  1.244818  0.033569 -1.012640  0.856109  0.020013  0.597249 -2.020385   \n",
       "4  0.703249  0.815969 -0.065154 -1.043488  0.719195  0.163808 -0.936084   \n",
       "\n",
       "       f_07      f_08      f_09      f_10      f_11      f_12      f_13  \\\n",
       "0 -1.226551  1.220861 -0.832130  0.499632 -0.522922 -0.477867  1.145009   \n",
       "1  0.584225 -1.293760  0.389265  0.499632  2.078555  0.089454 -0.155626   \n",
       "2  0.584225 -0.665105  0.389265 -0.108080  0.777817  0.656775 -0.805943   \n",
       "3 -0.019367 -1.293760  0.999962  1.715054 -1.173291  1.224096 -0.805943   \n",
       "4 -0.622959 -0.036450 -0.221433 -0.108080  0.127447  1.224096 -1.456260   \n",
       "\n",
       "       f_14      f_15      f_16      f_17      f_18      f_19      f_20  \\\n",
       "0 -0.379035  1.847503 -0.061861 -1.265929 -0.680877 -1.727077 -0.187118   \n",
       "1 -1.114439 -0.701972  2.502279 -1.265929 -0.680877  0.154349 -1.121819   \n",
       "2 -0.379035 -0.701972 -1.343932  0.096928  2.512803 -0.346548 -0.319705   \n",
       "3 -1.114439  0.572765 -0.702897 -0.584500  0.596595 -1.457389 -0.937806   \n",
       "4  1.091772 -0.701972 -0.702897  0.096928  1.235331 -0.953495  0.669086   \n",
       "\n",
       "       f_21      f_22      f_23      f_24      f_25      f_26      f_28  \\\n",
       "0 -1.076850 -0.571024  1.635696  0.241191 -1.626472 -0.572666  0.086945   \n",
       "1  1.603352  1.263992  0.351786  1.488661 -0.348562 -0.093987 -1.880083   \n",
       "2  1.002260  0.854489 -1.798432 -0.556603  1.075183  0.773779 -0.359217   \n",
       "3 -0.011453 -0.282369 -0.977962 -2.593742 -0.303552 -0.317593 -0.124444   \n",
       "4 -1.946372  0.474970 -0.636617  1.117569  0.814266 -0.563729  1.243772   \n",
       "\n",
       "       f_29      f_30  \n",
       "0  1.376157 -1.224123  \n",
       "1  1.376157 -1.224123  \n",
       "2 -0.726661 -0.002898  \n",
       "3 -0.726661  1.218327  \n",
       "4 -0.726661  1.218327  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "      <td>8.100000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.277222e-17</td>\n",
       "      <td>-2.741291e-18</td>\n",
       "      <td>-1.000900e-17</td>\n",
       "      <td>-7.184377e-18</td>\n",
       "      <td>2.745677e-18</td>\n",
       "      <td>-9.684434e-18</td>\n",
       "      <td>-1.142132e-17</td>\n",
       "      <td>-5.172049e-17</td>\n",
       "      <td>-1.485648e-16</td>\n",
       "      <td>6.184353e-17</td>\n",
       "      <td>6.965073e-17</td>\n",
       "      <td>-3.005333e-17</td>\n",
       "      <td>1.459683e-17</td>\n",
       "      <td>-1.030901e-16</td>\n",
       "      <td>9.513378e-17</td>\n",
       "      <td>-6.732612e-17</td>\n",
       "      <td>1.230906e-16</td>\n",
       "      <td>-1.571966e-17</td>\n",
       "      <td>-6.828228e-17</td>\n",
       "      <td>-7.308941e-17</td>\n",
       "      <td>1.747409e-17</td>\n",
       "      <td>-2.929892e-18</td>\n",
       "      <td>-4.035181e-18</td>\n",
       "      <td>-2.333387e-18</td>\n",
       "      <td>1.224590e-17</td>\n",
       "      <td>3.477273e-17</td>\n",
       "      <td>7.921236e-17</td>\n",
       "      <td>-1.642143e-17</td>\n",
       "      <td>1.593019e-17</td>\n",
       "      <td>-1.786445e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.000001e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.606128e+00</td>\n",
       "      <td>-4.686569e+00</td>\n",
       "      <td>-4.638847e+00</td>\n",
       "      <td>-4.626203e+00</td>\n",
       "      <td>-4.747684e+00</td>\n",
       "      <td>-4.750866e+00</td>\n",
       "      <td>-4.844201e+00</td>\n",
       "      <td>-1.226551e+00</td>\n",
       "      <td>-1.293760e+00</td>\n",
       "      <td>-1.442828e+00</td>\n",
       "      <td>-1.323502e+00</td>\n",
       "      <td>-1.173291e+00</td>\n",
       "      <td>-1.612509e+00</td>\n",
       "      <td>-1.456260e+00</td>\n",
       "      <td>-1.114439e+00</td>\n",
       "      <td>-1.339340e+00</td>\n",
       "      <td>-1.343932e+00</td>\n",
       "      <td>-1.265929e+00</td>\n",
       "      <td>-1.319613e+00</td>\n",
       "      <td>-5.005996e+00</td>\n",
       "      <td>-4.615012e+00</td>\n",
       "      <td>-4.840804e+00</td>\n",
       "      <td>-4.833224e+00</td>\n",
       "      <td>-4.862037e+00</td>\n",
       "      <td>-4.638884e+00</td>\n",
       "      <td>-5.004338e+00</td>\n",
       "      <td>-5.917825e+00</td>\n",
       "      <td>-5.149230e+00</td>\n",
       "      <td>-7.266613e-01</td>\n",
       "      <td>-1.224123e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.756203e-01</td>\n",
       "      <td>-6.767832e-01</td>\n",
       "      <td>-6.746520e-01</td>\n",
       "      <td>-6.749347e-01</td>\n",
       "      <td>-6.753925e-01</td>\n",
       "      <td>-6.735980e-01</td>\n",
       "      <td>-6.745475e-01</td>\n",
       "      <td>-6.229591e-01</td>\n",
       "      <td>-6.651048e-01</td>\n",
       "      <td>-8.321305e-01</td>\n",
       "      <td>-7.157908e-01</td>\n",
       "      <td>-5.229217e-01</td>\n",
       "      <td>-4.778670e-01</td>\n",
       "      <td>-8.059429e-01</td>\n",
       "      <td>-1.114439e+00</td>\n",
       "      <td>-7.019718e-01</td>\n",
       "      <td>-7.028966e-01</td>\n",
       "      <td>-5.845004e-01</td>\n",
       "      <td>-6.808767e-01</td>\n",
       "      <td>-6.668599e-01</td>\n",
       "      <td>-6.773138e-01</td>\n",
       "      <td>-6.691450e-01</td>\n",
       "      <td>-6.676056e-01</td>\n",
       "      <td>-6.726050e-01</td>\n",
       "      <td>-6.758463e-01</td>\n",
       "      <td>-6.688259e-01</td>\n",
       "      <td>-6.537707e-01</td>\n",
       "      <td>-6.660426e-01</td>\n",
       "      <td>-7.266613e-01</td>\n",
       "      <td>-1.224123e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.639482e-03</td>\n",
       "      <td>9.684993e-04</td>\n",
       "      <td>8.415122e-04</td>\n",
       "      <td>-6.451620e-04</td>\n",
       "      <td>-1.086850e-03</td>\n",
       "      <td>-5.721649e-04</td>\n",
       "      <td>-6.598558e-04</td>\n",
       "      <td>-1.936711e-02</td>\n",
       "      <td>-3.644958e-02</td>\n",
       "      <td>-2.214329e-01</td>\n",
       "      <td>-1.080796e-01</td>\n",
       "      <td>1.274475e-01</td>\n",
       "      <td>8.945390e-02</td>\n",
       "      <td>-1.556257e-01</td>\n",
       "      <td>-3.790352e-01</td>\n",
       "      <td>-6.460321e-02</td>\n",
       "      <td>-6.186147e-02</td>\n",
       "      <td>9.692768e-02</td>\n",
       "      <td>-4.214080e-02</td>\n",
       "      <td>9.453292e-03</td>\n",
       "      <td>-4.633341e-03</td>\n",
       "      <td>1.538327e-03</td>\n",
       "      <td>1.628116e-02</td>\n",
       "      <td>-8.559671e-03</td>\n",
       "      <td>1.643847e-04</td>\n",
       "      <td>-6.335318e-03</td>\n",
       "      <td>1.961251e-02</td>\n",
       "      <td>-5.247088e-04</td>\n",
       "      <td>-7.266613e-01</td>\n",
       "      <td>-2.897771e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.751812e-01</td>\n",
       "      <td>6.745771e-01</td>\n",
       "      <td>6.758278e-01</td>\n",
       "      <td>6.737077e-01</td>\n",
       "      <td>6.737887e-01</td>\n",
       "      <td>6.751838e-01</td>\n",
       "      <td>6.750750e-01</td>\n",
       "      <td>5.842249e-01</td>\n",
       "      <td>5.922056e-01</td>\n",
       "      <td>3.892647e-01</td>\n",
       "      <td>4.996316e-01</td>\n",
       "      <td>7.778166e-01</td>\n",
       "      <td>6.567748e-01</td>\n",
       "      <td>4.946915e-01</td>\n",
       "      <td>3.563685e-01</td>\n",
       "      <td>5.727654e-01</td>\n",
       "      <td>5.791736e-01</td>\n",
       "      <td>7.783558e-01</td>\n",
       "      <td>5.965951e-01</td>\n",
       "      <td>6.788330e-01</td>\n",
       "      <td>6.760890e-01</td>\n",
       "      <td>6.695690e-01</td>\n",
       "      <td>6.815786e-01</td>\n",
       "      <td>6.627166e-01</td>\n",
       "      <td>6.741058e-01</td>\n",
       "      <td>6.698445e-01</td>\n",
       "      <td>6.749149e-01</td>\n",
       "      <td>6.676423e-01</td>\n",
       "      <td>1.376157e+00</td>\n",
       "      <td>1.218327e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.755027e+00</td>\n",
       "      <td>4.817020e+00</td>\n",
       "      <td>4.957258e+00</td>\n",
       "      <td>4.456473e+00</td>\n",
       "      <td>4.949807e+00</td>\n",
       "      <td>4.971246e+00</td>\n",
       "      <td>4.825883e+00</td>\n",
       "      <td>7.827329e+00</td>\n",
       "      <td>8.764723e+00</td>\n",
       "      <td>7.106938e+00</td>\n",
       "      <td>7.184455e+00</td>\n",
       "      <td>7.281508e+00</td>\n",
       "      <td>7.464626e+00</td>\n",
       "      <td>6.347546e+00</td>\n",
       "      <td>9.181213e+00</td>\n",
       "      <td>7.583820e+00</td>\n",
       "      <td>8.271595e+00</td>\n",
       "      <td>6.911209e+00</td>\n",
       "      <td>6.983954e+00</td>\n",
       "      <td>4.665918e+00</td>\n",
       "      <td>4.854528e+00</td>\n",
       "      <td>4.904886e+00</td>\n",
       "      <td>4.633284e+00</td>\n",
       "      <td>5.140847e+00</td>\n",
       "      <td>5.333899e+00</td>\n",
       "      <td>5.111510e+00</td>\n",
       "      <td>5.069625e+00</td>\n",
       "      <td>5.152186e+00</td>\n",
       "      <td>1.376157e+00</td>\n",
       "      <td>1.218327e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f_00          f_01          f_02          f_03          f_04  \\\n",
       "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
       "mean   1.277222e-17 -2.741291e-18 -1.000900e-17 -7.184377e-18  2.745677e-18   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -4.606128e+00 -4.686569e+00 -4.638847e+00 -4.626203e+00 -4.747684e+00   \n",
       "25%   -6.756203e-01 -6.767832e-01 -6.746520e-01 -6.749347e-01 -6.753925e-01   \n",
       "50%    1.639482e-03  9.684993e-04  8.415122e-04 -6.451620e-04 -1.086850e-03   \n",
       "75%    6.751812e-01  6.745771e-01  6.758278e-01  6.737077e-01  6.737887e-01   \n",
       "max    4.755027e+00  4.817020e+00  4.957258e+00  4.456473e+00  4.949807e+00   \n",
       "\n",
       "               f_05          f_06          f_07          f_08          f_09  \\\n",
       "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
       "mean  -9.684434e-18 -1.142132e-17 -5.172049e-17 -1.485648e-16  6.184353e-17   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -4.750866e+00 -4.844201e+00 -1.226551e+00 -1.293760e+00 -1.442828e+00   \n",
       "25%   -6.735980e-01 -6.745475e-01 -6.229591e-01 -6.651048e-01 -8.321305e-01   \n",
       "50%   -5.721649e-04 -6.598558e-04 -1.936711e-02 -3.644958e-02 -2.214329e-01   \n",
       "75%    6.751838e-01  6.750750e-01  5.842249e-01  5.922056e-01  3.892647e-01   \n",
       "max    4.971246e+00  4.825883e+00  7.827329e+00  8.764723e+00  7.106938e+00   \n",
       "\n",
       "               f_10          f_11          f_12          f_13          f_14  \\\n",
       "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
       "mean   6.965073e-17 -3.005333e-17  1.459683e-17 -1.030901e-16  9.513378e-17   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -1.323502e+00 -1.173291e+00 -1.612509e+00 -1.456260e+00 -1.114439e+00   \n",
       "25%   -7.157908e-01 -5.229217e-01 -4.778670e-01 -8.059429e-01 -1.114439e+00   \n",
       "50%   -1.080796e-01  1.274475e-01  8.945390e-02 -1.556257e-01 -3.790352e-01   \n",
       "75%    4.996316e-01  7.778166e-01  6.567748e-01  4.946915e-01  3.563685e-01   \n",
       "max    7.184455e+00  7.281508e+00  7.464626e+00  6.347546e+00  9.181213e+00   \n",
       "\n",
       "               f_15          f_16          f_17          f_18          f_19  \\\n",
       "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
       "mean  -6.732612e-17  1.230906e-16 -1.571966e-17 -6.828228e-17 -7.308941e-17   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -1.339340e+00 -1.343932e+00 -1.265929e+00 -1.319613e+00 -5.005996e+00   \n",
       "25%   -7.019718e-01 -7.028966e-01 -5.845004e-01 -6.808767e-01 -6.668599e-01   \n",
       "50%   -6.460321e-02 -6.186147e-02  9.692768e-02 -4.214080e-02  9.453292e-03   \n",
       "75%    5.727654e-01  5.791736e-01  7.783558e-01  5.965951e-01  6.788330e-01   \n",
       "max    7.583820e+00  8.271595e+00  6.911209e+00  6.983954e+00  4.665918e+00   \n",
       "\n",
       "               f_20          f_21          f_22          f_23          f_24  \\\n",
       "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05   \n",
       "mean   1.747409e-17 -2.929892e-18 -4.035181e-18 -2.333387e-18  1.224590e-17   \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00   \n",
       "min   -4.615012e+00 -4.840804e+00 -4.833224e+00 -4.862037e+00 -4.638884e+00   \n",
       "25%   -6.773138e-01 -6.691450e-01 -6.676056e-01 -6.726050e-01 -6.758463e-01   \n",
       "50%   -4.633341e-03  1.538327e-03  1.628116e-02 -8.559671e-03  1.643847e-04   \n",
       "75%    6.760890e-01  6.695690e-01  6.815786e-01  6.627166e-01  6.741058e-01   \n",
       "max    4.854528e+00  4.904886e+00  4.633284e+00  5.140847e+00  5.333899e+00   \n",
       "\n",
       "               f_25          f_26          f_28          f_29          f_30  \n",
       "count  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  8.100000e+05  \n",
       "mean   3.477273e-17  7.921236e-17 -1.642143e-17  1.593019e-17 -1.786445e-17  \n",
       "std    1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  1.000001e+00  \n",
       "min   -5.004338e+00 -5.917825e+00 -5.149230e+00 -7.266613e-01 -1.224123e+00  \n",
       "25%   -6.688259e-01 -6.537707e-01 -6.660426e-01 -7.266613e-01 -1.224123e+00  \n",
       "50%   -6.335318e-03  1.961251e-02 -5.247088e-04 -7.266613e-01 -2.897771e-03  \n",
       "75%    6.698445e-01  6.749149e-01  6.676423e-01  1.376157e+00  1.218327e+00  \n",
       "max    5.111510e+00  5.069625e+00  5.152186e+00  1.376157e+00  1.218327e+00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ceGWFcgVykIZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the hole dataset\n",
    "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
    "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
    "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5LkKFcbPwlHM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(688499, 121501)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KUnpQLqcwylO"
   },
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "def checkpoint(epoch, model, optimizer, criteria, save_as: Path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'criteria': criteria,\n",
    "    }, save_as)\n",
    "    \n",
    "\n",
    "# Validate\n",
    "def validation(model, device, valid_loader, criteria):\n",
    "    # Settings\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    accuracy_total = 0\n",
    "\n",
    "    # Test validation data\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss_total += loss.item() * inputs.size(0)\n",
    "            matches = torch.round(torch.sigmoid(outputs)) == labels\n",
    "            accuracy_total += (torch.sum(matches.to(torch.int32))).item()\n",
    "                        \n",
    "    return (loss_total / len(valid_loader), accuracy_total / len(valid_loader.dataset))\n",
    "\n",
    "\n",
    "# Train\n",
    "def train(device, model, epochs, optimizer, criteria, train_loader, valid_loader, resume=1, save_as=Path('../Output/model.pth')):\n",
    "    # Early stopping\n",
    "    last_loss = 100\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "    \n",
    "    train_loss_track, train_accuracy_track = [], []\n",
    "    test_loss_track, test_accuracy_track = [], []\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(resume, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        for times, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward and backward propagation\n",
    "            outputs = model(inputs).squeeze(dim=1)\n",
    "            loss = criteria(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Train loss and train accuracy\n",
    "        train_current_loss, train_current_accuracy = validation(model, device, train_loader, criteria)\n",
    "\n",
    "        # Early stopping\n",
    "        current_loss, current_accuracy = validation(model, device, valid_loader, criteria)\n",
    "        print('[{}/{} | Train loss: {:.8} | Train accuracy: {:.8} | Test loss: {:.8} | Test accuracy: {:.8}]'.format(epoch, epochs, train_current_loss, train_current_accuracy, current_loss, current_accuracy))\n",
    "        \n",
    "        train_loss_track.append(train_current_loss)\n",
    "        train_accuracy_track.append(train_current_accuracy)\n",
    "        test_loss_track.append(current_loss)\n",
    "        test_accuracy_track.append(current_accuracy)\n",
    "        \n",
    "        if current_loss > last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger times: {}/{}'.format(trigger_times, patience))\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model\n",
    "\n",
    "        else:\n",
    "            print('Trigger times: 0/{}'.format(patience))\n",
    "            trigger_times = 0\n",
    "\n",
    "        last_loss = current_loss\n",
    "        \n",
    "        if trigger_times == 0:\n",
    "            print('New checkpoint...')\n",
    "            checkpoint(epoch, model, optimizer, criteria, save_as=save_as)\n",
    "\n",
    "    return model, (train_loss_track, train_accuracy_track), (test_loss_track, test_accuracy_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sZnDhkhlwyiV",
    "outputId": "d5c2646c-1dff-41af-f336-44eda69a44f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for new training process... =)\n"
     ]
    }
   ],
   "source": [
    "model = Net(input_features=X_tensor.shape[1])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "resume = 1\n",
    "v0_path = Path('../Output/NetV0.pth')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "recover = False\n",
    "\n",
    "if v0_path.exists() and recover:\n",
    "    print('Recovering old training process...')\n",
    "    checkpoint = torch.load(v0_path)\n",
    "    resume = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criteria = checkpoint['criteria']\n",
    "else:\n",
    "    print('Ready for new training process... =)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "iekQga58wybC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100 | Train loss: 95.670495 | Train accuracy: 0.84489447 | Test loss: 96.313071 | Test accuracy: 0.8433017]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[2/100 | Train loss: 90.243038 | Train accuracy: 0.85618425 | Test loss: 91.660623 | Test accuracy: 0.85304648]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[3/100 | Train loss: 90.800795 | Train accuracy: 0.852726 | Test loss: 91.693725 | Test accuracy: 0.85063497]\n",
      "Trigger times: 1/5\n",
      "[4/100 | Train loss: 84.122516 | Train accuracy: 0.86897875 | Test loss: 85.689398 | Test accuracy: 0.86491469]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[5/100 | Train loss: 82.10752 | Train accuracy: 0.87277977 | Test loss: 83.818487 | Test accuracy: 0.8696307]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[6/100 | Train loss: 81.30915 | Train accuracy: 0.87395624 | Test loss: 82.986915 | Test accuracy: 0.87082411]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[7/100 | Train loss: 79.907329 | Train accuracy: 0.87778051 | Test loss: 82.155314 | Test accuracy: 0.87348252]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[8/100 | Train loss: 79.185129 | Train accuracy: 0.87915741 | Test loss: 81.109956 | Test accuracy: 0.87489815]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[9/100 | Train loss: 77.573065 | Train accuracy: 0.88242975 | Test loss: 79.709979 | Test accuracy: 0.87767179]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[10/100 | Train loss: 77.549071 | Train accuracy: 0.88355393 | Test loss: 79.654351 | Test accuracy: 0.87885696]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[11/100 | Train loss: 76.799457 | Train accuracy: 0.885201 | Test loss: 79.13216 | Test accuracy: 0.88100509]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[12/100 | Train loss: 74.941328 | Train accuracy: 0.8889294 | Test loss: 77.025222 | Test accuracy: 0.88416556]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[13/100 | Train loss: 74.807331 | Train accuracy: 0.88890761 | Test loss: 77.214791 | Test accuracy: 0.88332606]\n",
      "Trigger times: 1/5\n",
      "[14/100 | Train loss: 74.518053 | Train accuracy: 0.88977617 | Test loss: 76.983635 | Test accuracy: 0.88494745]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[15/100 | Train loss: 74.39701 | Train accuracy: 0.88879577 | Test loss: 76.869058 | Test accuracy: 0.8838775]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[16/100 | Train loss: 74.638595 | Train accuracy: 0.89053143 | Test loss: 76.822326 | Test accuracy: 0.88570464]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[17/100 | Train loss: 74.830292 | Train accuracy: 0.89037021 | Test loss: 76.831854 | Test accuracy: 0.88564703]\n",
      "Trigger times: 1/5\n",
      "[18/100 | Train loss: 73.055656 | Train accuracy: 0.89242541 | Test loss: 75.715836 | Test accuracy: 0.88716142]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[19/100 | Train loss: 72.347949 | Train accuracy: 0.89449658 | Test loss: 74.890465 | Test accuracy: 0.88914495]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[20/100 | Train loss: 72.49951 | Train accuracy: 0.89306884 | Test loss: 75.360342 | Test accuracy: 0.88716965]\n",
      "Trigger times: 1/5\n",
      "[21/100 | Train loss: 71.907923 | Train accuracy: 0.89456775 | Test loss: 74.691564 | Test accuracy: 0.88926017]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[22/100 | Train loss: 72.695633 | Train accuracy: 0.89236005 | Test loss: 75.684629 | Test accuracy: 0.88680752]\n",
      "Trigger times: 1/5\n",
      "[23/100 | Train loss: 72.375319 | Train accuracy: 0.89363528 | Test loss: 75.121925 | Test accuracy: 0.88813261]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[24/100 | Train loss: 72.252572 | Train accuracy: 0.89377762 | Test loss: 75.211598 | Test accuracy: 0.88819022]\n",
      "Trigger times: 1/5\n",
      "[25/100 | Train loss: 73.106245 | Train accuracy: 0.89201292 | Test loss: 76.094933 | Test accuracy: 0.88651945]\n",
      "Trigger times: 2/5\n",
      "[26/100 | Train loss: 72.061649 | Train accuracy: 0.89469411 | Test loss: 74.728928 | Test accuracy: 0.88969638]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[27/100 | Train loss: 71.182668 | Train accuracy: 0.89544792 | Test loss: 74.172136 | Test accuracy: 0.88931778]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[28/100 | Train loss: 70.977616 | Train accuracy: 0.89663602 | Test loss: 73.923733 | Test accuracy: 0.89074164]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[29/100 | Train loss: 70.711878 | Train accuracy: 0.89657066 | Test loss: 74.135815 | Test accuracy: 0.89099678]\n",
      "Trigger times: 1/5\n",
      "[30/100 | Train loss: 70.975733 | Train accuracy: 0.89590254 | Test loss: 74.150021 | Test accuracy: 0.88943301]\n",
      "Trigger times: 2/5\n",
      "[31/100 | Train loss: 70.652475 | Train accuracy: 0.89689455 | Test loss: 74.242267 | Test accuracy: 0.89097209]\n",
      "Trigger times: 3/5\n",
      "[32/100 | Train loss: 72.326944 | Train accuracy: 0.89326491 | Test loss: 75.620601 | Test accuracy: 0.88768817]\n",
      "Trigger times: 4/5\n",
      "[33/100 | Train loss: 70.436759 | Train accuracy: 0.89670864 | Test loss: 73.768474 | Test accuracy: 0.89046181]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[34/100 | Train loss: 71.044896 | Train accuracy: 0.89581684 | Test loss: 74.444702 | Test accuracy: 0.88958116]\n",
      "Trigger times: 1/5\n",
      "[35/100 | Train loss: 70.206544 | Train accuracy: 0.89770936 | Test loss: 73.677479 | Test accuracy: 0.89120254]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[36/100 | Train loss: 70.343372 | Train accuracy: 0.89774568 | Test loss: 73.988383 | Test accuracy: 0.89209142]\n",
      "Trigger times: 1/5\n",
      "[37/100 | Train loss: 70.291811 | Train accuracy: 0.89753072 | Test loss: 74.102174 | Test accuracy: 0.89124369]\n",
      "Trigger times: 2/5\n",
      "[38/100 | Train loss: 70.215558 | Train accuracy: 0.89785025 | Test loss: 74.039441 | Test accuracy: 0.89195151]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[39/100 | Train loss: 69.028109 | Train accuracy: 0.899965 | Test loss: 72.982739 | Test accuracy: 0.89256056]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[40/100 | Train loss: 69.879536 | Train accuracy: 0.89907756 | Test loss: 74.088384 | Test accuracy: 0.89279101]\n",
      "Trigger times: 1/5\n",
      "[41/100 | Train loss: 70.277507 | Train accuracy: 0.89694538 | Test loss: 74.268235 | Test accuracy: 0.89071695]\n",
      "Trigger times: 2/5\n",
      "[42/100 | Train loss: 69.554928 | Train accuracy: 0.89921699 | Test loss: 73.581045 | Test accuracy: 0.89196797]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[43/100 | Train loss: 70.189983 | Train accuracy: 0.89795773 | Test loss: 74.148596 | Test accuracy: 0.89100501]\n",
      "Trigger times: 1/5\n",
      "[44/100 | Train loss: 69.954383 | Train accuracy: 0.89787494 | Test loss: 74.006192 | Test accuracy: 0.89149061]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[45/100 | Train loss: 69.674715 | Train accuracy: 0.89870138 | Test loss: 73.348706 | Test accuracy: 0.8920585]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[46/100 | Train loss: 69.422554 | Train accuracy: 0.89889746 | Test loss: 73.360306 | Test accuracy: 0.89200089]\n",
      "Trigger times: 1/5\n",
      "[47/100 | Train loss: 69.111532 | Train accuracy: 0.89955831 | Test loss: 73.316124 | Test accuracy: 0.89367989]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[48/100 | Train loss: 70.823057 | Train accuracy: 0.89614074 | Test loss: 74.405331 | Test accuracy: 0.89034658]\n",
      "Trigger times: 1/5\n",
      "[49/100 | Train loss: 68.852849 | Train accuracy: 0.90008555 | Test loss: 73.214494 | Test accuracy: 0.89297207]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[50/100 | Train loss: 69.936648 | Train accuracy: 0.89820465 | Test loss: 73.593638 | Test accuracy: 0.89195974]\n",
      "Trigger times: 1/5\n",
      "[51/100 | Train loss: 69.716243 | Train accuracy: 0.89871445 | Test loss: 73.748134 | Test accuracy: 0.8914083]\n",
      "Trigger times: 2/5\n",
      "[52/100 | Train loss: 69.432517 | Train accuracy: 0.89958301 | Test loss: 73.382075 | Test accuracy: 0.89240418]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[53/100 | Train loss: 68.738424 | Train accuracy: 0.90052128 | Test loss: 72.886599 | Test accuracy: 0.89358935]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[54/100 | Train loss: 69.047653 | Train accuracy: 0.89973406 | Test loss: 73.704836 | Test accuracy: 0.89223957]\n",
      "Trigger times: 1/5\n",
      "[55/100 | Train loss: 69.440802 | Train accuracy: 0.89893812 | Test loss: 73.514649 | Test accuracy: 0.89173752]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[56/100 | Train loss: 68.772304 | Train accuracy: 0.89956122 | Test loss: 73.203981 | Test accuracy: 0.89293092]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[57/100 | Train loss: 68.67259 | Train accuracy: 0.90013929 | Test loss: 73.107702 | Test accuracy: 0.89293915]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[58/100 | Train loss: 70.105984 | Train accuracy: 0.89801728 | Test loss: 74.502219 | Test accuracy: 0.89085687]\n",
      "Trigger times: 1/5\n",
      "[59/100 | Train loss: 68.362886 | Train accuracy: 0.9010703 | Test loss: 73.406532 | Test accuracy: 0.89342475]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[60/100 | Train loss: 69.582004 | Train accuracy: 0.89821045 | Test loss: 74.160249 | Test accuracy: 0.89098032]\n",
      "Trigger times: 1/5\n",
      "[61/100 | Train loss: 68.423654 | Train accuracy: 0.9005024 | Test loss: 73.290487 | Test accuracy: 0.89237949]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[62/100 | Train loss: 68.54558 | Train accuracy: 0.90068105 | Test loss: 73.074678 | Test accuracy: 0.8927087]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[63/100 | Train loss: 71.489187 | Train accuracy: 0.89570646 | Test loss: 75.755342 | Test accuracy: 0.88791862]\n",
      "Trigger times: 1/5\n",
      "[64/100 | Train loss: 68.101199 | Train accuracy: 0.90126928 | Test loss: 72.623424 | Test accuracy: 0.89504613]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[65/100 | Train loss: 68.56879 | Train accuracy: 0.90027001 | Test loss: 73.394817 | Test accuracy: 0.89279924]\n",
      "Trigger times: 1/5\n",
      "[66/100 | Train loss: 67.963386 | Train accuracy: 0.90128235 | Test loss: 73.184342 | Test accuracy: 0.89337536]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[67/100 | Train loss: 69.617204 | Train accuracy: 0.89788947 | Test loss: 74.791557 | Test accuracy: 0.88958939]\n",
      "Trigger times: 1/5\n",
      "[68/100 | Train loss: 69.020443 | Train accuracy: 0.89879869 | Test loss: 73.945285 | Test accuracy: 0.89177867]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[69/100 | Train loss: 67.841008 | Train accuracy: 0.90153508 | Test loss: 72.97592 | Test accuracy: 0.89415725]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[70/100 | Train loss: 68.215929 | Train accuracy: 0.90127364 | Test loss: 73.047979 | Test accuracy: 0.89358935]\n",
      "Trigger times: 1/5\n",
      "[71/100 | Train loss: 67.929648 | Train accuracy: 0.9014 | Test loss: 73.098889 | Test accuracy: 0.89315314]\n",
      "Trigger times: 2/5\n",
      "[72/100 | Train loss: 68.088821 | Train accuracy: 0.90121119 | Test loss: 73.434715 | Test accuracy: 0.89273339]\n",
      "Trigger times: 3/5\n",
      "[73/100 | Train loss: 67.795709 | Train accuracy: 0.90205505 | Test loss: 73.361527 | Test accuracy: 0.89425601]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[74/100 | Train loss: 68.643275 | Train accuracy: 0.90029615 | Test loss: 73.966279 | Test accuracy: 0.8920585]\n",
      "Trigger times: 1/5\n",
      "[75/100 | Train loss: 67.782907 | Train accuracy: 0.90175585 | Test loss: 73.095626 | Test accuracy: 0.89340005]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[76/100 | Train loss: 68.10817 | Train accuracy: 0.90090908 | Test loss: 73.327963 | Test accuracy: 0.89279101]\n",
      "Trigger times: 1/5\n",
      "[77/100 | Train loss: 67.674922 | Train accuracy: 0.90205215 | Test loss: 73.16222 | Test accuracy: 0.89369635]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[78/100 | Train loss: 67.421493 | Train accuracy: 0.90222353 | Test loss: 72.688603 | Test accuracy: 0.89432186]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[79/100 | Train loss: 68.145212 | Train accuracy: 0.9022032 | Test loss: 73.174913 | Test accuracy: 0.89423132]\n",
      "Trigger times: 1/5\n",
      "[80/100 | Train loss: 67.84292 | Train accuracy: 0.90157284 | Test loss: 73.127796 | Test accuracy: 0.89368812]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[81/100 | Train loss: 67.708439 | Train accuracy: 0.90155832 | Test loss: 73.313854 | Test accuracy: 0.89330129]\n",
      "Trigger times: 1/5\n",
      "[82/100 | Train loss: 67.65943 | Train accuracy: 0.90228308 | Test loss: 73.015828 | Test accuracy: 0.89364697]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[83/100 | Train loss: 67.272556 | Train accuracy: 0.90255469 | Test loss: 73.081068 | Test accuracy: 0.89440416]\n",
      "Trigger times: 1/5\n",
      "[84/100 | Train loss: 67.70215 | Train accuracy: 0.90171228 | Test loss: 73.301179 | Test accuracy: 0.89330129]\n",
      "Trigger times: 2/5\n",
      "[85/100 | Train loss: 67.375234 | Train accuracy: 0.90254016 | Test loss: 73.099982 | Test accuracy: 0.89410787]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[86/100 | Train loss: 67.655085 | Train accuracy: 0.90218141 | Test loss: 72.988448 | Test accuracy: 0.89416548]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[87/100 | Train loss: 67.493061 | Train accuracy: 0.90211896 | Test loss: 73.584967 | Test accuracy: 0.89397618]\n",
      "Trigger times: 1/5\n",
      "[88/100 | Train loss: 67.677824 | Train accuracy: 0.90202019 | Test loss: 73.409748 | Test accuracy: 0.89338359]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[89/100 | Train loss: 67.15761 | Train accuracy: 0.90278853 | Test loss: 73.23639 | Test accuracy: 0.89451939]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[90/100 | Train loss: 67.615193 | Train accuracy: 0.90174132 | Test loss: 73.464302 | Test accuracy: 0.89255232]\n",
      "Trigger times: 1/5\n",
      "[91/100 | Train loss: 66.749144 | Train accuracy: 0.90386914 | Test loss: 72.672956 | Test accuracy: 0.89530127]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[92/100 | Train loss: 67.839943 | Train accuracy: 0.90142615 | Test loss: 73.924166 | Test accuracy: 0.89311199]\n",
      "Trigger times: 1/5\n",
      "[93/100 | Train loss: 67.299796 | Train accuracy: 0.90295265 | Test loss: 73.26431 | Test accuracy: 0.89417371]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[94/100 | Train loss: 67.369172 | Train accuracy: 0.90248933 | Test loss: 73.616163 | Test accuracy: 0.89318606]\n",
      "Trigger times: 1/5\n",
      "[95/100 | Train loss: 66.941456 | Train accuracy: 0.90344794 | Test loss: 72.775782 | Test accuracy: 0.89425601]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[96/100 | Train loss: 66.811262 | Train accuracy: 0.90334046 | Test loss: 73.200968 | Test accuracy: 0.89390211]\n",
      "Trigger times: 1/5\n",
      "[97/100 | Train loss: 67.34822 | Train accuracy: 0.90231068 | Test loss: 73.642109 | Test accuracy: 0.89313668]\n",
      "Trigger times: 2/5\n",
      "[98/100 | Train loss: 66.930298 | Train accuracy: 0.90333465 | Test loss: 73.024509 | Test accuracy: 0.89424778]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[99/100 | Train loss: 66.606628 | Train accuracy: 0.90387059 | Test loss: 73.271592 | Test accuracy: 0.89456054]\n",
      "Trigger times: 1/5\n",
      "[100/100 | Train loss: 66.960379 | Train accuracy: 0.90350313 | Test loss: 73.659675 | Test accuracy: 0.89363874]\n",
      "Trigger times: 2/5\n"
     ]
    }
   ],
   "source": [
    "wanna_train = False\n",
    "if wanna_train:\n",
    "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v0_path)\n",
    "else:\n",
    "    print('Not training this model anymore...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "j44nOD2HwyYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying last checkpoint of model NetV0 state to trained folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('../Trained/NetV0.pth')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "v0_path_trained = Path('../Trained/NetV0.pth')\n",
    "if v0_path.exists():\n",
    "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV0'))\n",
    "    shutil.copy(v0_path, v0_path_trained)\n",
    "else:\n",
    "    print('Model {0} is not created yeat'.format('NetV0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (7): Tanh()\n",
       "    (8): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (9): Tanh()\n",
       "    (10): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (11): Tanh()\n",
       "    (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (13): Tanh()\n",
       "    (14): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (15): Tanh()\n",
       "    (16): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (17): Tanh()\n",
       "    (18): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (19): Tanh()\n",
       "    (20): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (21): Tanh()\n",
       "    (22): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (90000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>209540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>182173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>489479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  target\n",
       "0  209540       0\n",
       "1  182173       0\n",
       "2  195991       1\n",
       "3   43111       0\n",
       "4  489479       1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v0_path_trained = Path('../Trained/NetV0.pth')\n",
    "v0_state = torch.load(v0_path_trained)\n",
    "v0_state_dict = v0_state['model_state_dict']\n",
    "v0_model = Net()\n",
    "v0_model.load_state_dict(v0_state_dict)\n",
    "\n",
    "validate_path = Path('../Data/validate.csv')\n",
    "validate_df = pd.read_csv(validate_path)\n",
    "X_val = validate_df.drop(['id', 'f_27'], axis=1)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val_tensor = torch.from_numpy(X_val)\n",
    "X_val_tensor = X_val_tensor.type(torch.float)\n",
    "X_val_tensor = X_val_tensor.to(device)\n",
    "\n",
    "v0_model = v0_model.to(device)\n",
    "v0_model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = v0_model(X_val_tensor)\n",
    "    y_pred = torch.round(torch.sigmoid(outputs))\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    print(type(y_pred), y_pred.shape)\n",
    "    \n",
    "submision_df = pd.DataFrame({\n",
    "    'id': validate_df['id'],\n",
    "    'target': y_pred.flatten()\n",
    "})\n",
    "\n",
    "submision_df = submision_df.astype('int')\n",
    "\n",
    "submit_path_model_v0 = Path('../Submit/V0.csv')\n",
    "submision_df.to_csv(submit_path_model_v0, index=False)\n",
    "submision_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode f_27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the way to go because the dataframe increases a lot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 31), (810000,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(678113, 810000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_27 = X['f_27']\n",
    "f_27.nunique(), len(f_27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 678112), scipy.sparse._csr.csr_matrix)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X['f_27'] = X['f_27'].astype('category')\n",
    "drop_enc = OneHotEncoder(drop='first')\n",
    "sparse = drop_enc.fit_transform(X[['f_27']])\n",
    "sparse.shape, type(sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimentionally reduction doesn't work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TruncatedSVD(n_components=10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.46741814e-05 1.32668698e-05 1.17623774e-05 1.17496912e-05\n",
      " 1.03026316e-05 1.02071320e-05 9.98196498e-06 9.45805114e-06\n",
      " 9.19224881e-06 8.83016400e-06]\n",
      "0.00010942531230727259\n",
      "[3.43478639 3.25197463 3.0571312  3.04839654 2.85091836 2.83100822\n",
      " 2.79509639 2.71544367 2.68575579 2.63845535]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "pca = TruncatedSVD(n_components=10)\n",
    "pca.fit(sparse)\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "print(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusterization doesnt perform good either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements asigned to each cluster: [810000]\n",
      "Number of elements asigned to each cluster: [809996      4]\n",
      "Number of elements asigned to each cluster: [809994      4      2]\n",
      "Number of elements asigned to each cluster: [809993      4      2      1]\n",
      "Number of elements asigned to each cluster: [809990      4      2      1      3]\n",
      "Number of elements asigned to each cluster: [809988      4      2      1      3      2]\n",
      "Number of elements asigned to each cluster: [809984      4      2      1      3      2      4]\n",
      "Number of elements asigned to each cluster: [809980      4      2      3      2      4      3      2]\n",
      "Number of elements asigned to each cluster: [809979      4      2      3      2      4      3      2      1]\n",
      "Number of elements asigned to each cluster: [809978      4      2      3      2      4      3      2      1      1]\n",
      "\n",
      "True number of documents in each category according to the class labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for c in range(1, 11):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=c,\n",
    "        max_iter=300,\n",
    "        n_init='auto',\n",
    "        random_state=42,\n",
    "    ).fit(sparse)\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "    print(f\"Number of elements asigned to each cluster: {cluster_sizes}\")\n",
    "print()\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{cluster_ids}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart encoding f_27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "df['f_27'] = df['f_27'].str.upper()\n",
    "df['length'] = df['f_27'].str.len()\n",
    "print(df['length'].min(), df['length'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810000, 32), (810000,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop(['target', 'id'], axis=1, errors='ignore'), df['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>length</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>pos_0</th>\n",
       "      <th>pos_1</th>\n",
       "      <th>pos_2</th>\n",
       "      <th>pos_3</th>\n",
       "      <th>pos_4</th>\n",
       "      <th>pos_5</th>\n",
       "      <th>pos_6</th>\n",
       "      <th>pos_7</th>\n",
       "      <th>pos_8</th>\n",
       "      <th>pos_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>-0.183690</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>-1.186794</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.946208</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.690715</td>\n",
       "      <td>-0.628005</td>\n",
       "      <td>-2.832295</td>\n",
       "      <td>-1.409039</td>\n",
       "      <td>3.645067</td>\n",
       "      <td>0.233039</td>\n",
       "      <td>-3.754846</td>\n",
       "      <td>-1.061733</td>\n",
       "      <td>BDBBCACIBB</td>\n",
       "      <td>20.308715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.286392</td>\n",
       "      <td>1.780592</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>-2.690658</td>\n",
       "      <td>1.321997</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>0.371070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664517</td>\n",
       "      <td>-2.871912</td>\n",
       "      <td>3.826628</td>\n",
       "      <td>3.087653</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>3.210875</td>\n",
       "      <td>-0.666457</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>ACBDCBCADA</td>\n",
       "      <td>-449.291063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.290303</td>\n",
       "      <td>-0.485907</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>1.083632</td>\n",
       "      <td>-1.129914</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.494988</td>\n",
       "      <td>-0.946303</td>\n",
       "      <td>2.333223</td>\n",
       "      <td>2.084169</td>\n",
       "      <td>-4.782668</td>\n",
       "      <td>-1.671375</td>\n",
       "      <td>2.774382</td>\n",
       "      <td>2.273130</td>\n",
       "      <td>AABBABCLAF</td>\n",
       "      <td>-86.206118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.243590</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.854267</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>-2.020416</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.066427</td>\n",
       "      <td>-2.430158</td>\n",
       "      <td>-0.185332</td>\n",
       "      <td>-0.701691</td>\n",
       "      <td>-2.769142</td>\n",
       "      <td>-6.534231</td>\n",
       "      <td>-0.557677</td>\n",
       "      <td>-0.429972</td>\n",
       "      <td>ADBBABEEBA</td>\n",
       "      <td>-30.157403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-1.045483</td>\n",
       "      <td>0.718374</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>-0.936620</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.899984</td>\n",
       "      <td>1.427460</td>\n",
       "      <td>-4.992610</td>\n",
       "      <td>1.154162</td>\n",
       "      <td>-1.931443</td>\n",
       "      <td>2.325042</td>\n",
       "      <td>2.143811</td>\n",
       "      <td>-1.039599</td>\n",
       "      <td>ABBBBBCMBB</td>\n",
       "      <td>296.484562</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
       "0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
       "1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
       "2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
       "3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
       "4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
       "\n",
       "   f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
       "0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
       "1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
       "2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
       "3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
       "4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
       "\n",
       "       f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
       "0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
       "1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
       "2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
       "3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
       "4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
       "\n",
       "         f_27        f_28  f_29  f_30  length  A  B  C  D  E  F  G  H  I  J  \\\n",
       "0  BDBBCACIBB   20.308715     1     0      10  1  5  2  1  0  0  0  0  1  0   \n",
       "1  ACBDCBCADA -449.291063     1     0      10  3  2  3  2  0  0  0  0  0  0   \n",
       "2  AABBABCLAF  -86.206118     0     1      10  4  3  1  0  0  1  0  0  0  0   \n",
       "3  ADBBABEEBA  -30.157403     0     2      10  3  4  0  1  2  0  0  0  0  0   \n",
       "4  ABBBBBCMBB  296.484562     0     2      10  1  7  1  0  0  0  0  0  0  0   \n",
       "\n",
       "   K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y  Z  pos_0  pos_1  pos_2  pos_3  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      1      3      1      1   \n",
       "1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      2      1      3   \n",
       "2  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      0      1      1   \n",
       "3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0      0      3      1      1   \n",
       "4  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0      0      1      1      1   \n",
       "\n",
       "   pos_4  pos_5  pos_6  pos_7  pos_8  pos_9  \n",
       "0      2      0      2      8      1      1  \n",
       "1      2      1      2      0      3      0  \n",
       "2      0      1      2     11      0      5  \n",
       "3      0      1      4      4      1      0  \n",
       "4      1      1      2     12      1      1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts the number of times a letter appear in the code\n",
    "def add_letters_count(data):\n",
    "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for char in letters:\n",
    "        data[char] = data['f_27'].str.count(char)\n",
    "    return data\n",
    "\n",
    "# For each of the 10 position of the letter code I assign which code letter was assigned to it\n",
    "def add_letter_position(data):\n",
    "    for i in range(10):\n",
    "        data['pos_' + str(i)] = (data['f_27'].str[i]).apply(lambda x: ord(x)) - 65\n",
    "    return data\n",
    "\n",
    "X = add_letters_count(X)\n",
    "X = add_letter_position(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>f_09</th>\n",
       "      <th>f_10</th>\n",
       "      <th>f_11</th>\n",
       "      <th>f_12</th>\n",
       "      <th>f_13</th>\n",
       "      <th>f_14</th>\n",
       "      <th>f_15</th>\n",
       "      <th>f_16</th>\n",
       "      <th>f_17</th>\n",
       "      <th>f_18</th>\n",
       "      <th>f_19</th>\n",
       "      <th>f_20</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>pos_0</th>\n",
       "      <th>pos_1</th>\n",
       "      <th>pos_2</th>\n",
       "      <th>pos_3</th>\n",
       "      <th>pos_4</th>\n",
       "      <th>pos_5</th>\n",
       "      <th>pos_6</th>\n",
       "      <th>pos_7</th>\n",
       "      <th>pos_8</th>\n",
       "      <th>pos_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>-0.183690</td>\n",
       "      <td>0.664383</td>\n",
       "      <td>-1.186794</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.946208</td>\n",
       "      <td>0.729857</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.690715</td>\n",
       "      <td>-0.628005</td>\n",
       "      <td>-2.832295</td>\n",
       "      <td>-1.409039</td>\n",
       "      <td>3.645067</td>\n",
       "      <td>0.233039</td>\n",
       "      <td>-3.754846</td>\n",
       "      <td>-1.061733</td>\n",
       "      <td>20.308715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.286392</td>\n",
       "      <td>1.780592</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>-2.690658</td>\n",
       "      <td>1.321997</td>\n",
       "      <td>-0.675894</td>\n",
       "      <td>0.371070</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664517</td>\n",
       "      <td>-2.871912</td>\n",
       "      <td>3.826628</td>\n",
       "      <td>3.087653</td>\n",
       "      <td>0.494209</td>\n",
       "      <td>3.210875</td>\n",
       "      <td>-0.666457</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>-449.291063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.290303</td>\n",
       "      <td>-0.485907</td>\n",
       "      <td>0.808350</td>\n",
       "      <td>-0.156288</td>\n",
       "      <td>1.083632</td>\n",
       "      <td>-1.129914</td>\n",
       "      <td>0.767396</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.494988</td>\n",
       "      <td>-0.946303</td>\n",
       "      <td>2.333223</td>\n",
       "      <td>2.084169</td>\n",
       "      <td>-4.782668</td>\n",
       "      <td>-1.671375</td>\n",
       "      <td>2.774382</td>\n",
       "      <td>2.273130</td>\n",
       "      <td>-86.206118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.243590</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>-1.013236</td>\n",
       "      <td>0.854267</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>0.597892</td>\n",
       "      <td>-2.020416</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.066427</td>\n",
       "      <td>-2.430158</td>\n",
       "      <td>-0.185332</td>\n",
       "      <td>-0.701691</td>\n",
       "      <td>-2.769142</td>\n",
       "      <td>-6.534231</td>\n",
       "      <td>-0.557677</td>\n",
       "      <td>-0.429972</td>\n",
       "      <td>-30.157403</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.702716</td>\n",
       "      <td>0.817044</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-1.045483</td>\n",
       "      <td>0.718374</td>\n",
       "      <td>0.164451</td>\n",
       "      <td>-0.936620</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.899984</td>\n",
       "      <td>1.427460</td>\n",
       "      <td>-4.992610</td>\n",
       "      <td>1.154162</td>\n",
       "      <td>-1.931443</td>\n",
       "      <td>2.325042</td>\n",
       "      <td>2.143811</td>\n",
       "      <td>-1.039599</td>\n",
       "      <td>296.484562</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       f_00      f_01      f_02      f_03      f_04      f_05      f_06  f_07  \\\n",
       "0  0.315471 -0.183690  0.664383 -1.186794  0.665098  0.946208  0.729857     0   \n",
       "1 -1.286392  1.780592  0.576698 -2.690658  1.321997 -0.675894  0.371070     3   \n",
       "2 -0.290303 -0.485907  0.808350 -0.156288  1.083632 -1.129914  0.767396     3   \n",
       "3  1.243590  0.035112 -1.013236  0.854267  0.019192  0.597892 -2.020416     2   \n",
       "4  0.702716  0.817044 -0.064907 -1.045483  0.718374  0.164451 -0.936620     1   \n",
       "\n",
       "   f_08  f_09  f_10  f_11  f_12  f_13  f_14  f_15  f_16  f_17  f_18      f_19  \\\n",
       "0     4     1     3     1     2     4     1     5     2     0     1 -3.690715   \n",
       "1     0     3     3     5     3     2     0     1     6     0     1  0.664517   \n",
       "2     1     3     2     3     4     1     1     1     0     2     6 -0.494988   \n",
       "3     0     4     5     0     5     1     0     3     1     1     3 -3.066427   \n",
       "4     2     2     2     2     5     0     3     1     1     2     4 -1.899984   \n",
       "\n",
       "       f_20      f_21      f_22      f_23      f_24      f_25      f_26  \\\n",
       "0 -0.628005 -2.832295 -1.409039  3.645067  0.233039 -3.754846 -1.061733   \n",
       "1 -2.871912  3.826628  3.087653  0.494209  3.210875 -0.666457  0.123854   \n",
       "2 -0.946303  2.333223  2.084169 -4.782668 -1.671375  2.774382  2.273130   \n",
       "3 -2.430158 -0.185332 -0.701691 -2.769142 -6.534231 -0.557677 -0.429972   \n",
       "4  1.427460 -4.992610  1.154162 -1.931443  2.325042  2.143811 -1.039599   \n",
       "\n",
       "         f_28  f_29  f_30  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  \\\n",
       "0   20.308715     1     0  1  5  2  1  0  0  0  0  1  0  0  0  0  0  0  0  0   \n",
       "1 -449.291063     1     0  3  2  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2  -86.206118     0     1  4  3  1  0  0  1  0  0  0  0  0  1  0  0  0  0  0   \n",
       "3  -30.157403     0     2  3  4  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "4  296.484562     0     2  1  7  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0   \n",
       "\n",
       "   R  S  T  U  V  W  X  Y  Z  pos_0  pos_1  pos_2  pos_3  pos_4  pos_5  pos_6  \\\n",
       "0  0  0  0  0  0  0  0  0  0      1      3      1      1      2      0      2   \n",
       "1  0  0  0  0  0  0  0  0  0      0      2      1      3      2      1      2   \n",
       "2  0  0  0  0  0  0  0  0  0      0      0      1      1      0      1      2   \n",
       "3  0  0  0  0  0  0  0  0  0      0      3      1      1      0      1      4   \n",
       "4  0  0  0  0  0  0  0  0  0      0      1      1      1      1      1      2   \n",
       "\n",
       "   pos_7  pos_8  pos_9  \n",
       "0      8      1      1  \n",
       "1      0      3      0  \n",
       "2     11      0      5  \n",
       "3      4      1      0  \n",
       "4     12      1      1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drops unneded features\n",
    "X = X.drop(['f_27', 'length'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(data=scaler.fit_transform(X), columns=X.columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3155, -0.1837,  0.6644,  ...,  8.0000,  1.0000,  1.0000],\n",
       "        [-1.2864,  1.7806,  0.5767,  ...,  0.0000,  3.0000,  0.0000],\n",
       "        [-0.2903, -0.4859,  0.8084,  ..., 11.0000,  0.0000,  5.0000],\n",
       "        ...,\n",
       "        [-0.2353, -0.3175, -0.3885,  ..., 11.0000,  3.0000,  3.0000],\n",
       "        [-0.9948, -0.2667,  1.9090,  ..., 13.0000,  1.0000,  2.0000],\n",
       "        [-0.2540,  1.2699,  1.9680,  ..., 17.0000,  1.0000,  2.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the hole dataset\n",
    "X_tensor, y_tensor = torch.from_numpy(X.to_numpy()), torch.from_numpy(y.to_numpy())\n",
    "X_tensor, y_tensor = X_tensor.type(torch.float), y_tensor.type(torch.float)\n",
    "X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
    "X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "7QqrCvTnwyT_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(810000, 66) (810000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5002, 0.5000, 0.5003,  ..., 0.5033, 0.5004, 0.5004],\n",
       "        [0.4995, 0.5008, 0.5003,  ..., 0.5000, 0.5013, 0.5000],\n",
       "        [0.4999, 0.4998, 0.5004,  ..., 0.5045, 0.5000, 0.5021],\n",
       "        ...,\n",
       "        [0.4999, 0.4999, 0.4999,  ..., 0.5045, 0.5013, 0.5013],\n",
       "        [0.4996, 0.4999, 0.5008,  ..., 0.5053, 0.5004, 0.5009],\n",
       "        [0.4999, 0.5006, 0.5008,  ..., 0.5070, 0.5004, 0.5009]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor = (X_tensor - X_tensor.min()) / (X_tensor.max() - X_tensor.min())\n",
    "print(X.shape, y.shape)\n",
    "X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "fDII01Oiwx6m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for new training process... =)\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
    "\n",
    "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = Net(input_features=X_tensor.shape[1])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "resume = 1\n",
    "v1_path = Path('../Output/NetV1.pth')\n",
    "\n",
    "recover = False\n",
    "\n",
    "if v_path.exists() and recover:\n",
    "    print('Recovering old training process...')\n",
    "    checkpoint = torch.load(v1_path)\n",
    "    resume = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criteria = checkpoint['criteria']\n",
    "else:\n",
    "    print('Ready for new training process... =)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100 | Train loss: 177.31987 | Train accuracy: 0.51351273 | Test loss: 177.21183 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[2/100 | Train loss: 177.32319 | Train accuracy: 0.51351273 | Test loss: 177.21514 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2\n",
      "[3/100 | Train loss: 177.31651 | Train accuracy: 0.51351273 | Test loss: 177.20847 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[4/100 | Train loss: 177.32002 | Train accuracy: 0.51351273 | Test loss: 177.21197 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[5/100 | Train loss: 177.31729 | Train accuracy: 0.51351273 | Test loss: 177.20925 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[6/100 | Train loss: 177.31726 | Train accuracy: 0.51351273 | Test loss: 177.20922 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[7/100 | Train loss: 177.31633 | Train accuracy: 0.51351273 | Test loss: 177.20829 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[8/100 | Train loss: 177.31587 | Train accuracy: 0.51351273 | Test loss: 177.20783 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[9/100 | Train loss: 177.32375 | Train accuracy: 0.51351273 | Test loss: 177.21571 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[10/100 | Train loss: 177.3177 | Train accuracy: 0.51351273 | Test loss: 177.20965 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[11/100 | Train loss: 177.31779 | Train accuracy: 0.51351273 | Test loss: 177.20975 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[12/100 | Train loss: 177.31622 | Train accuracy: 0.51351273 | Test loss: 177.20818 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[13/100 | Train loss: 177.31629 | Train accuracy: 0.51351273 | Test loss: 177.20825 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[14/100 | Train loss: 177.31588 | Train accuracy: 0.51351273 | Test loss: 177.20784 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[15/100 | Train loss: 177.31947 | Train accuracy: 0.51351273 | Test loss: 177.21144 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[16/100 | Train loss: 177.31713 | Train accuracy: 0.51351273 | Test loss: 177.20908 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[17/100 | Train loss: 177.32008 | Train accuracy: 0.51351273 | Test loss: 177.21204 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[18/100 | Train loss: 177.31691 | Train accuracy: 0.51351273 | Test loss: 177.20888 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[19/100 | Train loss: 177.32802 | Train accuracy: 0.51351273 | Test loss: 177.21998 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[20/100 | Train loss: 177.31586 | Train accuracy: 0.51351273 | Test loss: 177.20782 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[21/100 | Train loss: 177.31748 | Train accuracy: 0.51351273 | Test loss: 177.20943 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[22/100 | Train loss: 177.31642 | Train accuracy: 0.51351273 | Test loss: 177.20838 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[23/100 | Train loss: 177.31605 | Train accuracy: 0.51351273 | Test loss: 177.20801 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0\n",
      "New checkpoint...\n",
      "[24/100 | Train loss: 177.3161 | Train accuracy: 0.51351273 | Test loss: 177.20806 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1\n",
      "[25/100 | Train loss: 177.31674 | Train accuracy: 0.51351273 | Test loss: 177.20871 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2\n",
      "[26/100 | Train loss: 177.31881 | Train accuracy: 0.51351273 | Test loss: 177.21078 | Test accuracy: 0.51351018]\n",
      "Trigger times: 3\n",
      "Early stopping!\n",
      "Start to test process.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Net object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m wanna_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wanna_train:\n\u001b[1;32m----> 3\u001b[0m     trained_model, train_stats, test_stats \u001b[38;5;241m=\u001b[39m train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume\u001b[38;5;241m=\u001b[39mresume, save_as\u001b[38;5;241m=\u001b[39mv1_path)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot training this model anymore...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable Net object"
     ]
    }
   ],
   "source": [
    "wanna_train = False\n",
    "if wanna_train: # Doen't learn with the previous model\n",
    "    trained_model, (train_stats, test_stats) = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v1_path)\n",
    "else:\n",
    "    print('Not training this model anymore...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, input_features=30):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=256), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for new training process... =)\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(data=X_tensor, labels=y_tensor)\n",
    "\n",
    "train_dataset, test_dataset = datasets(dataset, y, test_size=0.15)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = NetV2(input_features=X_tensor.shape[1])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criteria = nn.BCEWithLogitsLoss()\n",
    "resume = 1\n",
    "v1_path = Path('../Output/NetV1.pth')\n",
    "\n",
    "recover = False\n",
    "\n",
    "if v_path.exists() and recover:\n",
    "    print('Recovering old training process...')\n",
    "    checkpoint = torch.load(v1_path)\n",
    "    resume = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    criteria = checkpoint['criteria']\n",
    "else:\n",
    "    print('Ready for new training process... =)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100 | Train loss: 177.31226 | Train accuracy: 0.51351273 | Test loss: 177.20556 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[2/100 | Train loss: 177.32093 | Train accuracy: 0.51351273 | Test loss: 177.21286 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2/5\n",
      "[3/100 | Train loss: 177.31588 | Train accuracy: 0.51351273 | Test loss: 177.20786 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[4/100 | Train loss: 178.04188 | Train accuracy: 0.48648727 | Test loss: 177.9333 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[5/100 | Train loss: 178.15609 | Train accuracy: 0.51351273 | Test loss: 178.04764 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2/5\n",
      "[6/100 | Train loss: 178.02308 | Train accuracy: 0.48648727 | Test loss: 177.91451 | Test accuracy: 0.48648982]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[7/100 | Train loss: 177.39413 | Train accuracy: 0.51351273 | Test loss: 177.28607 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[8/100 | Train loss: 178.06508 | Train accuracy: 0.48648727 | Test loss: 177.95648 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[9/100 | Train loss: 177.36136 | Train accuracy: 0.51351273 | Test loss: 177.25331 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[10/100 | Train loss: 177.56907 | Train accuracy: 0.48648727 | Test loss: 177.46082 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[11/100 | Train loss: 177.3905 | Train accuracy: 0.51351273 | Test loss: 177.28245 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[12/100 | Train loss: 177.50171 | Train accuracy: 0.51351273 | Test loss: 177.3936 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[13/100 | Train loss: 177.43106 | Train accuracy: 0.48648727 | Test loss: 177.32291 | Test accuracy: 0.48648982]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[14/100 | Train loss: 177.41555 | Train accuracy: 0.48648727 | Test loss: 177.30741 | Test accuracy: 0.48648982]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[15/100 | Train loss: 177.31828 | Train accuracy: 0.51351273 | Test loss: 177.21024 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[16/100 | Train loss: 177.53031 | Train accuracy: 0.51351273 | Test loss: 177.42219 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[17/100 | Train loss: 177.35647 | Train accuracy: 0.51351273 | Test loss: 177.24842 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[18/100 | Train loss: 178.04303 | Train accuracy: 0.48648727 | Test loss: 177.93444 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[19/100 | Train loss: 177.34503 | Train accuracy: 0.51351273 | Test loss: 177.23695 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[20/100 | Train loss: 177.82756 | Train accuracy: 0.48648727 | Test loss: 177.71913 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[21/100 | Train loss: 178.15982 | Train accuracy: 0.51351273 | Test loss: 178.05137 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2/5\n",
      "[22/100 | Train loss: 177.32081 | Train accuracy: 0.51351273 | Test loss: 177.21278 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[23/100 | Train loss: 178.57689 | Train accuracy: 0.48648727 | Test loss: 178.46795 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[24/100 | Train loss: 177.56625 | Train accuracy: 0.48648727 | Test loss: 177.458 | Test accuracy: 0.48648982]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[25/100 | Train loss: 177.78317 | Train accuracy: 0.48648727 | Test loss: 177.67477 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[26/100 | Train loss: 177.59842 | Train accuracy: 0.51351273 | Test loss: 177.49027 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[27/100 | Train loss: 177.44841 | Train accuracy: 0.48648727 | Test loss: 177.34024 | Test accuracy: 0.48648982]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[28/100 | Train loss: 177.4543 | Train accuracy: 0.48648727 | Test loss: 177.34613 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[29/100 | Train loss: 177.34429 | Train accuracy: 0.51351273 | Test loss: 177.23625 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[30/100 | Train loss: 177.48945 | Train accuracy: 0.48648727 | Test loss: 177.38126 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[31/100 | Train loss: 177.31774 | Train accuracy: 0.51351273 | Test loss: 177.20969 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[32/100 | Train loss: 177.47857 | Train accuracy: 0.48648727 | Test loss: 177.37038 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[33/100 | Train loss: 177.71759 | Train accuracy: 0.48648727 | Test loss: 177.60923 | Test accuracy: 0.48648982]\n",
      "Trigger times: 2/5\n",
      "[34/100 | Train loss: 177.85736 | Train accuracy: 0.51351273 | Test loss: 177.74908 | Test accuracy: 0.51351018]\n",
      "Trigger times: 3/5\n",
      "[35/100 | Train loss: 177.31713 | Train accuracy: 0.51351273 | Test loss: 177.20908 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[36/100 | Train loss: 178.60892 | Train accuracy: 0.48648727 | Test loss: 178.49996 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[37/100 | Train loss: 177.83535 | Train accuracy: 0.51351273 | Test loss: 177.72708 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[38/100 | Train loss: 177.53461 | Train accuracy: 0.51351273 | Test loss: 177.42649 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[39/100 | Train loss: 177.32908 | Train accuracy: 0.51351273 | Test loss: 177.22104 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[40/100 | Train loss: 177.31915 | Train accuracy: 0.51351273 | Test loss: 177.2111 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[41/100 | Train loss: 177.34377 | Train accuracy: 0.51351273 | Test loss: 177.23569 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[42/100 | Train loss: 177.33814 | Train accuracy: 0.51351273 | Test loss: 177.23007 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[43/100 | Train loss: 177.40068 | Train accuracy: 0.51351273 | Test loss: 177.29262 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[44/100 | Train loss: 177.82145 | Train accuracy: 0.48648727 | Test loss: 177.71302 | Test accuracy: 0.48648982]\n",
      "Trigger times: 2/5\n",
      "[45/100 | Train loss: 177.31587 | Train accuracy: 0.51351273 | Test loss: 177.20783 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[46/100 | Train loss: 177.40163 | Train accuracy: 0.51351273 | Test loss: 177.2935 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[47/100 | Train loss: 178.51986 | Train accuracy: 0.51351273 | Test loss: 178.41121 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2/5\n",
      "[48/100 | Train loss: 177.43339 | Train accuracy: 0.48648727 | Test loss: 177.32524 | Test accuracy: 0.48648982]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[49/100 | Train loss: 177.96613 | Train accuracy: 0.48648727 | Test loss: 177.8576 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[50/100 | Train loss: 177.56007 | Train accuracy: 0.51351273 | Test loss: 177.45193 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[51/100 | Train loss: 177.80627 | Train accuracy: 0.51351273 | Test loss: 177.69801 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[52/100 | Train loss: 177.34944 | Train accuracy: 0.51351273 | Test loss: 177.24136 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[53/100 | Train loss: 177.31774 | Train accuracy: 0.51351273 | Test loss: 177.2097 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[54/100 | Train loss: 177.31823 | Train accuracy: 0.51351273 | Test loss: 177.21019 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[55/100 | Train loss: 177.36314 | Train accuracy: 0.51351273 | Test loss: 177.25509 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2/5\n",
      "[56/100 | Train loss: 177.3162 | Train accuracy: 0.51351273 | Test loss: 177.20815 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[57/100 | Train loss: 179.57486 | Train accuracy: 0.48648727 | Test loss: 179.46526 | Test accuracy: 0.48648982]\n",
      "Trigger times: 1/5\n",
      "[58/100 | Train loss: 180.00883 | Train accuracy: 0.51351273 | Test loss: 179.89934 | Test accuracy: 0.51351018]\n",
      "Trigger times: 2/5\n",
      "[59/100 | Train loss: 177.35767 | Train accuracy: 0.51351273 | Test loss: 177.24957 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[60/100 | Train loss: 177.62619 | Train accuracy: 0.51351273 | Test loss: 177.51802 | Test accuracy: 0.51351018]\n",
      "Trigger times: 1/5\n",
      "[61/100 | Train loss: 177.5325 | Train accuracy: 0.48648727 | Test loss: 177.42427 | Test accuracy: 0.48648982]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n",
      "[62/100 | Train loss: 177.35255 | Train accuracy: 0.51351273 | Test loss: 177.24446 | Test accuracy: 0.51351018]\n",
      "Trigger times: 0/5\n",
      "New checkpoint...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m wanna_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wanna_train:\n\u001b[1;32m----> 3\u001b[0m     trained_model, train_stats, test_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv1_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot training this model anymore...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[100], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(device, model, epochs, optimizer, criteria, train_loader, valid_loader, resume, save_as)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(resume, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     47\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m times, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     50\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     51\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:140\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wanna_train = True\n",
    "if wanna_train:\n",
    "    trained_model, train_stats, test_stats = train(device, model, epochs, optimizer, criteria, train_loader, test_loader, resume=resume, save_as=v1_path)\n",
    "else:\n",
    "    print('Not training this model anymore...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "v1_path_trained = Path('../Trained/NetV1.pth')\n",
    "if v0_path.exists():\n",
    "    print('Copying last checkpoint of model {0} state to trained folder'.format('NetV1'))\n",
    "    shutil.copy(v1_path, v1_path_trained)\n",
    "else:\n",
    "    print('Model {0} is not created yeat'.format('NetV1'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO1PZIgcCRT/kG8sHY+B9DH",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
